<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1081</article-id>
<article-id pub-id-type="doi">10.21105/joss.01081</article-id>
<title-group>
<article-title>IDTxl: The Information Dynamics Toolkit xl: a Python
package for the efficient analysis of multivariate information dynamics
in networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7105-5207</contrib-id>
<string-name>Patricia Wollstadt</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9910-8972</contrib-id>
<string-name>Joseph T. Lizier</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7894-6213</contrib-id>
<string-name>Raul Vicente</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-8542-4205</contrib-id>
<string-name>Conor Finn</string-name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6866-3316</contrib-id>
<string-name>Mario Martinez-Zarzuela</string-name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1789-5894</contrib-id>
<string-name>Pedro Mediano</string-name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6081-3367</contrib-id>
<string-name>Leonardo Novelli</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-8010-5862</contrib-id>
<string-name>Michael Wibral</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-7"/>
<xref ref-type="aff" rid="aff-8"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>MEG Unit, Brain Imaging Center, Goethe-University
Frankfurt, Fankfurt am Main, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Centre for Complex Systems, Faculty of Engineering and IT,
The University of Sydney, Sydney, Australia</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Computational Neuroscience Lab, Institute of Computer
Science, Tartu, Estonia</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Data61, CSIRO, Epping, Australia</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Communications and Signal Theory and Telematics
Engineering, University of Valladolid, Valladolid, Spain</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Computational Neurodynamics Group, Department of Computing,
Imperial College London, London, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>Max Planck Institute for Dynamics and Self-Organization,
Göttingen, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-8">
<institution-wrap>
<institution>Campus Institute for Dynamics of Biological Networks,
Georg-August Universität, Göttingen, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2018-07-07">
<day>7</day>
<month>7</month>
<year>2018</year>
</pub-date>
<volume>4</volume>
<issue>34</issue>
<fpage>1081</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>information theory</kwd>
<kwd>network inference</kwd>
<kwd>multivariate transfer entropy</kwd>
<kwd>mutual information</kwd>
<kwd>active information storage</kwd>
<kwd>partial information decomposition</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>We present IDTxl (the <bold>I</bold>nformation
  <bold>D</bold>ynamics <bold>T</bold>oolkit <bold>xl</bold>), a new
  open source Python toolbox for effective network inference from
  multivariate time series using information theory, available from
  GitHub (https://github.com/pwollstadt/IDTxl).</p>
  <p>Information theory
  (<xref alt="Cover &amp; Thomas, 2006" rid="ref-Cover2006" ref-type="bibr">Cover
  &amp; Thomas, 2006</xref>;
  <xref alt="MacKay, 2003" rid="ref-MacKay2003" ref-type="bibr">MacKay,
  2003</xref>;
  <xref alt="Shannon, 1948" rid="ref-Shannon1948" ref-type="bibr">Shannon,
  1948</xref>) is the mathematical theory of information and its
  transmission over communication channels. Information theory provides
  quantitative measures of the information content of a single random
  variable (entropy) and of the information shared between two variables
  (mutual information). The defined measures build on probability theory
  and solely depend on the probability distributions of the variables
  involved. As a consequence, the dependence between two variables can
  be quantified as the information shared between them, without the need
  to explicitly model a specific type of dependence. Hence, mutual
  information is a model-free measure of dependence, which makes it a
  popular choice for the analysis of systems other than communication
  channels.</p>
  <p>Transfer entropy (TE)
  (<xref alt="Schreiber, 2000" rid="ref-Schreiber2000" ref-type="bibr">Schreiber,
  2000</xref>) is an extension of mutual information that measures the
  directed information transfer between time series of a source and a
  target variable. TE has become popular in many scientific disciplines
  to infer dependencies and whole networks from data. Notable
  application domains include neuroscience
  (<xref alt="Wibral et al., 2014" rid="ref-Wibral2014" ref-type="bibr">Wibral
  et al., 2014</xref>) and dynamical systems analysis
  (<xref alt="Lizier et al., 2014" rid="ref-Lizier2014FrameworkCAs" ref-type="bibr">Lizier
  et al., 2014</xref>) (see Bossomaier et al.
  (<xref alt="2016" rid="ref-Bossomaier2016" ref-type="bibr">2016</xref>)
  for an introduction to TE and a comprehensive discussion of its
  application). In the majority of the applications, TE is used in a
  bivariate fashion, where information transfer is quantified between
  all source-target pairs. In a multivariate setting, however, such a
  bivariate analysis may infer spurious or redundant interactions, where
  multiple sources provide the same information about the target.
  Conversely, bivariate analysis may also miss synergistic interactions
  between multiple relevant sources and the target, where these multiple
  sources jointly transfer more information into the target than what
  could be detected from examining source contributions individually.
  Hence, tools for multivariate TE estimation, accounting for all
  relevant sources of a target, are required. An exhaustive multivariate
  approach is computationally intractable, even for a small number of
  potential sources in the data. Thus, a suitable approximate approach
  is needed. Although such approaches have been proposed (e.g., Lizier
  &amp; Rubinov
  (<xref alt="2012" rid="ref-Lizier2012" ref-type="bibr">2012</xref>)
  and Faes et al.
  (<xref alt="2011" rid="ref-Faes2011" ref-type="bibr">2011</xref>)) and
  first software implementations exist
  (<xref alt="Montalto et al., 2014" rid="ref-Montalto2014" ref-type="bibr">Montalto
  et al., 2014</xref>), there is no current implementation that deals
  with the practical problems that arise in multivariate TE estimation.
  These problems include the control of statistical errors that arise
  from testing multiple potential sources in a data set, and the
  optimization of parameters necessary for the estimation of
  multivariate TE.</p>
  <p>IDTxl provides such an implementation, controlling for false
  positives during the selection of relevant sources and providing
  methods for automatic parameter selection. To estimate multivariate
  TE, IDTxl utilises a greedy or iterative approach that builds sets of
  parent sources for each target node in the network through
  maximisation of a conditional mutual information criterion
  (<xref alt="Faes et al., 2011" rid="ref-Faes2011" ref-type="bibr">Faes
  et al., 2011</xref>;
  <xref alt="Lizier &amp; Rubinov, 2012" rid="ref-Lizier2012" ref-type="bibr">Lizier
  &amp; Rubinov, 2012</xref>). This iterative conditioning is designed
  to both removes redundancies and capture synergistic interactions in
  building each parent set. The conditioning thus automatically
  constructs a non-uniform, multivariate embedding of potential sources
  (<xref alt="Faes et al., 2011" rid="ref-Faes2011" ref-type="bibr">Faes
  et al., 2011</xref>) and optimizes source-target delays
  (<xref alt="Wibral et al., 2013" rid="ref-Wibral2013" ref-type="bibr">Wibral
  et al., 2013</xref>). Rigorous statistical controls (based on
  comparison to null distributions from time-series surrogates) are used
  to gate parent selection and to provide automatic stopping conditions
  for the inference, requiring only a minimum of user-specified
  settings.</p>
  <p>Following this greedy approach, IDTxl implements further algorithms
  for network inference (multivariate mutual information, bivariate
  mutual information, and bivariate transfer entropy), and provides
  measures to study the dynamics of various information flows on the
  inferred networks. These measures include active information storage
  (AIS)
  (<xref alt="Lizier et al., 2012" rid="ref-Lizier2012lais" ref-type="bibr">Lizier
  et al., 2012</xref>) for the analysis of information storage within
  network nodes, and partial information decomposition (PID)
  (<xref alt="Bertschinger et al., 2014" rid="ref-Bertschinger2014" ref-type="bibr">Bertschinger
  et al., 2014</xref>;
  <xref alt="Makkeh et al., 2018" rid="ref-Makkeh2018" ref-type="bibr">Makkeh
  et al., 2018</xref>;
  <xref alt="Williams &amp; Beer, 2010" rid="ref-Williams2010" ref-type="bibr">Williams
  &amp; Beer, 2010</xref>) for the analysis of synergistic, redundant,
  and unique information two source nodes have about one target node.
  Where applicable, IDTxl provides the option to return local variants
  of estimated measures
  (<xref alt="Lizier, 2014b" rid="ref-Lizier2014Local" ref-type="bibr">Lizier,
  2014b</xref>). Also, tools are included for group-level analysis of
  the inferred networks, e.g. comparing between subjects or conditions
  in neural recordings.</p>
  <p>The toolkit is highly flexible, providing various
  information-theoretic estimators for the user to select from; these
  handle both discrete and continuous time-series data, and allow
  choices, e.g. using linear Gaussian estimators (i.e. Granger
  causality, Granger
  (<xref alt="1969" rid="ref-Granger1969" ref-type="bibr">1969</xref>))
  for speed versus nonlinear estimators (e.g. Kraskov et al.
  (<xref alt="2004" rid="ref-Kraskov2004" ref-type="bibr">2004</xref>))
  for accuracy (see the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/pwollstadt/IDTxl">IDTxl
  homepage</ext-link> for details). Further, estimator implementations
  for both CPU and GPU compute platforms are provided, which offer
  parallel computing engines for efficiency. IDTxl provides these
  low-level estimator choices for network analysis algorithms but also
  allows direct access to estimators for linear and nonlinear estimation
  of (conditional) mutual information, TE, and AIS for both discrete and
  continuous data. Furthermore low-level estimators for the estimation
  of PID from discrete data are provided.</p>
  <p>The toolkit is a next-generation combination of the existing
  TRENTOOL
  (<xref alt="Lindner et al., 2011" rid="ref-Lindner2011" ref-type="bibr">Lindner
  et al., 2011</xref>) and JIDT
  (<xref alt="Lizier, 2014a" rid="ref-Lizier2014jidt" ref-type="bibr">Lizier,
  2014a</xref>) toolkits, extending TRENTOOL’s pairwise transfer entropy
  analysis to a multivariate one, and adding a wider variety of
  estimator types. Further, IDTxl is Python3 based and requires no
  proprietary libraries. The primary application area for IDTxl lies in
  analysing brain imaging data (import tools for common neuroscience
  formats, e.g. FieldTrip, are included). However, the toolkit is
  generic for analysing multivariate time-series data from any
  discipline. This is realised by providing a generic data format and
  the possibility to easily extend the toolkit by adding import or
  export routines, by adding new core estimators, or by adding new
  algorithms based on existing estimators.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Bossomaier2016">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Bossomaier</surname><given-names>Terry</given-names></name>
          <name><surname>Barnett</surname><given-names>Lionel</given-names></name>
          <name><surname>Harré</surname><given-names>Michael</given-names></name>
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
        </person-group>
        <source>An introduction to transfer entropy</source>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Cham, Switzerland</publisher-loc>
        <year iso-8601-date="2016">2016</year>
        <pub-id pub-id-type="doi">10.1007/978-3-319-43222-9</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Faes2011">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Faes</surname><given-names>Luca</given-names></name>
          <name><surname>Nollo</surname><given-names>Giandomenico</given-names></name>
          <name><surname>Porta</surname><given-names>Alberto</given-names></name>
        </person-group>
        <article-title>Information-based detection of nonlinear Granger causality in multivariate processes via a nonuniform embedding technique</article-title>
        <source>Physical Review E - Statistical, Nonlinear, and Soft Matter Physics</source>
        <year iso-8601-date="2011">2011</year>
        <volume>83</volume>
        <pub-id pub-id-type="doi">10.1103/PhysRevE.83.051112</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Kraskov2004">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kraskov</surname><given-names>Alexander</given-names></name>
          <name><surname>Stögbauer</surname><given-names>Harald</given-names></name>
          <name><surname>Grassberger</surname><given-names>Peter</given-names></name>
        </person-group>
        <article-title>Estimating mutual information.</article-title>
        <source>Physical Review E - Statistical, Nonlinear, and Soft Matter Physics</source>
        <year iso-8601-date="2004">2004</year>
        <volume>69</volume>
        <pub-id pub-id-type="doi">10.1103/PhysRevE.69.066138</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Lizier2012">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
          <name><surname>Rubinov</surname><given-names>Mikail</given-names></name>
        </person-group>
        <article-title>Multivariate construction of effective computational networks from observational data</article-title>
        <source>Max Planck Institute: Preprint</source>
        <year iso-8601-date="2012">2012</year>
        <volume>Preprint no. 25</volume>
      </element-citation>
    </ref>
    <ref id="ref-Montalto2014">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Montalto</surname><given-names>Alessandro</given-names></name>
          <name><surname>Faes</surname><given-names>Luca</given-names></name>
          <name><surname>Marinazzo</surname><given-names>Daniele</given-names></name>
        </person-group>
        <article-title>MuTE : a MATLAB toolbox to compare established and novel estimators of the multivariate transfer entropy</article-title>
        <source>PLoS ONE</source>
        <year iso-8601-date="2014">2014</year>
        <volume>9</volume>
        <issue>10</issue>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0109462</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Schreiber2000">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schreiber</surname><given-names>Thomas</given-names></name>
        </person-group>
        <article-title>Measuring information transfer</article-title>
        <source>Physical Review Letters</source>
        <year iso-8601-date="2000">2000</year>
        <volume>85</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1103/PhysRevLett.85.461</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Wibral2013">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wibral</surname><given-names>Michael</given-names></name>
          <name><surname>Pampu</surname><given-names>Nicolae</given-names></name>
          <name><surname>Priesemann</surname><given-names>Viola</given-names></name>
          <name><surname>Siebenhühner</surname><given-names>Felix</given-names></name>
          <name><surname>Seiwert</surname><given-names>Hannes</given-names></name>
          <name><surname>Lindner</surname><given-names>Michael</given-names></name>
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
          <name><surname>Vicente</surname><given-names>Raul</given-names></name>
        </person-group>
        <article-title>Measuring information-transfer delays</article-title>
        <source>PloS ONE</source>
        <year iso-8601-date="2013">2013</year>
        <volume>8</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0055809</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Shannon1948">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shannon</surname><given-names>Claude E</given-names></name>
        </person-group>
        <article-title>A mathematical theory of communication</article-title>
        <source>The Bell System Technical Journal</source>
        <year iso-8601-date="1948">1948</year>
        <volume>27</volume>
        <pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb00917.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MacKay2003">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>MacKay</surname><given-names>David J. C.</given-names></name>
        </person-group>
        <source>Information theory, inference and learning algorithms</source>
        <publisher-name>Cambridge University Press</publisher-name>
        <publisher-loc>Cambridge</publisher-loc>
        <year iso-8601-date="2003">2003</year>
      </element-citation>
    </ref>
    <ref id="ref-Cover2006">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Cover</surname><given-names>Thomas M.</given-names></name>
          <name><surname>Thomas</surname><given-names>Joy A.</given-names></name>
        </person-group>
        <source>Elements of information theory</source>
        <publisher-name>John Wiley &amp; Sons</publisher-name>
        <publisher-loc>New York, NY</publisher-loc>
        <year iso-8601-date="2006">2006</year>
        <pub-id pub-id-type="doi">10.1002/047174882X</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Wibral2014">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Wibral</surname><given-names>Michael</given-names></name>
          <name><surname>Vicente</surname><given-names>Raul</given-names></name>
          <name><surname>Lindner</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>Transfer entropy in neuroscience</article-title>
        <source>Directed information measures in neuroscience</source>
        <person-group person-group-type="editor">
          <name><surname>Wibral</surname><given-names>Michael</given-names></name>
          <name><surname>Vicente</surname><given-names>Raul</given-names></name>
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <pub-id pub-id-type="doi">10.1007/978-3-642-54474-3_1</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Lizier2012lais">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
          <name><surname>Prokopenko</surname><given-names>Mikhail</given-names></name>
          <name><surname>Zomaya</surname><given-names>Albert Y.</given-names></name>
        </person-group>
        <article-title>Local measures of information storage in complex distributed computation</article-title>
        <source>Information Sciences</source>
        <year iso-8601-date="2012">2012</year>
        <volume>208</volume>
        <pub-id pub-id-type="doi">10.1016/j.ins.2012.04.016</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Williams2010">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Williams</surname><given-names>Paul L.</given-names></name>
          <name><surname>Beer</surname><given-names>Randall D.</given-names></name>
        </person-group>
        <article-title>Nonnegative Decomposition of Multivariate Information</article-title>
        <source>arXiv preprint</source>
        <year iso-8601-date="2010">2010</year>
        <volume>arXiv:1004.2515</volume>
        <pub-id pub-id-type="doi">http://arxiv.org/abs/1004.2515</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Bertschinger2014">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bertschinger</surname><given-names>Nils</given-names></name>
          <name><surname>Rauh</surname><given-names>Johannes</given-names></name>
          <name><surname>Olbrich</surname><given-names>Eckehard</given-names></name>
          <name><surname>Jost</surname><given-names>Jürgen</given-names></name>
          <name><surname>Ay</surname><given-names>Nihat</given-names></name>
        </person-group>
        <article-title>Quantifying unique information</article-title>
        <source>Entropy</source>
        <year iso-8601-date="2014">2014</year>
        <volume>16</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.3390/e16042161</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Makkeh2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Makkeh</surname><given-names>Abdullah</given-names></name>
          <name><surname>Theis</surname><given-names>Dirk Oliver</given-names></name>
          <name><surname>Vicente</surname><given-names>Raul</given-names></name>
        </person-group>
        <article-title>BROJA-2PID: A robust estimator for bertschinger et al.’s bivariate partial information decomposition</article-title>
        <source>Entropy</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>20</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.3390/e20040271</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Lizier2014jidt">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
        </person-group>
        <article-title>JIDT: An Information-Theoretic Toolkit for Studying the Dynamics of Complex Systems</article-title>
        <source>Frontiers in Robotics and AI</source>
        <year iso-8601-date="2014">2014</year>
        <volume>1</volume>
        <issue>December</issue>
        <pub-id pub-id-type="doi">10.3389/frobt.2014.00011</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Lindner2011">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lindner</surname><given-names>Michael</given-names></name>
          <name><surname>Vicente</surname><given-names>Raul</given-names></name>
          <name><surname>Priesemann</surname><given-names>Viola</given-names></name>
          <name><surname>Wibral</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>TRENTOOL: A Matlab open source toolbox to analyse information flow in time series data with transfer entropy</article-title>
        <source>BMC Neuroscience</source>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1186/1471-2202-12-119</pub-id>
        <pub-id pub-id-type="pmid">22098775</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Lizier2014Local">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
        </person-group>
        <article-title>Measuring the dynamics of information processing on a local scale in time and space</article-title>
        <source>Directed information measures in neuroscience</source>
        <person-group person-group-type="editor">
          <name><surname>Wibral</surname><given-names>Michael</given-names></name>
          <name><surname>Vicente</surname><given-names>Raul</given-names></name>
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin, Heidelberg</publisher-loc>
        <year iso-8601-date="2014">2014</year>
        <pub-id pub-id-type="doi">10.1007/978-3-642-54474-3_7</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Lizier2014FrameworkCAs">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
          <name><surname>Prokopenko</surname><given-names>Mikhail</given-names></name>
          <name><surname>Zomaya</surname><given-names>Albert Y.</given-names></name>
        </person-group>
        <article-title>A framework for the local information dynamics of distributed computation in complex systems</article-title>
        <source>Guided self-organization: inception</source>
        <person-group person-group-type="editor">
          <name><surname>Prokopenko</surname><given-names>Mikhail</given-names></name>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin, Heidelberg</publisher-loc>
        <year iso-8601-date="2014">2014</year>
        <volume>9</volume>
        <pub-id pub-id-type="doi">10.1007/978-3-642-53734-9_5</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Granger1969">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Granger</surname><given-names>Clive W. J.</given-names></name>
        </person-group>
        <article-title>Investigating causal relations by econometric models and cross-spectral methods</article-title>
        <source>Econometrica</source>
        <year iso-8601-date="1969">1969</year>
        <volume>37</volume>
        <pub-id pub-id-type="doi">10.2307/1912791</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
