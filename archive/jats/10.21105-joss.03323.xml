<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3323</article-id>
<article-id pub-id-type="doi">10.21105/joss.03323</article-id>
<title-group>
<article-title>RENT: A Python Package for Repeated Elastic Net Feature
Selection</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6919-3483</contrib-id>
<string-name>Anna Jenul</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1327-4855</contrib-id>
<string-name>Stefan Schrunner</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-5210-132X</contrib-id>
<string-name>Bao Ngoc Huynh</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1595-9962</contrib-id>
<string-name>Oliver Tomic</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Data Science, Norwegian University of Life
Sciences</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Physics, Norwegian University of Life
Sciences</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>6</volume>
<issue>63</issue>
<fpage>3323</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>feature selection</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Due to modern data acquisition techniques, the number of generated
  features in measurement data keeps increasing. This increase can make
  the analysis with standard machine learning methods difficult because
  of underdetermined systems where the dimensionality of the feature
  space (number of features) exceeds the dimensionality of the object
  space (number of observations). A concrete example of such a situation
  is data acquisition in the healthcare domain, where the number of
  patients (observations) suffering from a specific condition may be
  relatively low, but a lot of measurements (number of features) are
  generated for each patient to acquire a good understanding of the
  patient’s health. A very common challenge is that not all features in
  a high dimensional space are equally important for predictive tasks —
  many might even be redundant. Feature selection deals with finding the
  most relevant features of a dataset. With help of appropriate
  methodology, feature selection can reduce (a) the complexity of and
  (b) noise in the dataset. More importantly, data interpretation of the
  model becomes easier with fewer features, which is of great importance
  within domains such as healthcare. Even though feature selection is a
  well-established research topic, relatively few approaches are
  focusing on the stability of the selection. The important question at
  hand is: can we trust that the selected features are really valid or
  is their selection very dependent on which observations are included
  in the data? Providing information on the stability of feature
  selection is vital, especially in wide data sets where the number of
  features can be many times higher than the number of observations.
  Here, the inclusion or exclusion of a few observations can have a high
  impact on which features may be selected.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>To get an understanding of which features are important and how
  stable the selection of each feature in the dataset is, a
  user-friendly software package is needed for this purpose. The RENT
  package, implementing the feature selection method of the same name
  (<xref alt="Jenul et al., 2021" rid="ref-JenulU003A2021" ref-type="bibr">Jenul
  et al., 2021</xref>), provides this information through an easy-to-use
  interface. The package includes functionalities for binary
  classification and regression problems. RENT is based on an ensemble
  of elastic net regularized models, which are trained on randomly, iid
  subsets of the rows of the full training data. Along with selecting
  informative features, the method provides information on model
  performance, selection stability, as well as interpretability.
  Compared to established feature selection packages available in
  <monospace>R</monospace> and Python, such as
  <monospace>Rdimtools</monospace>
  (<xref alt="You, 2020" rid="ref-RdimtoolsU003A2020" ref-type="bibr">You,
  2020</xref>) implementing Laplacian and Fisher scores or the
  scikit-learn feature selection module
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) implementing recursive feature elimination and
  sequential feature selection, RENT creates a deeper understanding of
  the data by utilizing information acquired through the ensemble. This
  aspect is realized through tools for post hoc data analysis,
  visualization, and feature selection validation provided with the
  package, along with an efficient and user-friendly implementation of
  the main methodology.</p>
</sec>
<sec id="concept-and-structure-of-rent">
  <title>Concept and Structure of RENT</title>
  <p>At its core, RENT trains <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  independent elastic net regularized models on distinct subsets of the
  training dataset. Each subset is generated using the scikit-learn
  function <monospace>train_test_split()</monospace> which delivers an
  iid sample from the full training dataset. The sampling processes of
  different subsets are mutually independent, with the condition that a
  single data point can appear at most once in each subset. A data
  point, however, can appear in multiple subsets. The framework is
  demonstrated in
  <xref alt="Figure 1" rid="figU003ARENT">Figure 1</xref>.</p>
  <fig>
    <caption><p>Summary of RENT method
    (<xref alt="Jenul et al., 2021" rid="ref-JenulU003A2021" ref-type="bibr">Jenul
    et al.,
    2021</xref>).<styled-content id="figU003ARENT"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="images/RENT_overview.png" xlink:title="" />
  </fig>
  <p>Based on three statistical cutoff criteria
  <inline-formula><alternatives>
  <tex-math><![CDATA[\tau_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\tau_2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\tau_3]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></alternatives></inline-formula>,
  relevant features are selected. While <inline-formula><alternatives>
  <tex-math><![CDATA[\tau_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  counts how often each feature was selected over
  <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  models, <inline-formula><alternatives>
  <tex-math><![CDATA[\tau_2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  quantifies the stability of the feature weights — a feature where the
  <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  weight signs alternate between positive and negative is less stable
  than a feature where all weights are of a constant sign. The third
  criterion <inline-formula><alternatives>
  <tex-math><![CDATA[\tau_3]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  deploys a Student’s <inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>-test
  to judge whether feature weights are significantly different from
  zero. The presented implementation builds on an abstract class
  <monospace>RENT_Base</monospace> with a general skeleton for feature
  selection and post hoc analysis. Two inherited classes,
  <monospace>RENT_Classification</monospace> and
  <monospace>RENT_Regression</monospace>, offer target-specific methods.
  The constructor of <monospace>RENT_Base</monospace> initializes the
  different user-specific parameters such as the dataset, elastic net
  regularization parameters, or the number of models
  <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>.
  After training, feature selection is conducted by use of the cutoff
  criteria. Deeper insights are provided by a matrix containing the
  cutoff criteria values of each feature, as well as a matrix comprising
  raw model weights of each feature throughout the
  <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  elementary model. For initial analysis of the results, the package
  delivers multiple plotting functions, such as a barplot of
  <inline-formula><alternatives>
  <tex-math><![CDATA[\tau_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>.
  Additionally, two validation studies are implemented: first, a model
  based on random feature selection is trained, while second, a model
  based on randomly permuted labels of the test dataset is obtained.
  Results of both validation models are compared to a model built with
  RENT features using Student’s <inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>-tests
  as well as empirical densities.</p>
  <p>In addition to feature selection, RENT offers a detailed summary of
  prediction accuracies for the training objects. For each training
  object, this information can be visualized as histograms of class
  probabilities for classification problems or histograms of mean
  absolute errors for regression problems, respectively. For extended
  analysis, principal component analysis reveals properties of training
  objects and their relation to features selected by RENT. For
  computation and visualization of principal components, RENT uses
  functionality from the <monospace>hoggorm</monospace> and
  <monospace>hoggormplot</monospace> packages
  (<xref alt="Tomic et al., 2019" rid="ref-TomicU003A2019" ref-type="bibr">Tomic
  et al., 2019</xref>).</p>
</sec>
<sec id="ongoing-research-and-dissemination">
  <title>Ongoing Research and Dissemination</title>
  <p>The manuscript RENT - Repeated Elastic Net Technique for Feature
  Selection is currently under review. Further, the method and the
  package are used in different master thesis projects at the Norwegian
  University of Life Sciences, mainly in the field of healthcare data
  analysis.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank Runar Helin for proofreading the documentation.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-TomicU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tomic</surname><given-names>Oliver</given-names></name>
          <name><surname>Graff</surname><given-names>Thomas</given-names></name>
          <name><surname>Liland</surname><given-names>Kristian Hovde</given-names></name>
          <name><surname>Næs</surname><given-names>Tormod</given-names></name>
        </person-group>
        <article-title>Hoggorm: A python library for explorative multivariate statistics</article-title>
        <source>The Journal of Open Source Software</source>
        <year iso-8601-date="2019">2019</year>
        <volume>4</volume>
        <issue>39</issue>
        <uri>http://joss.theoj.org/papers/10.21105/joss.00980</uri>
        <pub-id pub-id-type="doi">10.21105/joss.00980</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-RdimtoolsU003A2020">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>You</surname><given-names>Kisung</given-names></name>
        </person-group>
        <source>Rdimtools: Dimension reduction and estimation methods</source>
        <year iso-8601-date="2020">2020</year>
        <uri>https://CRAN.R-project.org/package=Rdimtools</uri>
      </element-citation>
    </ref>
    <ref id="ref-JenulU003A2021">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Jenul</surname><given-names>Anna</given-names></name>
          <name><surname>Schrunner</surname><given-names>Stefan</given-names></name>
          <name><surname>Liland</surname><given-names>Kristian Hovde</given-names></name>
          <name><surname>Indahl</surname><given-names>Ulf Geir</given-names></name>
          <name><surname>Futsaether</surname><given-names>Cecilia Marie</given-names></name>
          <name><surname>Tomic</surname><given-names>Oliver</given-names></name>
        </person-group>
        <article-title>RENT – repeated elastic net technique for feature selection</article-title>
        <year iso-8601-date="2021">2021</year>
        <uri>https://arxiv.org/abs/2009.12780</uri>
      </element-citation>
    </ref>
    <ref id="ref-scikit-learn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
          <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
          <name><surname>Gramfort</surname><given-names>A.</given-names></name>
          <name><surname>Michel</surname><given-names>V.</given-names></name>
          <name><surname>Thirion</surname><given-names>B.</given-names></name>
          <name><surname>Grisel</surname><given-names>O.</given-names></name>
          <name><surname>Blondel</surname><given-names>M.</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
          <name><surname>Weiss</surname><given-names>R.</given-names></name>
          <name><surname>Dubourg</surname><given-names>V.</given-names></name>
          <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
          <name><surname>Passos</surname><given-names>A.</given-names></name>
          <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
          <name><surname>Brucher</surname><given-names>M.</given-names></name>
          <name><surname>Perrot</surname><given-names>M.</given-names></name>
          <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
