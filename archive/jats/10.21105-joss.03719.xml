<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3719</article-id>
<article-id pub-id-type="doi">10.21105/joss.03719</article-id>
<title-group>
<article-title>AuDoLab: Automatic document labelling and classification
for extremely unbalanced data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0872-7098</contrib-id>
<string-name>Arne Tillmann</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Anton Thielmann</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Gillian Kant</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Christoph Weisser</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Benjamin Säfken</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Alexander Silbersdorff</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Thomas Kneib</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Georg-August-Universität Göttingen, Göttingen,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Campus-Institut Data Science (CIDAS), Göttingen,
Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-04-24">
<day>24</day>
<month>4</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>66</issue>
<fpage>3719</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>One-class SVM</kwd>
<kwd>Unsupervised Document Classification</kwd>
<kwd>One-class Document Classification</kwd>
<kwd>LDA Topic Modelling</kwd>
<kwd>Out-of-domain Training Data</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>AuDoLab provides a novel approach to one-class document
  classification for heavily imbalanced datasets, even if labelled
  training data is not available. Our package enables the user to create
  specific out-of-domain training data to classify a heavily
  underrepresented target class in a document dataset using a recently
  developed integration of Web Scraping, Latent Dirichlet Allocation
  Topic Modelling and One-class Support Vector Machines
  (<xref alt="Thielmann, Weisser, Krenz, &amp; Säfken, 2021" rid="ref-Thielmann" ref-type="bibr">Thielmann,
  Weisser, Krenz, &amp; Säfken, 2021</xref>). AuDoLab can achieve high
  quality results even on highly specific classification problems
  without the need to invest in the time and cost intensive labelling of
  training documents by humans. Hence, AuDoLab has a broad range of
  scientific research or business real world applications. In the
  following, a few potential use cases will be briefly discussed that
  should illustrate the broad range of applications in various domains.
  For example AuDoLab could be used to identify emails with very
  specific topics such as fraud or money laundering that might have an
  extremely low prevalence. Similarly, AuDoLab could be used in the
  medical field to classify medical documents that are concerned with
  very specific topics such as heart attacks or dental problems.
  Furthermore, AuDoLab may be used to identify legal documents with very
  specific topics such as machine learning. Note that, the only limiting
  factor to the broad range of use cases, is the availability of
  out-of-domain training data, that can be generated via Web Scraping
  from IEEEXplore
  (<xref alt="IEEE Xplore, 2020" rid="ref-IEEE" ref-type="bibr">IEEE
  Xplore, 2020</xref>), ArXiv or PubMed. Given that a broad range of
  training documents can be obtained from these websites AuDoLab has a
  correspondingly broad range of applications. The following section
  provides an overview of AuDoLab. AuDoLab can be installed conveniently
  via pip. A detailed description of the package and installation and
  can be found in the packages repository or on the documentation
  website.<xref ref-type="fn" rid="fn1">1</xref></p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Unsupervised document classification is mainly performed to gain
  insight into the underlying topics of large text corpora. In this
  process, documents covering highly underrepresented topics have only a
  minor impact on the algorithm’s topic definitions. As a result,
  underrepresented topics can sometimes be “overlooked” and documents
  are assigned topic prevalences that do not reflect the underlying
  content. Thus, labeling underrepresented topics in large text corpora
  is often done manually and can therefore be very labour-intensive and
  time-consuming. AuDoLab enables the user to tackle this problem and
  perform unsupervised one-class document classification for heavily
  underrepresented document classes. This leverages the results of
  one-class document classification using One-class Support Vector
  Machines (SVM)
  (<xref alt="Manevitz &amp; Yousef, 2001" rid="ref-Manevitz" ref-type="bibr">Manevitz
  &amp; Yousef, 2001</xref>;
  <xref alt="Schölkopf et al., 2001" rid="ref-Scholkopf" ref-type="bibr">Schölkopf
  et al., 2001</xref>) and extends them to the use case of severely
  imbalanced datasets. This adaptation and extension is achieved by
  implementing a multi-level classification rule as visualised in the
  graph below.</p>
  <fig>
    <caption><p>Classification
    Procedure.<styled-content id="figU003Atest2"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="figures/tree.PNG" xlink:title="" />
  </fig>
  <p>The first part of the package allows the user to scrape training
  documents (scientific papers), ideally covering the target topic in
  which the user is interested, from IEEEXplore
  (<xref alt="IEEE Xplore, 2020" rid="ref-IEEE" ref-type="bibr">IEEE
  Xplore, 2020</xref>), ArXiv or PubMed. The user can search for
  multiple search terms and specify an individual search query and, in
  the case of IEEEXplore, scrape additional information such as the
  author names or the number of citations. Thus, an individually
  labelled (e.g., via author-keywords) training data set is created.
  Through the integration of pre-labelled out-of-domain training data,
  the problem of the heavily underrepresented target class can be
  circumvented, as large enough training corpora can be automatically
  generated. Subsequently, the text data is preprocessed for the
  classification part. The text preprocessing includes common Natural
  Language Processing (NLP) text preprocessing techniques such as
  stopword removal and lemmatization. As document representations the
  term frequency-inverse document frequency (tf-idf) representations are
  chosen. The tf-idf scores are computed on a joint corpus from the
  web-scraped out-of-domain training data and the target text data.</p>
  <p>The second and main part of the classification rule lies in the
  training of the one-class SVM
  (<xref alt="Schölkopf et al., 2001" rid="ref-Scholkopf" ref-type="bibr">Schölkopf
  et al., 2001</xref>). As a training corpus, only the out-of-domain
  training data is used. By adjusting hyperparameters, the user can
  create a strict or relaxed classification rule, that reflects the
  users belief about the prevalence of the target class inside the
  target data set and the quality of the scraped out-of-domain training
  data. The last part of the classification rule enables the user to
  control the classifiers results with the help of Latent Dirichlet
  Allocation (LDA) topic models
  (<xref alt="Blei et al., 2003" rid="ref-Blei" ref-type="bibr">Blei et
  al., 2003</xref>) (and e.g., wordclouds). Additionally, the user can
  generate interactive plots depicting the identified topics during the
  LDA topic modelling
  (<xref alt="Sievert &amp; Shirley, 2014" rid="ref-ldavis" ref-type="bibr">Sievert
  &amp; Shirley, 2014</xref>).</p>
  <p>The second step can be repeated, depending on the users perceived
  quality of the classification results.</p>
  <sec id="comparison-with-existing-tools">
    <title>Comparison with existing tools</title>
    <p>At the moment no Python Package with a comparable functionality
    of AuDoLab is available, since AuDoLab is based on a novel and
    recently published classification prodcedure
    (<xref alt="Thielmann, Weisser, Krenz, &amp; Säfken, 2021" rid="ref-Thielmann" ref-type="bibr">Thielmann,
    Weisser, Krenz, &amp; Säfken, 2021</xref>). Thereby, AuDoLab uses
    and integrates in particular a combination of Web Scraping, Topic
    Modelling and One-class Classifcation for which various individual
    packages are available. Details on the statistical methodology can
    be found in
    (<xref alt="Thielmann, Weisser, Krenz, &amp; Säfken, 2021" rid="ref-Thielmann" ref-type="bibr">Thielmann,
    Weisser, Krenz, &amp; Säfken, 2021</xref>). An application of the
    methodology on a data set of patent data can found in
    (<xref alt="Thielmann, Weisser, &amp; Krenz, 2021" rid="ref-Thielmann2021" ref-type="bibr">Thielmann,
    Weisser, &amp; Krenz, 2021</xref>). For Topic Modelling available
    packages are the LDA algorithm as implemented in the package Gensim
    (<xref alt="Řehůřek &amp; Sojka, 2010" rid="ref-rehurek_lrec" ref-type="bibr">Řehůřek
    &amp; Sojka, 2010</xref>) or the package TTLocVis
    (<xref alt="Kant et al., 2020" rid="ref-Kant2020" ref-type="bibr">Kant
    et al., 2020</xref>) for short and sparse text. Visual
    representations of the topics can be implemented with LDAvis
    (<xref alt="Sievert &amp; Shirley, 2014" rid="ref-ldavis" ref-type="bibr">Sievert
    &amp; Shirley, 2014</xref>). The One-class SVM classification
    package is availabe in Scikit-learn
    (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
    et al., 2011</xref>). Alternative further research could explore
    Deep Learning Algorithms as well
    (<xref alt="Säfken et al., 2020" rid="ref-Saefken2020" ref-type="bibr">Säfken
    et al., 2020</xref>,
    <xref alt="2021" rid="ref-Saefken2021" ref-type="bibr">2021</xref>).</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank the Campus-Institut Data Science (CIDAS), Göttingen,
  Germany for funding this project.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Blei">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Blei</surname><given-names>David M</given-names></name>
          <name><surname>Ng</surname><given-names>Andrew Y</given-names></name>
          <name><surname>Jordan</surname><given-names>Michael I</given-names></name>
        </person-group>
        <article-title>Latent dirichlet allocation</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2003">2003</year>
        <volume>3</volume>
        <issue>Jan</issue>
        <pub-id pub-id-type="doi">10.1162/jmlr.2003.3.4-5.993</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Thielmann">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Thielmann</surname><given-names>Anton</given-names></name>
          <name><surname>Weisser</surname><given-names>Christoph</given-names></name>
          <name><surname>Krenz</surname><given-names>Astrid</given-names></name>
          <name><surname>Säfken</surname><given-names>Benjamin</given-names></name>
        </person-group>
        <article-title>Unsupervised document classification integrating web scraping, one-class SVM and LDA topic modelling</article-title>
        <source>Journal of Applied Statistics</source>
        <publisher-name>Taylor &amp; Francis</publisher-name>
        <year iso-8601-date="2021">2021</year>
        <volume>0</volume>
        <issue>0</issue>
        <pub-id pub-id-type="doi">10.1080/02664763.2021.1919063</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Scholkopf">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schölkopf</surname><given-names>Bernhard</given-names></name>
          <name><surname>Platt</surname><given-names>John C</given-names></name>
          <name><surname>Shawe-Taylor</surname><given-names>John</given-names></name>
          <name><surname>Smola</surname><given-names>Alex J</given-names></name>
          <name><surname>Williamson</surname><given-names>Robert C</given-names></name>
        </person-group>
        <article-title>Estimating the support of a high-dimensional distribution</article-title>
        <source>Neural Computation</source>
        <publisher-name>MIT Press</publisher-name>
        <year iso-8601-date="2001">2001</year>
        <volume>13</volume>
        <issue>7</issue>
        <pub-id pub-id-type="doi">10.1162/089976601750264965</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-ldavis">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Sievert</surname><given-names>Carson</given-names></name>
          <name><surname>Shirley</surname><given-names>Kenneth</given-names></name>
        </person-group>
        <article-title>LDAvis: A method for visualizing and interpreting topics</article-title>
        <source>Proceedings of the workshop on interactive language learning, visualization, and interfaces</source>
        <year iso-8601-date="2014">2014</year>
        <pub-id pub-id-type="doi">10.3115/v1/W14-3110</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Manevitz">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Manevitz</surname><given-names>Larry M</given-names></name>
          <name><surname>Yousef</surname><given-names>Malik</given-names></name>
        </person-group>
        <article-title>One-class SVMs for document classification</article-title>
        <source>Journal of machine Learning research</source>
        <year iso-8601-date="2001">2001</year>
        <volume>2</volume>
        <issue>Dec</issue>
        <pub-id pub-id-type="doi">10.1162/15324430260185574</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Kant2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kant</surname><given-names>Gillian</given-names></name>
          <name><surname>Weisser</surname><given-names>Christoph</given-names></name>
          <name><surname>Säfken</surname><given-names>Benjamin</given-names></name>
        </person-group>
        <article-title>TTLocVis: A twitter topic location visualization package</article-title>
        <source>Journal of Open Source Software</source>
        <publisher-name>The Open Journal</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>5</volume>
        <issue>54</issue>
        <pub-id pub-id-type="doi">10.21105/joss.02507</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-rehurek_lrec">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Řehůřek</surname><given-names>Radim</given-names></name>
          <name><surname>Sojka</surname><given-names>Petr</given-names></name>
        </person-group>
        <article-title>Software Framework for Topic Modelling with Large Corpora</article-title>
        <source>Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</source>
        <publisher-name>ELRA</publisher-name>
        <year iso-8601-date="2010-05">2010</year><month>05</month>
        <pub-id pub-id-type="doi">10.13140/2.1.2393.1847</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-scikit-learn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
          <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
          <name><surname>Gramfort</surname><given-names>A.</given-names></name>
          <name><surname>Michel</surname><given-names>V.</given-names></name>
          <name><surname>Thirion</surname><given-names>B.</given-names></name>
          <name><surname>Grisel</surname><given-names>O.</given-names></name>
          <name><surname>Blondel</surname><given-names>M.</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
          <name><surname>Weiss</surname><given-names>R.</given-names></name>
          <name><surname>Dubourg</surname><given-names>V.</given-names></name>
          <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
          <name><surname>Passos</surname><given-names>A.</given-names></name>
          <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
          <name><surname>Brucher</surname><given-names>M.</given-names></name>
          <name><surname>Perrot</surname><given-names>M.</given-names></name>
          <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
      </element-citation>
    </ref>
    <ref id="ref-IEEE">
      <element-citation>
        <person-group person-group-type="author">
          <string-name>IEEE Xplore</string-name>
        </person-group>
        <article-title>IEEE Xplore Digital Library</article-title>
        <year iso-8601-date="2020">2020</year>
        <uri>https://ieeexplore.ieee.org/Xplore/home.jsp</uri>
      </element-citation>
    </ref>
    <ref id="ref-Saefken2020">
      <element-citation publication-type="book">
        <source>Learning deep: Perspectives on deep learning algorithms and artificial intelligence</source>
        <person-group person-group-type="editor">
          <name><surname>Säfken</surname><given-names>Benjamin</given-names></name>
          <name><surname>Silbersdorff</surname><given-names>Alexander</given-names></name>
          <name><surname>Weisser</surname><given-names>Christoph</given-names></name>
        </person-group>
        <publisher-name>Universitätsverlag Göttingen</publisher-name>
        <publisher-loc>Göttingen</publisher-loc>
        <year iso-8601-date="2020">2020</year>
        <pub-id pub-id-type="doi">10.17875/gup2020-1338</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Saefken2021">
      <element-citation publication-type="book">
        <source>Learning Deep Textwork: Perspectives on Natural Language Processing and Artificial Intelligence</source>
        <person-group person-group-type="editor">
          <name><surname>Säfken</surname><given-names>Benjamin</given-names></name>
          <name><surname>Silbersdorff</surname><given-names>Alexander</given-names></name>
          <name><surname>Weisser</surname><given-names>Christoph</given-names></name>
        </person-group>
        <publisher-name>Universitätsverlag Göttingen</publisher-name>
        <publisher-loc>Göttingen</publisher-loc>
        <year iso-8601-date="2021">2021</year>
        <pub-id pub-id-type="doi">10.17875/gup2021-1608</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Thielmann2021">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Thielmann</surname><given-names>Anton</given-names></name>
          <name><surname>Weisser</surname><given-names>Christoph</given-names></name>
          <name><surname>Krenz</surname><given-names>Astrid</given-names></name>
        </person-group>
        <article-title>One-class support vector machine and LDA topic model integration—evidence for AI patents</article-title>
        <source>Soft computing: Biomedical and related applications</source>
        <person-group person-group-type="editor">
          <name><surname>Phuong</surname><given-names>Nguyen Hoang</given-names></name>
          <name><surname>Kreinovich</surname><given-names>Vladik</given-names></name>
        </person-group>
        <publisher-name>Springer International Publishing</publisher-name>
        <year iso-8601-date="2021">2021</year>
        <pub-id pub-id-type="doi">10.1007/978-3-030-76620-7_23</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>https://AuDoLab.readthedocs.io</p>
  </fn>
</fn-group>
</back>
</article>
