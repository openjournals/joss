<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1132</article-id>
<article-id pub-id-type="doi">10.21105/joss.01132</article-id>
<title-group>
<article-title>GAMA: Genetic Automated Machine learning
Assistant</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7346-8075</contrib-id>
<string-name>Pieter Gijsbers</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7044-9805</contrib-id>
<string-name>Joaquin Vanschoren</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Eindhoven University of Technology</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2018-10-13">
<day>13</day>
<month>10</month>
<year>2018</year>
</pub-date>
<volume>4</volume>
<issue>33</issue>
<fpage>1132</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>AutoML</kwd>
<kwd>evolutionary algorithm</kwd>
<kwd>genetic programming</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Successful machine learning applications hinge on a plethora of
  design decisions, which require extensive experience and relentless
  empirical evaluation. To train a successful model, one has to decide
  which algorithms to use, how to preprocess the data, and how to tune
  any hyperparameters that influence the final model. Automating this
  process of algorithm selection and hyperparameter optimization in the
  context of machine learning is often called AutoML (Automated Machine
  Learning).</p>
  <p>The usefulness of AutoML is twofold. It makes creating good machine
  learning models more accessible to non-experts, as AutoML systems
  often support simple to use interfaces. For experts, it takes away a
  time consuming process of model selection and hyperparameter tuning.
  They can instead focus on related tasks such as interpreting the model
  or curating the data.</p>
  <p>GAMA is an AutoML package for end-users and AutoML researchers. It
  uses genetic programming to efficiently generate optimized machine
  learning pipelines given specific input data and resource constraints.
  A machine learning pipeline contains data preprocessing as well as a
  machine learning algorithm, with fine-tuned hyperparameter settings.
  By default, GAMA uses scikit-learn
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) implementations for preprocessing
  (e.g. Normalizer, PCA, ICA) and learners (e.g. Gaussian Naive Bayes,
  Support Vector Machine, Random Forest).</p>
  <p>GAMA can also combine multiple tuned machine learning pipelines
  together into an ensemble, which on average should help model
  performance. At the moment, GAMA is restricted to classification and
  regression problems on tabular data.</p>
  <p>In addition to its general use AutoML functionality, GAMA aims to
  serve AutoML researchers as well. During the optimization process,
  GAMA keeps an extensive log of progress made. Using this log, insight
  can be obtained on the behaviour of the population of pipelines. It
  can answer questions such as which mutation operator is most
  effective, how fitness changes over time, and how much time each
  algorithm takes. For example, Figure 1 shows fitness over time (in
  blue) and pipeline length over time (in red), and is (simplified)
  output that can be generated from the analysis log.</p>
  <fig>
    <caption><p>A simplified visualization of the optimization process
    based on log data. The blue line indicates the moving average of
    performance over time (left y-axis, higher is better). The red line
    represents the moving average of number of steps in the pipeline
    over time (right y axis, lower is better).</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="https://raw.githubusercontent.com/PGijsbers/gama/master/images/fitnessgraph.png" xlink:title="" />
  </fig>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>There are already many AutoML systems, both open-source and
  closed-source. Amongst these are auto-sklearn
  (<xref alt="Feurer et al., 2015" rid="ref-autosklearn" ref-type="bibr">Feurer
  et al., 2015</xref>), TPOT
  (<xref alt="Olson et al., 2016" rid="ref-TPOT" ref-type="bibr">Olson
  et al., 2016</xref>), ML-Plan
  (<xref alt="Mohr et al., 2018" rid="ref-MLPlan" ref-type="bibr">Mohr
  et al., 2018</xref>) and Auto-WEKA
  (<xref alt="Thornton et al., 2013" rid="ref-AutoWEKA" ref-type="bibr">Thornton
  et al., 2013</xref>). They differ in optimization strategy,
  programming language, target audience or underlying machine learning
  package.</p>
  <p>Most closely related to GAMA is TPOT
  (<xref alt="Olson et al., 2016" rid="ref-TPOT" ref-type="bibr">Olson
  et al., 2016</xref>). TPOT is also a Python package performing AutoML
  based on genetic programming using scikit-learn pipelines. The major
  difference between the two is the evolutionary algorithm used. TPOT
  uses a synchronous evolutionary algorithm, whereas GAMA uses an
  asynchronous algorithm. Other differences include having a CLI (TPOT),
  exporting independent Python code (TPOT), direct ARFF support (GAMA)
  and visualization of the optimization process (GAMA). Further
  comparison is contained in GAMA’s documentation.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This software was developed with support from the Data Driven
  Discovery of Models (D3M) program run by DARPA and the Air Force
  Research Laboratory.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-TPOT">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Olson</surname><given-names>Randal S.</given-names></name>
          <name><surname>Urbanowicz</surname><given-names>Ryan J.</given-names></name>
          <name><surname>Andrews</surname><given-names>Peter C.</given-names></name>
          <name><surname>Lavender</surname><given-names>Nicole A.</given-names></name>
          <name><surname>Kidd</surname><given-names>La Creis</given-names></name>
          <name><surname>Moore</surname><given-names>Jason H.</given-names></name>
        </person-group>
        <article-title>Applications of evolutionary computation: 19th european conference, EvoApplications 2016, porto, portugal, march 30 – april 1, 2016, proceedings, part i</article-title>
        <person-group person-group-type="editor">
          <name><surname>Squillero</surname><given-names>Giovanni</given-names></name>
          <name><surname>Burelli</surname><given-names>Paolo</given-names></name>
        </person-group>
        <publisher-name>Springer International Publishing</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <isbn>978-3-319-31204-0</isbn>
        <uri>http://dx.doi.org/10.1007/978-3-319-31204-0_9</uri>
        <pub-id pub-id-type="doi">10.1007/978-3-319-31204-0_9</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-autosklearn">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Feurer</surname><given-names>Matthias</given-names></name>
          <name><surname>Klein</surname><given-names>Aaron</given-names></name>
          <name><surname>Eggensperger</surname><given-names>Katharina</given-names></name>
          <name><surname>Springenberg</surname><given-names>Jost</given-names></name>
          <name><surname>Blum</surname><given-names>Manuel</given-names></name>
          <name><surname>Hutter</surname><given-names>Frank</given-names></name>
        </person-group>
        <article-title>Efficient and robust automated machine learning</article-title>
        <source>Advances in neural information processing systems 28</source>
        <person-group person-group-type="editor">
          <name><surname>Cortes</surname><given-names>C.</given-names></name>
          <name><surname>Lawrence</surname><given-names>N. D.</given-names></name>
          <name><surname>Lee</surname><given-names>D. D.</given-names></name>
          <name><surname>Sugiyama</surname><given-names>M.</given-names></name>
          <name><surname>Garnett</surname><given-names>R.</given-names></name>
        </person-group>
        <publisher-name>Curran Associates, Inc.</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <uri>http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-MLPlan">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mohr</surname><given-names>Felix</given-names></name>
          <name><surname>Wever</surname><given-names>Marcel</given-names></name>
          <name><surname>Hüllermeier</surname><given-names>Eyke</given-names></name>
        </person-group>
        <article-title>ML-plan: Automated machine learning via hierarchical planning</article-title>
        <source>Machine Learning</source>
        <year iso-8601-date="2018">2018</year>
        <volume>107</volume>
        <issue>8-10</issue>
        <uri>https://doi.org/10.1007/s10994-018-5735-z</uri>
        <pub-id pub-id-type="doi">10.1007/s10994-018-5735-z</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-AutoWEKA">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Thornton</surname><given-names>C.</given-names></name>
          <name><surname>Hutter</surname><given-names>F.</given-names></name>
          <name><surname>Hoos</surname><given-names>H. H.</given-names></name>
          <name><surname>Leyton-Brown</surname><given-names>K.</given-names></name>
        </person-group>
        <article-title>Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms</article-title>
        <source>Proc. Of KDD-2013</source>
        <year iso-8601-date="2013">2013</year>
      </element-citation>
    </ref>
    <ref id="ref-scikit-learn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
          <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
          <name><surname>Gramfort</surname><given-names>A.</given-names></name>
          <name><surname>Michel</surname><given-names>V.</given-names></name>
          <name><surname>Thirion</surname><given-names>B.</given-names></name>
          <name><surname>Grisel</surname><given-names>O.</given-names></name>
          <name><surname>Blondel</surname><given-names>M.</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
          <name><surname>Weiss</surname><given-names>R.</given-names></name>
          <name><surname>Dubourg</surname><given-names>V.</given-names></name>
          <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
          <name><surname>Passos</surname><given-names>A.</given-names></name>
          <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
          <name><surname>Brucher</surname><given-names>M.</given-names></name>
          <name><surname>Perrot</surname><given-names>M.</given-names></name>
          <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
