<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1799</article-id>
<article-id pub-id-type="doi">10.21105/joss.01799</article-id>
<title-group>
<article-title>PyUoI: The Union of Intersections Framework in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6809-2437</contrib-id>
<string-name>Pratik S. Sachdeva</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0494-8758</contrib-id>
<string-name>Jesse A. Livezey</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-1617-449X</contrib-id>
<string-name>Andrew J. Tritt</string-name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-1974-4603</contrib-id>
<string-name>Kristofer E. Bouchard</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-4"/>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Redwood Center for Theoretical Neuroscience, University of
California, Berkeley, Berkeley, California, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Physics, University of California, Berkeley,
Berkeley, California, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Biological Systems and Engineering Division, Lawrence
Berkeley National Laboratory, Berkeley, California, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Computational Research Division, Lawrence Berkeley National
Laboratory, Berkeley, California, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Helen Wills Neuroscience Institute, University of
California, Berkeley, Berkeley, California, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-10-05">
<day>5</day>
<month>10</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>44</issue>
<fpage>1799</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>generalized linear models</kwd>
<kwd>dimensionality reduction</kwd>
<kwd>sparsity</kwd>
<kwd>interpretability</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The increasing size and complexity of scientific data requires
  statistical analysis methods that scale and produce models that are
  both interpretable and predictive. Interpretability implies one can
  interpret the output of the model in terms of processes generating the
  data
  (<xref alt="Murdoch et al., 2019" rid="ref-murdoch2019" ref-type="bibr">Murdoch
  et al., 2019</xref>). This typically requires identification of a
  small number of features in the actual data and accurate estimation of
  their contributions
  (<xref alt="Bickel et al., 2006" rid="ref-bickel2006" ref-type="bibr">Bickel
  et al., 2006</xref>). Meanwhile, achieving predictive power requires
  optimizing the performance of some statistical measure such as
  precision, mean squared error, etc. Across inference procedures, there
  is often a trade-off between interpretability and predictive power.
  The impact of this trade-off is particularly acute for scientific
  applications, where the output of the model is used to provide insight
  into the underlying physical processes that generated the data.</p>
  <p>We recently introduced Union of Intersections (UoI), a flexible,
  modular, and scalable framework designed to enhance both the
  identification of features (model selection) as well as the estimation
  of the contributions of these features (model estimation)
  (<xref alt="Bouchard et al., 2017" rid="ref-bouchard2017" ref-type="bibr">Bouchard
  et al., 2017</xref>). UoI-based methods leverage stochastic data
  resampling and a range of sparsity-inducing regularization parameters
  to build families of potential feature sets robust to perturbations of
  the data, and then average nearly unbiased parameter estimates of
  selected features to maximize predictive accuracy. Models inferred
  through the UoI framework are characterized by their usage of fewer
  parameters with little or no loss in predictive accuracy, and reduced
  bias relative to benchmark approaches.</p>
  <p><monospace>PyUoI</monospace> is a Python package containing
  implementations of a variety of UoI-based algorithms, encompassing
  regression, classification, and dimensionality reduction. In order to
  better facilitate its usage, <monospace>PyUoI</monospace>‚Äôs API is
  structured similarly to the <monospace>scikit-learn</monospace>
  package, which is a commonly used Python machine learning library
  (<xref alt="Buitinck et al., 2013" rid="ref-sklearn_api" ref-type="bibr">Buitinck
  et al., 2013</xref>;
  <xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>).</p>
  <p>The UoI framework operates by fitting many models across resamples
  of the dataset and across a set of regularization parameters. Since
  these fits can be performed in parallel, the UoI framework is
  naturally scalable. <monospace>PyUoI</monospace> is equipped with
  <monospace>mpi4py</monospace> functionality to parallelize model
  fitting on large datasets
  (<xref alt="Dalcƒ±ÃÅn et al., 2005" rid="ref-dalcin2005" ref-type="bibr">Dalcƒ±ÃÅn
  et al., 2005</xref>).</p>
</sec>
<sec id="background">
  <title>Background</title>
  <p>The Union of Intersections is not a single method or algorithm, but
  a flexible statistical framework into which other algorithms can be
  inserted. In this section, we briefly describe UoI<sub>Lasso</sub>,
  the UoI implementation of lasso penalized regression.
  UoI<sub>Lasso</sub> is similar in structure to the UoI versions of
  other generalized linear models (logistic and poisson). We refer the
  user to existing literature on the UoI variants of column subset
  selection and non-negative matrix factorization
  (<xref alt="Bouchard et al., 2017" rid="ref-bouchard2017" ref-type="bibr">Bouchard
  et al., 2017</xref>;
  <xref alt="Ubaru et al., 2017" rid="ref-ubaru2017" ref-type="bibr">Ubaru
  et al., 2017</xref>).</p>
  <p>Linear regression consists of estimating parameters
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\beta} \in \mathbb{R}^{p\times 1}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle><mml:mo>‚àà</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>‚Ñù</mml:mi></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>√ó</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  that map a <inline-formula><alternatives>
  <tex-math><![CDATA[p]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>p</mml:mi></mml:math></alternatives></inline-formula>-dimensional
  vector of features <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{x} \in \mathbb{R}^{p \times 1}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>ùê±</mml:mi></mml:mstyle><mml:mo>‚àà</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>‚Ñù</mml:mi></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>√ó</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  to the observation variable <inline-formula><alternatives>
  <tex-math><![CDATA[y\in \mathbb{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>y</mml:mi><mml:mo>‚àà</mml:mo><mml:mstyle mathvariant="double-struck"><mml:mi>‚Ñù</mml:mi></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>,
  when the <inline-formula><alternatives>
  <tex-math><![CDATA[N]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
  samples are corrupted by i.i.d Gaussian noise:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[y = \boldsymbol{\beta}^T \mathbf{x} + \epsilon]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>ùê±</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>œµ</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[\epsilon \sim \mathcal{N}(0, \sigma^2)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>œµ</mml:mi><mml:mo>‚àº</mml:mo><mml:mstyle mathvariant="script"><mml:mi>ùí©</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  for each sample. When the true <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\beta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  is thought to be sparse (i.e., some subset of the
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\beta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  are exactly zero), an estimate of <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\beta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  can be found by solving a constrained optimization problem of the
  form</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}\in \mathbb{R}^p}{\text{argmin}}
                  \frac{1}{N}\sum_{i=1}^N(y_i - \boldsymbol{\beta}^T \mathbf{x}_i)^2
                  + \lambda |\boldsymbol{\beta}|_1]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mtext mathvariant="normal">argmin</mml:mtext><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle><mml:mo>‚àà</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>‚Ñù</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚àí</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>ùê±</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>Œª</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[|\boldsymbol{\beta}|_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  is the <inline-formula><alternatives>
  <tex-math><![CDATA[\ell_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>‚Ñì</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>-norm
  of the parameters and <inline-formula><alternatives>
  <tex-math><![CDATA[i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>
  indexes data samples. The <inline-formula><alternatives>
  <tex-math><![CDATA[\ell_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>‚Ñì</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>-norm
  is a convenient penalty because it will tend to force parameters to be
  set exactly equal to zero, performing feature selection
  (<xref alt="Tibshirani, 1994" rid="ref-tibshirani1994" ref-type="bibr">Tibshirani,
  1994</xref>). Typically, <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>,
  the degree to which feature sparsity is enforced, is unknown and must
  be determined through cross-validation or a penalized score function
  across a set of hyperparameters <inline-formula><alternatives>
  <tex-math><![CDATA[\left\{\lambda_j\right\}_{j=1}^k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">{</mml:mo><mml:msub><mml:mi>Œª</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>.</p>
  <p>The key mathematical idea underlying UoI is to perform model
  selection through intersection (compressive) operations and model
  estimation through union (expansive) operations, in that order. This
  separation of parameter selection and estimation provides selection
  profiles that are more robust and parameter estimates that have less
  bias. This can be contrasted with a typical Lasso fit wherein
  parameter selection and estimation are performed simultaneously. The
  Lasso procedure can lead to selection profiles that are not robust to
  data resampling and estimates that are biased by the penalty on
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\beta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>ùõÉ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.
  For UoI<sub>Lasso</sub>, the procedure is as follows (see Algorithm 1
  for a more detailed pseudocode):</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Model Selection:</bold> For each
      <inline-formula><alternatives>
      <tex-math><![CDATA[\lambda_j]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Œª</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      in the Lasso path, generate estimates on
      <inline-formula><alternatives>
      <tex-math><![CDATA[N_S]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      resamples of the data (Line 2). The support
      <inline-formula><alternatives>
      <tex-math><![CDATA[S_j]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      (i.e., the set of non-zero parameters) for
      <inline-formula><alternatives>
      <tex-math><![CDATA[\lambda_j]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Œª</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      consists of the features that persist in all model fits across the
      resamples (i.e., through an intersection) (Line 7).</p>
    </list-item>
    <list-item>
      <p><bold>Model Estimation:</bold> For each support
      <inline-formula><alternatives>
      <tex-math><![CDATA[S_j]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
      perform Ordinary Least Squares (OLS) on
      <inline-formula><alternatives>
      <tex-math><![CDATA[N_E]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      resamples of the data. The final model is obtained by averaging
      (i.e., taking the union) across the supports chosen according to
      some model selection criteria for each resample (Lines 15-16). The
      model selection criteria can be prediction quality on held-out
      data or penalized likelihood methods (e.g., AIC or BIC).</p>
    </list-item>
  </list>
  <p>Thus, the selection module ensures that, for each
  <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda_j]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Œª</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
  only features that are stable to perturbations in the data (resamples)
  are allowed in the support <inline-formula><alternatives>
  <tex-math><![CDATA[S_j]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>.
  This provides a family of resample-stable model supports with varying
  levels of sparsity due to <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda_j]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Œª</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  that can be used in estimation. Then, the estimation module ensures
  that the most predictive supports per resample are averaged together
  in the final model. The estimation module uses OLS rather than Lasso
  to provide parameter estimates with low bias. The degree of feature
  compression via intersections (quantified by
  <inline-formula><alternatives>
  <tex-math><![CDATA[N_S]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:math></alternatives></inline-formula>)
  and the degree of feature expansion via unions (quantified by
  <inline-formula><alternatives>
  <tex-math><![CDATA[N_E]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></alternatives></inline-formula>)
  can be balanced to maximize prediction accuracy for the response
  variable <inline-formula><alternatives>
  <tex-math><![CDATA[y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <p><monospace>PyUoI</monospace> is split up into two modules, with the
  following UoI algorithms:</p>
  <list list-type="bullet">
    <list-item>
      <p><monospace>linear_model</monospace> (generalized linear
      models)</p>
      <list list-type="bullet">
        <list-item>
          <p>Lasso penalized linear regression UoI<sub>Lasso</sub>.</p>
        </list-item>
        <list-item>
          <p>Elastic-net penalized linear regression
          (UoI<sub>ElasticNet</sub>).</p>
        </list-item>
        <list-item>
          <p>Logistic regression (Bernoulli and multinomial)
          (UoI<sub>Logistic</sub>).</p>
        </list-item>
        <list-item>
          <p>Poisson regression (UoI<sub>Poisson</sub>).</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p><monospace>decomposition</monospace> (dimensionality
      reduction)</p>
      <list list-type="bullet">
        <list-item>
          <p>Column subset selection (UoI<sub>CSS</sub>).</p>
        </list-item>
        <list-item>
          <p>Non-negative matrix factorization (UoI<sub>NMF</sub>).</p>
        </list-item>
      </list>
    </list-item>
  </list>
  <p>The generalized linear models we have implemented include the most
  commonly used models in a variety of scientific disciplines,
  particularly in the fields of neuroscience and genomics. Extensions to
  other generalized linear models (e.g., negative binomial regression,
  gamma regression, etc.) are left as future work. However, given the
  inheritance structure of the <monospace>PyUoI</monospace> framework,
  these extensions should be straightforward for the interested
  user.</p>
  <p>Similar to <monospace>scikit-learn</monospace>, each UoI algorithm
  has its own Python class. Instantiations of these classes are created
  with specific hyperparameters and are fit to user-provided datasets.
  The hyperparameters allow the user to fine-tune the number of
  resamples, fraction of data in each resample, and the model selection
  criteria used in the estimation module (in Algorithm 1, test set
  accuracy is used, but the Akaike and Bayesian Information Criteria are
  also available
  (<xref alt="Akaike, 1998" rid="ref-akaike1998" ref-type="bibr">Akaike,
  1998</xref>;
  <xref alt="Schwarz, 1978" rid="ref-schwarz1978" ref-type="bibr">Schwarz,
  1978</xref>)).</p>
  <p>Additionally, UoI is agnostic to the specific solver used for a
  given model. That is, the UoI framework operates on fits obtained from
  performing the optimization for a specified model (such as the lasso
  optimization problem for linear regression). In the case of
  <monospace>PyUoI</monospace>, the generalized linear models come
  equipped with a coordinate descent solver (from
  <monospace>scikit-learn</monospace>), a built-in Orthant-Wise Limited
  memory Quasi-Newton solver
  (<xref alt="Gong &amp; Ye, 2015" rid="ref-gong2015" ref-type="bibr">Gong
  &amp; Ye, 2015</xref>), and the <monospace>pycasso</monospace> solver
  (<xref alt="Ge, 2019" rid="ref-ge2019" ref-type="bibr">Ge,
  2019</xref>). The choice of solver is left to the user as a
  hyperparameter. If a different solver is desired,
  <monospace>PyUoI</monospace> could be extended by the user to utilize
  this solver in a straightforward manner.</p>
</sec>
<sec id="applications">
  <title>Applications</title>
  <p>We have used <monospace>PyUoI</monospace> largely in the realm of
  neuroscience and genomics
  (<xref alt="Bouchard et al., 2017" rid="ref-bouchard2017" ref-type="bibr">Bouchard
  et al., 2017</xref>;
  <xref alt="Ubaru et al., 2017" rid="ref-ubaru2017" ref-type="bibr">Ubaru
  et al., 2017</xref>). A few applications include:</p>
  <list list-type="bullet">
    <list-item>
      <p>Interpretable functional connectivity networks from neural
      populations in the visual, auditory, and motor cortices of various
      animal models;</p>
    </list-item>
    <list-item>
      <p>Sparse decoding of behavioral activity from spiking neural
      activity;</p>
    </list-item>
    <list-item>
      <p>Parts-based decomposition of electrocorticography recordings in
      rat auditory cortex that reflect functional cortical
      organization;</p>
    </list-item>
    <list-item>
      <p>Extraction of characteristic single nucleotide polymorphisms
      for the prediction of phenotypes in mice.</p>
    </list-item>
  </list>
  <p>However, the algorithms implemented in <monospace>PyUoI</monospace>
  are broadly applicable to problems where enforcement of sparsity at
  minimal cost to bias are desired.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank the contributors to earlier versions of this software.
  P.S.S. was supported by the Department of Defense (DoD) through the
  National Defense Science &amp; Engineering Graduate Fellowship (NDSEG)
  Program. K.E.B. and J.A.L. were supported through the Lawrence
  Berkeley National Laboratory-internal LDRD ‚ÄúDeep Learning for Science‚Äù
  led by Prabhat. A.J.T. was supported by the Department of Energy
  project ‚ÄúCo-design for artificial intelligence coupled with computing
  at scale for extremely large, complex datasets.‚Äù K.E.B. was funded by
  Lawrence Berkeley National Laboratory-internal LDRD
  ‚ÄúNeuro/Nano-Technology for BRAIN‚Äù led by Peter Denes.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-bouchard2017">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Bouchard</surname><given-names>Kristofer</given-names></name>
          <name><surname>Bujan</surname><given-names>Alejandro</given-names></name>
          <name><surname>Roosta-Khorasani</surname><given-names>Farbod</given-names></name>
          <name><surname>Ubaru</surname><given-names>Shashanka</given-names></name>
          <name><surname>Prabhat</surname><given-names>Mr.</given-names></name>
          <name><surname>Snijders</surname><given-names>Antoine</given-names></name>
          <name><surname>Mao</surname><given-names>Jian-Hua</given-names></name>
          <name><surname>Chang</surname><given-names>Edward</given-names></name>
          <name><surname>Mahoney</surname><given-names>Michael W</given-names></name>
          <name><surname>Bhattacharya</surname><given-names>Sharmodeep</given-names></name>
        </person-group>
        <article-title>Union of Intersections (UoI) for interpretable data driven discovery and prediction</article-title>
        <source>Advances in Neural Information Processing Systems 30</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-ubaru2017">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Ubaru</surname><given-names>S.</given-names></name>
          <name><surname>Wu</surname><given-names>K.</given-names></name>
          <name><surname>Bouchard</surname><given-names>K. E.</given-names></name>
        </person-group>
        <article-title>UoI-NMF cluster: A robust nonnegative matrix factorization algorithm for improved parts-based decomposition and reconstruction of noisy data</article-title>
        <source>2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)</source>
        <year iso-8601-date="2017">2017</year>
        <volume></volume>
        <pub-id pub-id-type="doi">10.1109/ICMLA.2017.0-152</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-tibshirani1994">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
        </person-group>
        <article-title>Regression shrinkage and selection via the lasso</article-title>
        <source>Journal of the Royal Statistical Society, Series B</source>
        <year iso-8601-date="1994">1994</year>
        <volume>58</volume>
      </element-citation>
    </ref>
    <ref id="ref-bickel2006">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bickel</surname><given-names>Peter J.</given-names></name>
          <name><surname>Li</surname><given-names>Bo</given-names></name>
          <name><surname>Tsybakov</surname><given-names>Alexandre B.</given-names></name>
          <name><surname>Geer</surname><given-names>Sara A. van de</given-names></name>
          <name><surname>Yu</surname><given-names>Bin</given-names></name>
          <name><surname>Vald√©s</surname><given-names>Te√≥filo</given-names></name>
          <name><surname>Rivero</surname><given-names>Carlos</given-names></name>
          <name><surname>Fan</surname><given-names>Jianqing</given-names></name>
          <name><surname>Vaart</surname><given-names>Aad van der</given-names></name>
        </person-group>
        <article-title>Regularization in statistics</article-title>
        <source>Test</source>
        <year iso-8601-date="2006">2006</year>
        <volume>15</volume>
        <pub-id pub-id-type="doi">10.1007/BF02607055</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-sklearn_api">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Buitinck</surname><given-names>Lars</given-names></name>
          <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
          <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
          <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
          <name><surname>Mueller</surname><given-names>Andreas</given-names></name>
          <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
          <name><surname>Niculae</surname><given-names>Vlad</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
          <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
          <name><surname>Grobler</surname><given-names>Jaques</given-names></name>
          <name><surname>Layton</surname><given-names>Robert</given-names></name>
          <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
          <name><surname>Joly</surname><given-names>Arnaud</given-names></name>
          <name><surname>Holt</surname><given-names>Brian</given-names></name>
          <name><surname>Varoquaux</surname><given-names>Ga√´l</given-names></name>
        </person-group>
        <article-title>API design for machine learning software: Experiences from the scikit-learn project</article-title>
        <source>ECML PKDD Workshop: Languages for Data Mining and Machine Learning</source>
        <year iso-8601-date="2013">2013</year>
      </element-citation>
    </ref>
    <ref id="ref-gong2015">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Gong</surname><given-names>Pinghua</given-names></name>
          <name><surname>Ye</surname><given-names>Jieping</given-names></name>
        </person-group>
        <article-title>A modified orthant-wise limited memory quasi-newton method with convergence analysis</article-title>
        <source>Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37</source>
        <year iso-8601-date="2015">2015</year>
      </element-citation>
    </ref>
    <ref id="ref-ge2019">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Ge</surname><given-names>Jason</given-names></name>
        </person-group>
        <article-title>PICASSO: PathwIse CalibrAted sparse shooting algOrithm</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/jasonge27/picasso; GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-murdoch2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Murdoch</surname><given-names>W. James</given-names></name>
          <name><surname>Singh</surname><given-names>Chandan</given-names></name>
          <name><surname>Kumbier</surname><given-names>Karl</given-names></name>
          <name><surname>Abbasi-Asl</surname><given-names>Reza</given-names></name>
          <name><surname>Yu</surname><given-names>Bin</given-names></name>
        </person-group>
        <article-title>Definitions, methods, and applications in interpretable machine learning</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <publisher-name>National Academy of Sciences</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>116</volume>
        <issue>44</issue>
        <pub-id pub-id-type="doi">10.1073/pnas.1900654116</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-dalcin2005">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dalcƒ±ÃÅn</surname><given-names>Lisandro</given-names></name>
          <name><surname>Paz</surname><given-names>Rodrigo</given-names></name>
          <name><surname>Storti</surname><given-names>Mario</given-names></name>
        </person-group>
        <article-title>MPI for Python</article-title>
        <source>Journal of Parallel and Distributed Computing</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2005">2005</year>
        <volume>65</volume>
        <issue>9</issue>
      </element-citation>
    </ref>
    <ref id="ref-scikit-learn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
          <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
          <name><surname>Gramfort</surname><given-names>A.</given-names></name>
          <name><surname>Michel</surname><given-names>V.</given-names></name>
          <name><surname>Thirion</surname><given-names>B.</given-names></name>
          <name><surname>Grisel</surname><given-names>O.</given-names></name>
          <name><surname>Blondel</surname><given-names>M.</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
          <name><surname>Weiss</surname><given-names>R.</given-names></name>
          <name><surname>Dubourg</surname><given-names>V.</given-names></name>
          <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
          <name><surname>Passos</surname><given-names>A.</given-names></name>
          <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
          <name><surname>Brucher</surname><given-names>M.</given-names></name>
          <name><surname>Perrot</surname><given-names>M.</given-names></name>
          <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
      </element-citation>
    </ref>
    <ref id="ref-akaike1998">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Akaike</surname><given-names>Hirotogu</given-names></name>
        </person-group>
        <article-title>Information theory and an extension of the maximum likelihood principle</article-title>
        <source>Selected papers of Hirotugu Akaike</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="1998">1998</year>
        <pub-id pub-id-type="doi">10.1007/978-1-4612-1694-0_15</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-schwarz1978">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schwarz</surname></name>
        </person-group>
        <article-title>Estimating the dimension of a model</article-title>
        <source>The Annals of Statistics</source>
        <publisher-name>Institute of Mathematical Statistics</publisher-name>
        <year iso-8601-date="1978">1978</year>
        <volume>6</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
