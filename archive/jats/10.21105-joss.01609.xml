<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1609</article-id>
<article-id pub-id-type="doi">10.21105/joss.01609</article-id>
<title-group>
<article-title>infotheory: A C++/Python package for multivariate
information theoretic analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9254-8641</contrib-id>
<string-name>Madhavun Candadai</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-2008-290X</contrib-id>
<string-name>Eduardo J. Izquierdo</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Cognitive Science Program, Indiana University, Bloomington,
IN, U.S.A.</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>School of Informatics, Computing, and Engineering, Indiana
University, Bloomington, IN, U.S.A.</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-07-03">
<day>3</day>
<month>7</month>
<year>2019</year>
</pub-date>
<volume>5</volume>
<issue>47</issue>
<fpage>1609</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>information theory</kwd>
<kwd>multivariate data analysis</kwd>
<kwd>entropy</kwd>
<kwd>mutual information</kwd>
<kwd>partial information decomposition</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>This paper introduces <monospace>infotheory</monospace>: a package
  written in C++ and usable from Python and C++, for multivariate
  information theoretic analyses of discrete and continuous data. It is
  open-source
  (<ext-link ext-link-type="uri" xlink:href="https://git.io/infot">https://git.io/infot</ext-link>)
  and details on how to install it and use it are available on its
  <ext-link ext-link-type="uri" xlink:href="http://mcandadai.com/infotheory/">website</ext-link>.
  This package allows the user to study the relationship between
  components of a complex system simply from the data recorded during
  its operation, using the tools of information theory. Specifically,
  this package enables the measurement of entropy and mutual
  information, and also allows the user to perform partial information
  decomposition of mutual information into unique, redundant, and
  synergistic information quantities.</p>
</sec>
<sec id="background">
  <title>Background</title>
  <p>Information theory was first introduced by Claude Shannon in his
  seminal paper “A mathematical theory of communication” as a
  methodology to develop efficient coding and communication of data
  across noisy channels
  (<xref alt="Shannon, 1948" rid="ref-shannon1948" ref-type="bibr">Shannon,
  1948</xref>). Its popularity can be primarily attributed to its
  ability to be applied in any domain, ranging from Economics to
  Neuroscience. Information theory provides a general framework to
  quantify stochastic properties (uncertainty in the outcome of an
  experiment) and relationships (mutual information that one variable
  provides about another) between different variables in a system of
  interest. It provides tools to measure these quantities in a way that
  is invariant to the scale of the system and allows comparison across
  systems.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Until relatively recently, information theory had been employed to
  study n-dimensional multivariate systems two variables at a time
  (bivariate). However, all natural systems are multivariate and a
  scientific inquiry into their operation requires understanding how
  these multiple variables interact. In a multivariate system, bivariate
  measures such as pairwise mutual information alone are insufficient to
  capture the polyadic interactions between the different variables
  (<xref alt="R. James &amp; Crutchfield, 2017" rid="ref-james2017" ref-type="bibr">R.
  James &amp; Crutchfield, 2017</xref>). Partial Information
  Decomposition (PID) is an extension of Shanon information measures
  that allows us to study the interaction between variables in a
  multivariate system by decomposing the total information that multiple
  source variables provide about a target variable into its constituent
  non-negative components
  (<xref alt="Williams &amp; Beer, 2010" rid="ref-williams2010" ref-type="bibr">Williams
  &amp; Beer, 2010</xref>). More specifically, in a trivariate case, the
  three variables can be separated into one target and two source
  variables. The total information that the two sources have about the
  target is given by the bivariate mutual information between the
  concatenated sources as one variable and the target. Using PID, the
  dependencies between the sources can be studied by decomposing this
  total information into the following non-negative components:
  information that each source uniquely provides about the target,
  information that they redundantly provide, and the synergistic
  information that is only available when both sources are known. There
  have been multiple approaches proposed to perform said decomposition
  (<xref alt="Bertschinger et al., 2014" rid="ref-bertschinger2014" ref-type="bibr">Bertschinger
  et al., 2014</xref>;
  <xref alt="Griffith &amp; Koch, 2014" rid="ref-griffith2014" ref-type="bibr">Griffith
  &amp; Koch, 2014</xref>;
  <xref alt="R. G. James, Emenheiser, et al., 2018" rid="ref-james2018a" ref-type="bibr">R.
  G. James, Emenheiser, et al., 2018</xref>;
  <xref alt="Williams &amp; Beer, 2010" rid="ref-williams2010" ref-type="bibr">Williams
  &amp; Beer, 2010</xref>). Here we focus on the approach proposed by
  (<xref alt="Williams &amp; Beer, 2010" rid="ref-williams2010" ref-type="bibr">Williams
  &amp; Beer, 2010</xref>) primarily because this package implements PID
  for two and three source decomposition, and as of now, this is the
  only approach that guarantees non-negative decomposition for the four
  variable case (one target and three sources). Multivariate analysis
  allows us to ask more detailed questions such as, what is the amount
  of information that is uniquely provided about a target random
  variable by one source and not another? and what is the amount of
  information that is transferred from one random process X to another Y
  over and above Y’s own information from its past? These questions
  enable us to understand the interactions between different components
  of a complex system, thereby leading us towards an understanding of
  its operation given just the observed data from the system.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <p><monospace>infotheory</monospace> implements widely used measures
  such as entropy and mutual information
  (<xref alt="Cover &amp; Thomas, 2012" rid="ref-cover2012" ref-type="bibr">Cover
  &amp; Thomas, 2012</xref>), as well as more recent measures that arise
  from multivariate extensions to information theory. As such, the tool
  has been designed to be easy to use and is ideal for pedagogical
  demonstrations of information theory as well as in research. Here, we
  highlight seven key aspects of its implementation that make our
  package a valuable addition to any information theoretic analyses
  tools set. First, the package is written in C++. One of the main
  challenges of multivariate analyses on a large, complex system is the
  amount of computations involved. Implementation in C++ makes the
  package efficient. Second, the package can be used from either C++ or
  Python. Python wrapping allows for ease of use, as well as
  compatibility with other powerful open-source libraries such as numpy.
  Third, the API allows adding the data only once to then perform
  various analyses across different sub-spaces of the dataset cheaply.
  Fourth, a custom sparse data structure is used to represent the random
  variables. This allows the package to work easily with large amounts
  of data. Fifth, to better estimate the data distribution in case of
  continuous variables, the package employs a kernel-based density
  estimation method called ‘averaged shifted histograms’ because of its
  beneficial trade-off between computational and statistical efficiency
  (<xref alt="Scott, 1985" rid="ref-scott1985" ref-type="bibr">Scott,
  1985</xref>). Sixth, the package includes user-controllable
  specification of binning. This is essential for estimating
  distributions on hybrid systems with a mix of continuous and discrete
  variables. Finally, this package implements decomposition of
  information in three as well as four variable systems, making it
  unique among related existing packages.</p>
  <p>One of the major challenges in utilizing information theoretic
  measures in experimental settings is the availability of sufficient
  data to infer the data distributions correctly
  (<xref alt="Paninski, 2003" rid="ref-paninski2003estimation" ref-type="bibr">Paninski,
  2003</xref>). To estimate data distribution from limited data, we have
  employed average shifted histograms for its beneficial trade-off
  between statistical and computational efficiency
  (<xref alt="Scott, 1985" rid="ref-scott1985" ref-type="bibr">Scott,
  1985</xref>). This involves discretizing the data space into a number
  of bins and estimating frequentist probabilities based on the bins
  occupied by data samples. To reduce the impact of arbitrarily chosen
  bin boundaries, the data distribution is estimated by averaging the
  bin occupancies across multiple shifted binnings of the data space.
  This binning-based estimator has been shown to approximate a triangle
  kernel estimator
  (<xref alt="Scott, 1985" rid="ref-scott1985" ref-type="bibr">Scott,
  1985</xref>). While the binning provides significant computational
  advantages, its approximation errors must be considered. Bias
  properties and guidelines for choosing the parameters for average
  shifted histograms are given in Fernando et al.
  (<xref alt="2009" rid="ref-fernando2009selection" ref-type="bibr">2009</xref>).
  For a moderate sample size, five to 10 shifted histograms has been
  shown to be adequate
  (<xref alt="Scott, 1985" rid="ref-scott1985" ref-type="bibr">Scott,
  1985</xref>). In general, average shifted histograms are best suited
  for noisy continuous data where the distribution of the data is
  unknown. For a more involved discussion on density estimation and its
  bias properties, we point the reader to
  (<xref alt="Scott &amp; Terrell, 1987" rid="ref-scott1987biased" ref-type="bibr">Scott
  &amp; Terrell, 1987</xref>) and
  (<xref alt="Wand &amp; Jones, 1994" rid="ref-wand1994kernel" ref-type="bibr">Wand
  &amp; Jones, 1994</xref>).</p>
  <p>Two existing packages that are most similar to ours are dit
  (<xref alt="R. G. James, Ellison, et al., 2018" rid="ref-james2018b" ref-type="bibr">R.
  G. James, Ellison, et al., 2018</xref>) and IDTxL
  (<xref alt="Wollstadt et al., 2019" rid="ref-wollstadt12019" ref-type="bibr">Wollstadt
  et al., 2019</xref>). Unlike dit, our package can also help analyze
  continuous-valued data and unlike dit and IDTxL we have implemented
  PID analysis of four variables: three sources and one target. Other
  notable packages include: pyentropy
  (<xref alt="Ince et al., n.d." rid="ref-inceinformation" ref-type="bibr">Ince
  et al., n.d.</xref>), which was primarily designed for estimating
  entropies; and JDIT
  (<xref alt="Lizier, 2014" rid="ref-lizier2014jidt" ref-type="bibr">Lizier,
  2014</xref>), which was primarily designed for measuring transfer
  entropy. Neither pyentropy nor JDIT implement PID measures, although
  authors of JDIT have an unpublished GitHub repository that has a Java
  implementation of PID called
  <ext-link ext-link-type="uri" xlink:href="https://github.com/jlizier/jpid">JPID</ext-link>.
  In comparison with these existing packages and their functionality,
  our package primarily focuses on measuring multivariate informational
  quantities on continuous data where the data distribution is not known
  a priori. However, it can still be used with discrete data using the
  same methods.</p>
  <p>The functions implementing the above-mentioned information
  theoretic measures have been designed to be flexibly used in
  alternative ways. For instance, the decomposed information components
  can be combined to measure transfer entropy
  (<xref alt="Schreiber, 2000" rid="ref-schreiber2000" ref-type="bibr">Schreiber,
  2000</xref>). When dealing with time-series data, one can restructure
  the data such that the two sources are past values of two random
  variables, and the target is a future value of one of them. It has
  been shown that the sum of the unique information that a source
  provides about the target (future value) and the synergistic
  information from both sources is equal to the amount of information
  transferred from that source
  (<xref alt="Williams &amp; Beer, 2011" rid="ref-williams2011" ref-type="bibr">Williams
  &amp; Beer, 2011</xref>). Transfer entropy is used extensively in
  neuroscience to infer directed functional connections between nodes of
  a network (nodes can be neurons, brain regions or EEG electrodes) from
  recorded data
  (<xref alt="Wibral et al., 2014" rid="ref-wibral2014" ref-type="bibr">Wibral
  et al., 2014</xref>). Another instance of extended use of this package
  is to measure changes in information in time. Again, with time-series
  data, if the user provides all data over all time points, then they
  can ask the tool to calculate all the previously discussed measures as
  aggregate values over time. Alternatively, the user can provide data
  that are only from a specific time point, calculate the information
  theoretic measures for that time point, and then repeat the analyses
  over the entire time course. Such analysis reveals how information in
  the variables of the system change dynamically during the course of
  its operation
  (<xref alt="Beer &amp; Williams, 2015" rid="ref-beer2015" ref-type="bibr">Beer
  &amp; Williams, 2015</xref>;
  <xref alt="Izquierdo et al., 2015" rid="ref-izquierdo2015" ref-type="bibr">Izquierdo
  et al., 2015</xref>). Both extensions are easily accessible by reusing
  the existing mutual information and PID functions in the package and
  providing different subsets of the data accordingly.</p>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>Altogether, <monospace>infotheory</monospace> provides an
  easy-to-use and flexible tool for performing information theoretic
  analyses on any multivariate dataset consisting of discrete or
  continuous data. Application areas are, in principle, as wide as that
  of information theory’s - any domain that has a multivariate system
  and aims to study how the different components interact. We are
  particularly encouraged by the potential applications in neuroscience,
  at all scales ranging from individual neurons to brain regions to
  integrated brain-body-environment systems. In our group, we are
  currently using this package to understand the flow of information in
  simulated neural circuits capable of producing behavior. This tool
  allows us to easily analyze how different neurons of a circuit or
  regions in the brain are encoding information about the sensory
  stimulus it is receiving, the actions it is producing, or indeed about
  other neurons/regions within the system itself. We are using
  multivariate measures to analyze how different nodes in the circuit
  encode information uniquely, redundantly, and synergistically about a
  signal of interest. We are using the tool to study information
  dynamics of the neural circuit over time during behavior. We are also
  using it to infer directed functional connections between the nodes of
  the network. Besides its use in research, we are using this package
  for pedagogical purposes to introduce students to information theory.
  As such, we have provided a number of benchmarks and examples in the
  <ext-link ext-link-type="uri" xlink:href="http://mcandadai.com/infotheory/measures.html">website</ext-link>.
  We also hope to continue to extend the package in the future by, for
  example, implementing additional approaches to multivariate
  information analyses, and providing GPU-support. Finally, in the
  spirit of free and open-source software development, we also welcome
  contributions from others.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The work in this paper was supported in part by NSF grant
  No. IIS-1524647. M.C. was funded by an assistantship from the
  Cognitive Science Program, Indiana University, Bloomington. The
  authors would like to thank Randall Beer for VectorMatrix, the C++
  vector libraries used in this package.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-shannon1948">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shannon</surname><given-names>Claude Elwood</given-names></name>
        </person-group>
        <article-title>A mathematical theory of communication</article-title>
        <source>Bell System Technical Journal</source>
        <publisher-name>Wiley Online Library</publisher-name>
        <year iso-8601-date="1948">1948</year>
        <volume>27</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-cover2012">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Cover</surname><given-names>Thomas M</given-names></name>
          <name><surname>Thomas</surname><given-names>Joy A</given-names></name>
        </person-group>
        <source>Elements of information theory</source>
        <publisher-name>John Wiley &amp; Sons</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <pub-id pub-id-type="doi">10.1002/047174882X</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-james2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>James</surname><given-names>Ryan</given-names></name>
          <name><surname>Crutchfield</surname><given-names>James</given-names></name>
        </person-group>
        <article-title>Multivariate dependence beyond Shannon information</article-title>
        <source>Entropy</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>19</volume>
        <issue>10</issue>
        <pub-id pub-id-type="doi">10.3390/e19100531</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-williams2010">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Williams</surname><given-names>Paul L</given-names></name>
          <name><surname>Beer</surname><given-names>Randall D</given-names></name>
        </person-group>
        <article-title>Nonnegative decomposition of multivariate information</article-title>
        <source>arXiv preprint arXiv:1004.2515</source>
        <year iso-8601-date="2010">2010</year>
      </element-citation>
    </ref>
    <ref id="ref-scott1985">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Scott</surname><given-names>David W</given-names></name>
        </person-group>
        <article-title>Averaged shifted histograms: Effective nonparametric density estimators in several dimensions</article-title>
        <source>The Annals of Statistics</source>
        <publisher-name>JSTOR</publisher-name>
        <year iso-8601-date="1985">1985</year>
        <pub-id pub-id-type="doi">10.1214/aos/1176349654</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-griffith2014">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Griffith</surname><given-names>Virgil</given-names></name>
          <name><surname>Koch</surname><given-names>Christof</given-names></name>
        </person-group>
        <article-title>Quantifying synergistic mutual information</article-title>
        <source>Guided self-organization: inception</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <pub-id pub-id-type="doi">10.1007/978-3-642-53734-9_6</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-bertschinger2014">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bertschinger</surname><given-names>Nils</given-names></name>
          <name><surname>Rauh</surname><given-names>Johannes</given-names></name>
          <name><surname>Olbrich</surname><given-names>Eckehard</given-names></name>
          <name><surname>Jost</surname><given-names>Jürgen</given-names></name>
          <name><surname>Ay</surname><given-names>Nihat</given-names></name>
        </person-group>
        <article-title>Quantifying unique information</article-title>
        <source>Entropy</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <volume>16</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.3390/e16042161</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-james2018a">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>James</surname><given-names>Ryan G</given-names></name>
          <name><surname>Emenheiser</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Crutchfield</surname><given-names>James P</given-names></name>
        </person-group>
        <article-title>Unique information via dependency constraints</article-title>
        <source>Journal of Physics A: Mathematical and Theoretical</source>
        <publisher-name>IOP Publishing</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>52</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1088/1751-8121/aaed53</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-schreiber2000">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schreiber</surname><given-names>Thomas</given-names></name>
        </person-group>
        <article-title>Measuring information transfer</article-title>
        <source>Physical Review Letters</source>
        <publisher-name>APS</publisher-name>
        <year iso-8601-date="2000">2000</year>
        <volume>85</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1103/PhysRevLett.85.461</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-williams2011">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Williams</surname><given-names>Paul L</given-names></name>
          <name><surname>Beer</surname><given-names>Randall D</given-names></name>
        </person-group>
        <article-title>Generalized measures of information transfer</article-title>
        <source>arXiv preprint arXiv:1102.1507</source>
        <year iso-8601-date="2011">2011</year>
      </element-citation>
    </ref>
    <ref id="ref-wibral2014">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Wibral</surname><given-names>Michael</given-names></name>
          <name><surname>Vicente</surname><given-names>Raul</given-names></name>
          <name><surname>Lindner</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>Transfer entropy in neuroscience</article-title>
        <source>Directed information measures in neuroscience</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <pub-id pub-id-type="doi">10.1007/978-3-642-54474-3_1</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-james2018b">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>James</surname><given-names>Ryan G</given-names></name>
          <name><surname>Ellison</surname><given-names>Christopher J</given-names></name>
          <name><surname>Crutchfield</surname><given-names>James P</given-names></name>
        </person-group>
        <article-title>Dit: A Python package for discrete information theory</article-title>
        <source>Journal of Open Source Software</source>
        <year iso-8601-date="2018">2018</year>
        <volume>3</volume>
        <issue>25</issue>
        <pub-id pub-id-type="doi">10.21105/joss.00738</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wollstadt12019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wollstadt</surname><given-names>Patricia</given-names></name>
          <name><surname>Lizier</surname><given-names>Joseph T.</given-names></name>
          <name><surname>Vicente</surname><given-names>Raul</given-names></name>
          <name><surname>Finn</surname><given-names>Conor</given-names></name>
          <name><surname>Martınez-Zarzuela</surname><given-names>Mario</given-names></name>
          <name><surname>Mediano</surname><given-names>Pedro</given-names></name>
          <name><surname>Novelli</surname><given-names>Leonardo</given-names></name>
          <name><surname>Wibral</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>IDTxl: The Information Dynamics Toolkit xl: a Python package for the efficient analysis of multivariate information dynamics in networks</article-title>
        <source>Journal of Open Source Software</source>
        <year iso-8601-date="2019">2019</year>
        <volume>4</volume>
        <issue>34</issue>
        <pub-id pub-id-type="doi">10.21105/joss.01081</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-izquierdo2015">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Izquierdo</surname><given-names>Eduardo J</given-names></name>
          <name><surname>Williams</surname><given-names>Paul L</given-names></name>
          <name><surname>Beer</surname><given-names>Randall D</given-names></name>
        </person-group>
        <article-title>Information flow through a model of the C. elegans klinotaxis circuit</article-title>
        <source>PLOS ONE</source>
        <publisher-name>Public Library of Science</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>10</volume>
        <issue>10</issue>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0140397</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-beer2015">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Beer</surname><given-names>Randall D</given-names></name>
          <name><surname>Williams</surname><given-names>Paul L</given-names></name>
        </person-group>
        <article-title>Information processing and dynamics in minimally cognitive agents</article-title>
        <source>Cognitive Science</source>
        <publisher-name>Wiley Online Library</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>39</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1111/cogs.12142</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-inceinformation">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Ince</surname><given-names>RAA</given-names></name>
          <name><surname>Bartolozzi</surname><given-names>C</given-names></name>
          <name><surname>Panzeri</surname><given-names>S</given-names></name>
        </person-group>
        <article-title>An information-theoretic library for the analysis of neural codes</article-title>
        <pub-id pub-id-type="doi">10.2417/1200906.1663</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-lizier2014jidt">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lizier</surname><given-names>Joseph T</given-names></name>
        </person-group>
        <article-title>JIDT: An information-theoretic toolkit for studying the dynamics of complex systems</article-title>
        <source>Frontiers in Robotics and AI</source>
        <publisher-name>Frontiers</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <volume>1</volume>
        <pub-id pub-id-type="doi">10.3389/frobt.2014.00011</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-paninski2003estimation">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Paninski</surname><given-names>Liam</given-names></name>
        </person-group>
        <article-title>Estimation of entropy and mutual information</article-title>
        <source>Neural Computation</source>
        <publisher-name>MIT Press</publisher-name>
        <year iso-8601-date="2003">2003</year>
        <volume>15</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.1162/089976603321780272</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-scott1987biased">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Scott</surname><given-names>David W</given-names></name>
          <name><surname>Terrell</surname><given-names>George R</given-names></name>
        </person-group>
        <article-title>Biased and unbiased cross-validation in density estimation</article-title>
        <source>Journal of the American Statistical Association</source>
        <publisher-name>Taylor &amp; Francis</publisher-name>
        <year iso-8601-date="1987">1987</year>
        <volume>82</volume>
        <issue>400</issue>
        <pub-id pub-id-type="doi">10.1080/01621459.1987.10478550</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wand1994kernel">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Wand</surname><given-names>Matt P</given-names></name>
          <name><surname>Jones</surname><given-names>M Chris</given-names></name>
        </person-group>
        <source>Kernel smoothing</source>
        <publisher-name>Chapman; Hall/CRC</publisher-name>
        <year iso-8601-date="1994">1994</year>
        <pub-id pub-id-type="doi">10.1201/b14876</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-fernando2009selection">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Fernando</surname><given-names>TMKG</given-names></name>
          <name><surname>Maier</surname><given-names>HR</given-names></name>
          <name><surname>Dandy</surname><given-names>GC</given-names></name>
        </person-group>
        <article-title>Selection of input variables for data driven models: An average shifted histogram partial mutual information estimator approach</article-title>
        <source>Journal of Hydrology</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>367</volume>
        <issue>3-4</issue>
        <pub-id pub-id-type="doi">10.1016/j.jhydrol.2008.10.019</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-scott1979optimal">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Scott</surname><given-names>David W</given-names></name>
        </person-group>
        <article-title>On optimal and data-based histograms</article-title>
        <source>Biometrika</source>
        <publisher-name>Oxford University Press</publisher-name>
        <year iso-8601-date="1979">1979</year>
        <volume>66</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1093/biomet/66.3.605</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-scott2012multivariate">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Scott</surname><given-names>David W</given-names></name>
        </person-group>
        <article-title>Multivariate density estimation and visualization</article-title>
        <source>Handbook of computational statistics</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <pub-id pub-id-type="doi">10.1007/978-3-642-21551-3_19</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
