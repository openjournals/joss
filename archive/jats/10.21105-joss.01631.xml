<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1631</article-id>
<article-id pub-id-type="doi">10.21105/joss.01631</article-id>
<title-group>
<article-title>openEyeTrack - A high speed multi-threaded eye tracker
for head-fixed applications</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9575-1717</contrib-id>
<string-name>Jorge Paolo Casas</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-1711-590X</contrib-id>
<string-name>Chandramouli Chandrasekaran</string-name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Biomedical Engineering, Boston University,
Boston, MA 02215, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Anatomy and Neurobiology, Boston University,
Boston, MA 02118, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Psychological and Brain Sciences, Boston
University, Boston, MA 02215, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Center for Systems Neuroscience, Boston University, Boston,
MA 02215, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-07-26">
<day>26</day>
<month>7</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>42</issue>
<fpage>1631</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Eye-Tracking</kwd>
<kwd>Threading</kwd>
<kwd>OpenCV</kwd>
<kwd>Teledyne DALSA</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>When faced with a decision, an organism uses information gathered
  by its senses in order to determine the best course of action. Vision
  is one of the primary senses, and tracking eye gaze can offer insight
  into the cues that affect decision-making behavior. Thus, to study
  decision-making and other cognitive processes, it is fundamentally
  necessary to track eye position accurately. However, commercial eye
  trackers are often very expensive, and incorporate proprietary
  software to detect the movement of the eye. Closed source solutions
  limit the researcher’s ability to be fully informed regarding the
  algorithms used to track the eye and to incorporate modifications
  tailored to their needs. Here, we present our software solution,
  <italic>openEyeTrack</italic>, a low-cost, high-speed, low-latency,
  open-source video-based eye tracker. Video-based eye trackers can
  perform nearly as well as classical scleral search coil methods and
  are suitable for most applications
  (<xref alt="Kimmel et al., 2012" rid="ref-10.3389U002Ffnbeh.2012.00049" ref-type="bibr">Kimmel
  et al., 2012</xref>). High-speed eye trackers improve ability to
  detect saccades and microsaccades for real-time behavioral and
  physiological experiments and also improve eye position
  estimation.</p>
  <p><bold>Planned Use Cases</bold>: We expect to incorporate
  <italic>openEyeTrack</italic> in research concerning the neural
  dynamics of cognition, decision-making, and motor-control conducted at
  the Chandrasekaran Lab and in labs performing human psychophysics
  experiments at Boston University. We also anticipate performing
  additional tests of <italic>openEyeTrack</italic> in experiments at
  the Zimmerman Lab at the University of Minnesota. These experiments
  will further validate <italic>openEyeTrack</italic>, help identify
  necessary enhancements, and provide additional performance
  metrics.</p>
</sec>
<sec id="software-and-hardware-components">
  <title>Software and Hardware components</title>
  <p><italic>openEyeTrack</italic> is a video-based eye-tracker that
  takes advantage of OpenCV
  (<xref alt="Bradski &amp; Kaehler, 2016" rid="ref-bradski2016learning" ref-type="bibr">Bradski
  &amp; Kaehler, 2016</xref>;
  <xref alt="OpenCV, 2019a" rid="ref-openCVcomputerlibrary" ref-type="bibr">OpenCV,
  2019a</xref>,
  <xref alt="2019b" rid="ref-opencvmanual" ref-type="bibr">2019b</xref>),
  a low-cost, high-speed infrared camera and GigE-V APIs for Linux
  provided by Teledyne DALSA
  (<xref alt="Teledyne DALSA, 2018" rid="ref-GigeV" ref-type="bibr">Teledyne
  DALSA, 2018</xref>), the graphical user interface toolkit QT5
  (<xref alt="The Qt Company Ltd., 2013" rid="ref-qt5" ref-type="bibr">The
  Qt Company Ltd., 2013</xref>) and cvui, the OpenCV based GUI
  (<xref alt="Bevilacqua, 2018" rid="ref-cvui" ref-type="bibr">Bevilacqua,
  2018</xref>). All of the software components are freely available. The
  only costs are from the hardware components such as the camera
  (<ext-link ext-link-type="uri" xlink:href="https://www.teledynedalsa.com/en/products/imaging/cameras/genie-nano-1gige/">Genie
  Nano M640 NIR</ext-link>, Teledyne DALSA, ~$450, ~730 frames per
  second) and infrared light source, an articulated arm to position the
  camera (Manfrotto: $130), a computer with one or more gigabit network
  interface cards, and a power over ethernet switch to power and receive
  data from the camera.</p>
  <p>By using the GigE-V Framework to capture the frames from the DALSA
  camera and the OpenCV simple blob detector,
  <italic>openEyeTrack</italic> can accurately estimate the position and
  area of the pupil. We include pupil size calculations because of its
  putative link to arousal levels and emotions of the subject
  (<xref alt="Võ et al., 2008" rid="ref-doiU003A10.1111U002Fj.1469-8986.2007.00606.x" ref-type="bibr">Võ
  et al., 2008</xref>).</p>
</sec>
<sec id="multithreading-provides-improvements-over-existing-open-source-solutions">
  <title>Multithreading provides improvements over existing open source
  solutions</title>
  <p><italic>openEyeTrack</italic> is modeled on other open-source eye
  trackers currently available such as <italic>“Oculomatic”</italic>
  (<xref alt="Zimmermann et al., 2016" rid="ref-ZIMMERMANN2016138" ref-type="bibr">Zimmermann
  et al., 2016</xref>). However, most of these programs are
  single-threaded: the frames are captured, analyzed, and displayed
  sequentially, only executing the next stage once the previous stage
  completes its processing. Although single-threaded methods have become
  more effective over the years, these stages are time-consuming and can
  limit the overall performance. In order to increase performance,
  <italic>openEyeTrack</italic> was developed as a multithreaded
  application. The capture, display, data transmission, and most
  importantly, pupil detection components all happen within their
  separate threads. By incorporating multiple threads, the processing
  speed of the frames can match the frame capture rate of the camera,
  allowing for the lossless processing of data.</p>
</sec>
<sec id="algorithm">
  <title>Algorithm</title>
  <p>Figure 1 shows the workflow for <italic>openEyeTrack</italic>. As
  frames transition between the capture, detection, and display stages,
  they are stored in queues which enable the separate stages to run
  independently and allow for asynchronous capture, detection, and
  display. Once the camera grabs a new frame, it is briefly stored in
  the Genicam memory buffers before being extracted and packaged by the
  “capture thread” into a struct and stored in a queue. This approach
  ensures that the sequence of acquisition frames is preserved and that
  the frame acquisition process can occur without being slowed down by
  the pupil detection process or display related processes. The frames
  in the “capture queue” are then popped off by the “n” (user-specified)
  detection thread(s). Each “detection thread” takes the data from the
  “capture queue,” converts it into an OpenCV Mat object, applies the
  OpenCV blob detection algorithm to identify the pupil, and notes the
  key features. Each detection thread also outputs the eye position
  (i.e., center of the pupil) as text on the frame and draws a circle
  around the blob identified as the pupil. All of these steps are very
  time consuming, which is why we recommend initializing multiple
  threads for higher performance. Once the detection stage has
  completed, the frames are stored in a “display queue” that the
  “display thread” will grab from to show the images. The detection
  threads also package the frame and keypoints information into a struct
  object and stores it in a “network queue.” The “network thread” reads
  from this queue and sends out packets over a UDP socket for downstream
  applications.</p>
</sec>
<sec id="performance">
  <title>Performance</title>
  <p>Under the conditions at the time of development, we were able to
  achieve frame acquisition rates of up to 715 frames per sec (fps) and
  display rates of up to 145 fps. Although more threads, in theory,
  should increase speed, four detection threads were sufficient to keep
  up with the camera. Performance was significantly improved when we
  used the tool provided by Teledyne Dalsa that adjusts various features
  of the network buffers and allows higher throughput transmission from
  the camera to the computer. Additionally, environmental lighting
  significantly affects the speed at which the blob detection occurs.
  The OpenCV blob detector by default looks for black blobs, and thus
  more light allows for easier detection by increasing the contrast
  between darker and lighter areas of the image. To facilitate blob
  detection, we also apply binary thresholding to the images. The user
  can also specify a region of interest for the blob detector, which
  again improves processing time. For eye tracking, it is necessary to
  have an infrared IR light source to obtain a eye image with increased
  contrast between the pupil and the surrounding regions.</p>
</sec>
<sec id="limitations">
  <title>Limitations</title>
  <p>Our eye tracking solution is not meant to solve all gaze tracking
  issues which may be more readily addressed in commercial
  solutions.</p>
  <list list-type="order">
    <list-item>
      <p><italic>openEyeTrack</italic> cannot be used if the head is
      freely moving. In our approach, which only detects the pupil, head
      motion is confounded with pupil motion. One future solution is to
      use both the corneal reflection and the pupil to allow for
      head-free eye tracking. We anticipate implementing corneal
      reflections in future versions of
      <italic>openEyeTrack</italic>.</p>
    </list-item>
    <list-item>
      <p><italic>openEyeTrack</italic> does not output signals to analog
      channels which is a typical feature of commercial eye trackers.
      These analog signals were proxies for the analog signals from
      scleral search coils used for eye tracking.</p>
    </list-item>
    <list-item>
      <p>Using <italic>openEyeTrack</italic> requires knowledge of Linux
      and some degree of comfort with the command line to compile and
      install various components — it is not as seamless and polished as
      commercial solutions. On the other hand, it provides open-source
      code for eye-tracking.</p>
    </list-item>
  </list>
  <p><italic>openEyeTrack</italic> is available on GitHub at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/chand-lab/openEyeTrack">https://github.com/chand-lab/openEyeTrack</ext-link>.
  A more detailed description of usage can be found under the README.md
  and USAGE.md files located in the repository.</p>
  <p>-<inline-graphic mimetype="image" mime-subtype="png" xlink:href="openEyeTrack_Overview(1).png" />
  <italic>Figure 1: A visual depiction of the overall software and
  hardware architecture in openEyeTrack.</italic></p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to acknowledge Jeremy Casas for his invaluable
  feedback and support throughout this project. This work was supported
  by an NIH/NINDS Grant to CC (4R00NS092972-03), Moorman-Simon
  Interdisciplinary Career Development Professorship (CC), and startup
  funds provided by Boston University to CC.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-doiU003A10.1111U002Fj.1469-8986.2007.00606.x">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Võ</surname><given-names>Melissa L.-H.</given-names></name>
          <name><surname>Jacobs</surname><given-names>Arthur M.</given-names></name>
          <name><surname>Kuchinke</surname><given-names>Lars</given-names></name>
          <name><surname>Hofmann</surname><given-names>Markus</given-names></name>
          <name><surname>Conrad</surname><given-names>Markus</given-names></name>
          <name><surname>Schacht</surname><given-names>Annekathrin</given-names></name>
          <name><surname>Hutzler</surname><given-names>Florian</given-names></name>
        </person-group>
        <article-title>The coupling of emotion and cognition in the eye: Introducing the pupil old/new effect</article-title>
        <source>Psychophysiology</source>
        <year iso-8601-date="2008">2008</year>
        <volume>45</volume>
        <issue>1</issue>
        <uri>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2007.00606.x</uri>
        <pub-id pub-id-type="doi">10.1111/j.1469-8986.2007.00606.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-opencvmanual">
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name><surname>OpenCV</surname></name>
        </person-group>
        <article-title>The OpenCV reference manual, edition 4.1.1</article-title>
        <year iso-8601-date="2019">2019</year>
        <date-in-citation content-type="access-date"><year iso-8601-date="2019-07-01">2019</year><month>07</month><day>01</day></date-in-citation>
        <uri>https://docs.opencv.org/4.1.1/</uri>
      </element-citation>
    </ref>
    <ref id="ref-cvui">
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name><surname>Bevilacqua</surname><given-names>Fernando</given-names></name>
        </person-group>
        <article-title>A (very) simple UI lib built on top of OpenCV drawing primitives</article-title>
        <year iso-8601-date="2018">2018</year>
        <date-in-citation content-type="access-date"><year iso-8601-date="2019-07-26">2019</year><month>07</month><day>26</day></date-in-citation>
        <uri>https://github.com/Dovyski/cvui</uri>
      </element-citation>
    </ref>
    <ref id="ref-openCVcomputerlibrary">
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name><surname>OpenCV</surname></name>
        </person-group>
        <article-title>OpenCV: Open Source Computer Vision Library</article-title>
        <year iso-8601-date="2019">2019</year>
        <date-in-citation content-type="access-date"><year iso-8601-date="2019-07-01">2019</year><month>07</month><day>01</day></date-in-citation>
        <uri>https://github.com/opencv/opencv</uri>
      </element-citation>
    </ref>
    <ref id="ref-bradski2016learning">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Bradski</surname><given-names>Gary</given-names></name>
          <name><surname>Kaehler</surname><given-names>Adrian</given-names></name>
        </person-group>
        <source>Learning OpenCV 3: Computer vision in C++ with the OpenCV library</source>
        <publisher-name>O’Reilly Media, Inc.</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <isbn>978-1491937990</isbn>
      </element-citation>
    </ref>
    <ref id="ref-ZIMMERMANN2016138">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Zimmermann</surname><given-names>Jan</given-names></name>
          <name><surname>Vazquez</surname><given-names>Yuriria</given-names></name>
          <name><surname>Glimcher</surname><given-names>Paul W.</given-names></name>
          <name><surname>Pesaran</surname><given-names>Bijan</given-names></name>
          <name><surname>Louie</surname><given-names>Kenway</given-names></name>
        </person-group>
        <article-title>Oculomatic: High speed, reliable, and accurate open-source eye tracking for humans and non-human primates</article-title>
        <source>Journal of Neuroscience Methods</source>
        <year iso-8601-date="2016">2016</year>
        <volume>270</volume>
        <issn>0165-0270</issn>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.06.016</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-GigeV">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <string-name>Teledyne DALSA</string-name>
        </person-group>
        <source>GigE-V framework for linux</source>
        <year iso-8601-date="2018-09-14">2018</year><month>09</month><day>14</day>
        <uri>https://www.teledynedalsa.com/en/support/downloads-center/software-development-kits/132/</uri>
      </element-citation>
    </ref>
    <ref id="ref-qt5">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <string-name>The Qt Company Ltd.</string-name>
        </person-group>
        <source>Qt</source>
        <year iso-8601-date="2013-12-12">2013</year><month>12</month><day>12</day>
        <uri>https://www.qt.io/</uri>
      </element-citation>
    </ref>
    <ref id="ref-10.3389U002Ffnbeh.2012.00049">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kimmel</surname><given-names>Daniel</given-names></name>
          <name><surname>Mammo</surname><given-names>Dagem</given-names></name>
          <name><surname>Newsome</surname><given-names>William</given-names></name>
        </person-group>
        <article-title>Tracking the eye non-invasively: Simultaneous comparison of the scleral search coil and optical tracking techniques in the macaque monkey</article-title>
        <source>Frontiers in Behavioral Neuroscience</source>
        <year iso-8601-date="2012">2012</year>
        <volume>6</volume>
        <issn>1662-5153</issn>
        <uri>https://www.frontiersin.org/article/10.3389/fnbeh.2012.00049</uri>
        <pub-id pub-id-type="doi">10.3389/fnbeh.2012.00049</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
