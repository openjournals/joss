<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1466</article-id>
<article-id pub-id-type="doi">10.21105/joss.01466</article-id>
<title-group>
<article-title>DynaMo: Dynamic Body Shape and Motion Capture with Intel
RealSense Cameras</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4260-0488</contrib-id>
<string-name>Abhishektha Boppana</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7808-8557</contrib-id>
<string-name>Allison P. Anderson</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Ann and H.J. Smead Department of Aerospace Engineering
Sciences, University of Colorado Boulder</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-05-08">
<day>8</day>
<month>5</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>41</issue>
<fpage>1466</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>biomechanics</kwd>
<kwd>body shape</kwd>
<kwd>motion capture</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="background">
  <title>Background</title>
  <p>Human body shape can be captured with a variety of methodologies,
  including laser lines, structured light, photogrammetry, and
  millimeter waves
  (<xref alt="Daanen &amp; Haar, 2013" rid="ref-DannenU003A2013" ref-type="bibr">Daanen
  &amp; Haar, 2013</xref>). However, these technologies require
  expensive modules and have limited ability to capture dynamic changes
  in body shape.</p>
  <p>Motion capture with specific markers is commonly done through
  camera-based motion tracking
  (<xref alt="Windolf et al., 2008" rid="ref-WindolfU003A2008" ref-type="bibr">Windolf
  et al., 2008</xref>) These systems for marker tracking are often cost
  prohibitive and unable to capture surface morphology.</p>
  <p>Recently, Intel released the D415 and D435 RealSense Depth Cameras,
  which use near-infrared structured light patterns and two infrared
  imagers to capture depth information at up to 90 frames per second.
  Purchasing a set of these cameras is more affordable than buying a
  dedicated motion-capture system for shape or marker tracking.</p>
  <p>While Intel provides the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/IntelRealSense/librealsense"><monospace>librealsense</monospace></ext-link>
  library to interface with their cameras, it lacks tools to use
  multiple devices at once to capture shape and marker-tracking
  information. <monospace>DynaMo</monospace> builds upon
  <ext-link ext-link-type="uri" xlink:href="https://github.com/IntelRealSense/librealsense"><monospace>librealsense</monospace></ext-link>
  to provide additional capability for researchers looking to capture
  such data.</p>
  <p><monospace>DynaMo</monospace> is designed to primarily assist those
  in the biomechanics and medical fields in capturing motion or
  body-shape data. It is currently being used in the Anderson
  Bioastronautics Research Group to capture dynamic changes in foot
  morphology.</p>
  <p><inline-graphic mimetype="image" mime-subtype="png" xlink:href="documentation/sampleFrames.png" />
  <italic>Figure 1: Sample frames collected by DynaMo showing dynamic
  shape capture (green) and marker identifaction (gray
  spheres)</italic></p>
</sec>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>DynaMo</monospace> is a Python library that provides
  tools to capture dynamic changes in body shape and track locations of
  markers using Intel RealSense D4XX cameras.
  <monospace>DynaMo</monospace> was developed from the examples provided
  by Intel in the Python
  <ext-link ext-link-type="uri" xlink:href="https://github.com/IntelRealSense/librealsense"><monospace>librealsense</monospace></ext-link>
  library. It has been successfully tested streaming six cameras at 90
  frames per second, all connected to one computer.
  <monospace>DynaMo</monospace> consists of several scripts that allow
  for calibration of multiple RealSense D4XX cameras to a common global
  coordinate system, simultaneous streaming of multiple RealSense D4XX
  cameras, viewing of data from multiple RealSense D4XX cameras in
  pointcloud format, and identification of reflecting markers from the
  pointclouds. The library is optimized to reduce the number of dropped
  frames while streaming.</p>
  <p><monospace>DynaMo</monospace> allows for the capture of depth,
  infrared, and color frames at an <inline-formula><alternatives>
  <tex-math><![CDATA[(u \times v)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>×</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  resolution from Intel RealSense cameras. The values that are captured
  in each frame are listed below:</p>
  <list list-type="bullet">
    <list-item>
      <p>Depth frames: <inline-formula><alternatives>
      <tex-math><![CDATA[s]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>s</mml:mi></mml:math></alternatives></inline-formula>,
      where <inline-formula><alternatives>
      <tex-math><![CDATA[s]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>s</mml:mi></mml:math></alternatives></inline-formula>
      is the distance to the object</p>
    </list-item>
    <list-item>
      <p>Infrared frames: <inline-formula><alternatives>
      <tex-math><![CDATA[Y]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>,
      where <inline-formula><alternatives>
      <tex-math><![CDATA[Y]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>
      is a single value from 0-255 denoting the monochrome pixel
      value</p>
    </list-item>
    <list-item>
      <p>Color frames: <inline-formula><alternatives>
      <tex-math><![CDATA[[R,G,B]]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
      where <inline-formula><alternatives>
      <tex-math><![CDATA[R,G,B]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      are red, green, and blue values, stacked to represent the color
      value of the pixel. This results in a
      <inline-formula><alternatives>
      <tex-math><![CDATA[(u\times v \times 3)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>×</mml:mo><mml:mi>v</mml:mi><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      dimensional frame.</p>
    </list-item>
  </list>
  <p>The pinhole camera model
  (<xref alt="Sturm, 2014" rid="ref-SturmU003A2014" ref-type="bibr">Sturm,
  2014</xref>) projects 3D points from the world
  <inline-formula><alternatives>
  <tex-math><![CDATA[[x,y,z]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  onto a 2D image plane <inline-formula><alternatives>
  <tex-math><![CDATA[[u,v]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  using the following formula:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  s\begin{bmatrix}u\\v\\1\end{bmatrix} = K \times \begin{bmatrix}x\\y\\z\end{bmatrix}
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>v</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>z</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>Where <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  is a matrix describing the camera’s intrinsic properties, and
  <inline-formula><alternatives>
  <tex-math><![CDATA[s]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>s</mml:mi></mml:math></alternatives></inline-formula>
  is the distance between the real-world point and the image plane.
  These properties include the focal length
  (<inline-formula><alternatives>
  <tex-math><![CDATA[f_{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[f_{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></alternatives></inline-formula>)
  and image offset (<inline-formula><alternatives>
  <tex-math><![CDATA[pp_{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[pp_{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>)
  in each direction. They are represented in the matrix as:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  K=\begin{bmatrix}f_{x}&0&pp_{x}\\0&f_{y}&pp_{y}\\0&0&1\end{bmatrix}
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mi>p</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mi>p</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>Since we are collecting 2D frames and we want to know the 3D
  location of the point to reconstruct the pointcloud, we can simply
  invert the <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  matrix and solve for the <inline-formula><alternatives>
  <tex-math><![CDATA[[x,y,z]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  location as we know <inline-formula><alternatives>
  <tex-math><![CDATA[s]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>s</mml:mi></mml:math></alternatives></inline-formula>,
  the distance between the 3D point and the 2D plane, and
  <inline-formula><alternatives>
  <tex-math><![CDATA[[u,v]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
  the coordinate of the point in the 2D plane:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \begin{bmatrix}x\\y\\z\end{bmatrix}=\begin{bmatrix}\dfrac{1}{f_{x}}&0&\dfrac{-pp_{x}}{f_{x}}\\0&\dfrac{1}{f_{y}}&\dfrac{-pp_{y}}{f_{y}}\\0&0&1\end{bmatrix}\begin{bmatrix}s*u\\s*v\\s\end{bmatrix}
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>z</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd columnalign="center"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>s</mml:mi><mml:mo>*</mml:mo><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>s</mml:mi><mml:mo>*</mml:mo><mml:mi>v</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>s</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>This transformation is known in the computer vision community, and
  is crucial to the functions present in <monospace>DynaMo</monospace>.
  <monospace>DynaMo</monospace> uses this transformation extensively in
  its calibration, streaming, and marker-tracking features.</p>
  <p>Connected cameras are setup using a
  <monospace>device_manager</monospace> object which handles calls for
  communicating with the cameras. Cameras are first calibrated to a
  common global coordinate system by using a defined chessboard viewable
  by all cameras. The chessboard points are detected using the
  <monospace>findChessboardCorners</monospace> function of the OpenCV
  library
  (<xref alt="Bradski, 2000" rid="ref-opencv_library" ref-type="bibr">Bradski,
  2000</xref>) for each camera’s color image. Once the chessboard
  corners are found, they are translated to 3D points from the
  perspective of each camera and centered.</p>
  <p>The Kabsch algorithm
  (<xref alt="Kabsch, 1976" rid="ref-KabschU003A1976" ref-type="bibr">Kabsch,
  1976</xref>) is used to compute the <inline-formula><alternatives>
  <tex-math><![CDATA[(3\times3)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  rotation matrix between each camera and the known chessboard
  coordinates. Translation is calculated by taking the difference
  between the known chessboard corners and the camera’s rotated
  chessboard perspective, resulting in a <inline-formula><alternatives>
  <tex-math><![CDATA[(3\times1)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  matrix. The rotation matrix is horizontally stacked to the translation
  matrix, and a row of <inline-formula><alternatives>
  <tex-math><![CDATA[[0,0,0,1]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  is added to create a <inline-formula><alternatives>
  <tex-math><![CDATA[(4\times4)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  matrix. This matrix transforms each camera’s pointcloud from its local
  coordinate system to a global coordinate system.</p>
  <p>Streaming is achieved by reading frames from each camera into a
  dictionary object saved in the computer’s RAM.
  <monospace>DynaMo</monospace> checks frame numbers for continuity to
  ensure that frames are collected synchronously and are not repeated.
  Once streaming is complete, <monospace>DynaMo</monospace> aligns the
  images collected by the sensors in each camera to a common image
  center and saves the images as <monospace>pickle</monospace> objects
  to the disk. The data from all cameras can then be viewed as a single
  pointcloud for each frame from all cameras by using the previously
  computed transformation matrix.</p>
  <p>A script is included to extract the locations of reflective markers
  on the pointcloud by simply thresholding for bright pixels on the
  infrared frame. Contours are then drawn fo each cluster of pixels on
  each camera’s infrared frame; these contours highlight the detected
  markers by each camera. The center of each cluster is calculated and
  then translated into a 3D point using the depth frame. All points from
  all cameras are translated into a global coordinate system using the
  previously computed transformation matrix, and clusters are scanned
  for duplicates seen from multiple cameras.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>This work was supported by a National Science Foundation Graduate
  Research under grant DGE 1650115. The authors would like to thank
  Dr. Rodger Kram and Dr. Wouter Hoogkamer for the use of their
  laboratory space for development and testing of the package.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-DannenU003A2013">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Daanen</surname><given-names>H. A. M.</given-names></name>
          <name><surname>Haar</surname><given-names>F. B. Ter</given-names></name>
        </person-group>
        <article-title>3D whole body scanners revisited</article-title>
        <source>Displays</source>
        <year iso-8601-date="2013">2013</year>
        <volume>34</volume>
        <issue>4</issue>
        <issn>0141-9382</issn>
        <uri>https://doi.org/10.1016/j.displa.2013.08.011</uri>
        <pub-id pub-id-type="doi">10.1016/j.displa.2013.08.011</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-WindolfU003A2008">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Windolf</surname><given-names>Markus</given-names></name>
          <name><surname>Götzen</surname><given-names>Nils</given-names></name>
          <name><surname>Morlock</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>Systematic accuracy and precision analysis of video motion capturing systems–exemplified on the vicon-460 system</article-title>
        <source>Journal of Biomechanics</source>
        <year iso-8601-date="2008">2008</year>
        <volume>41</volume>
        <issue>12</issue>
        <isbn>00219290</isbn>
        <uri>https://doi.org/10.1016/j.jbiomech.2008.06.024</uri>
        <pub-id pub-id-type="doi">10.1016/j.jbiomech.2008.06.024</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-opencv_library">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bradski</surname><given-names>G.</given-names></name>
        </person-group>
        <article-title>The OpenCV Library</article-title>
        <source>Dr. Dobb’s Journal of Software Tools</source>
        <year iso-8601-date="2000">2000</year>
      </element-citation>
    </ref>
    <ref id="ref-SturmU003A2014">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Sturm</surname><given-names>Peter</given-names></name>
        </person-group>
        <article-title>Pinhole camera model</article-title>
        <source>Computer vision: A reference guide</source>
        <person-group person-group-type="editor">
          <name><surname>Ikeuchi</surname><given-names>Katsushi</given-names></name>
        </person-group>
        <publisher-name>Springer US</publisher-name>
        <publisher-loc>Boston, MA</publisher-loc>
        <year iso-8601-date="2014">2014</year>
        <isbn>978-0-387-31439-6</isbn>
        <uri>https://doi.org/10.1007/978-0-387-31439-6_472</uri>
        <pub-id pub-id-type="doi">10.1007/978-0-387-31439-6_472</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-KabschU003A1976">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kabsch</surname><given-names>W.</given-names></name>
        </person-group>
        <article-title>A solution for the best rotation to relate two sets of vectors</article-title>
        <source>Acta Crystallographica Section A</source>
        <year iso-8601-date="1976-09">1976</year><month>09</month>
        <volume>32</volume>
        <issue>5</issue>
        <uri>https://doi.org/10.1107/S0567739476001873</uri>
        <pub-id pub-id-type="doi">10.1107/S0567739476001873</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
