<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1956</article-id>
<article-id pub-id-type="doi">10.21105/joss.01956</article-id>
<title-group>
<article-title>WordTokenizers.jl: Basic tools for tokenizing natural
language in Julia</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6703-0728</contrib-id>
<string-name>Ayush Kaushal</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1386-1646</contrib-id>
<string-name>Lyndon White</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0788-0242</contrib-id>
<string-name>Mike Innes</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6758-8350</contrib-id>
<string-name>Rohit Kumar</string-name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Indian Institute of Technology, Kharagpur</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>The University of Western Australia</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Julia Computing</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>ABV-Indian Institute of Information Technology and
Management Gwalior</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-07-01">
<day>1</day>
<month>7</month>
<year>2019</year>
</pub-date>
<volume>5</volume>
<issue>46</issue>
<fpage>1956</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>julialang</kwd>
<kwd>natural language processing (NLP)</kwd>
<kwd>tokenization</kwd>
<kwd>text mining</kwd>
<kwd>information retrieval</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>WordTokenizers.jl is a tool to help users of the Julia programming
  language
  (<xref alt="Bezanson et al., 2014" rid="ref-Julia" ref-type="bibr">Bezanson
  et al., 2014</xref>) work with natural language. In natural language
  processing (NLP) tokenization refers to breaking a text up into parts
  â€“ the tokens. Generally, tokenization refers to breaking a sentence up
  into words and other tokens such as punctuation. Complementary to word
  tokenization is <italic>sentence segmentation</italic> or
  <italic>sentence splitting</italic> (occasionally also called
  <italic>sentence tokenization</italic>), where a document is broken up
  into sentences, which can then be tokenized into words. Tokenization
  and sentence segmentation are some of the most fundamental operations
  to be performed before applying most NLP or information retrieval
  algorithms.</p>
  <p>WordTokenizers.jl provides a flexible API for defining fast
  tokenizers and sentence segmentors. Using this API several standard
  tokenizers and sentence segmenters have been implemented, allowing
  researchers and practitioners to focus on the higher details of their
  NLP tasks.</p>
  <p>WordTokenizers.jl does not implement significant novel tokenizers
  or sentence segmenters. Rather, it contains ports/implementations of
  the well-established and commonly used algorithms. At present, it
  contains rule-based methods primarily designed for English. Several of
  the implementations are sourced from the Python NLTK project
  (<xref alt="Bird et al., 2009" rid="ref-NLTK2" ref-type="bibr">Bird et
  al., 2009</xref>;
  <xref alt="Bird &amp; Loper, 2004" rid="ref-NLTK1" ref-type="bibr">Bird
  &amp; Loper, 2004</xref>), although these were in turn sourced from
  older pre-existing methods.</p>
  <p>WordTokenizers.jl uses a <monospace>TokenBuffer</monospace> API and
  its various lexers for fast word tokenization.
  <monospace>TokenBuffer</monospace> turns the string into a readable
  stream. A desired set of TokenBuffer lexers are used to read
  characters from the stream and flush out into an array of tokens. The
  package provides the following tokenizers made using this API.</p>
  <list list-type="bullet">
    <list-item>
      <p>A Tweet Tokenizer
      (<xref alt="Potts, 2019" rid="ref-tweettok" ref-type="bibr">Potts,
      2019</xref>) for casual text.</p>
    </list-item>
    <list-item>
      <p>A general purpose NLTK Tokenizer
      (<xref alt="Bird et al., 2009" rid="ref-NLTK2" ref-type="bibr">Bird
      et al., 2009</xref>;
      <xref alt="Bird &amp; Loper, 2004" rid="ref-NLTK1" ref-type="bibr">Bird
      &amp; Loper, 2004</xref>).</p>
    </list-item>
    <list-item>
      <p>An improved version of the multilingual Tok-tok tokenizer
      (<xref alt="Dehdari, 2014" rid="ref-toktokpub" ref-type="bibr">Dehdari,
      2014</xref>,
      <xref alt="2015" rid="ref-toktok" ref-type="bibr">2015</xref>).</p>
    </list-item>
  </list>
  <p>With various lexers written for the
  <monospace>TokenBuffer</monospace> API, users can also create their
  high-speed custom tokenizers with ease. The package also provides a
  simple reversible tokenizer
  (<xref alt="Mielke, 2019" rid="ref-reversibletok2" ref-type="bibr">Mielke,
  2019</xref>;
  <xref alt="Mielke &amp; Eisner, 2018" rid="ref-reversibletok1" ref-type="bibr">Mielke
  &amp; Eisner, 2018</xref>) that works by leaving certain merge
  symbols, as a means to reconstruct tokens into the original
  string.</p>
  <p>WordTokenizers.jl exposes a configurable default interface, which
  allows the tokenizer and sentence segmenters to be configured globally
  (where this is used). This allowed for easy benchmarking and
  comparisons of different methods.</p>
  <p>WordTokenizers.jl is currently being used by packages like
  <ext-link ext-link-type="uri" xlink:href="https://github.com/JuliaText/TextAnalysis.jl">TextAnalysis.jl</ext-link>,
  <ext-link ext-link-type="uri" xlink:href="https://github.com/chengchingwen/Transformers.jl">Transformers.jl</ext-link>
  and
  <ext-link ext-link-type="uri" xlink:href="https://github.com/JuliaText/CorpusLoaders.jl">CorpusLoaders.jl</ext-link>
  for tokenizing text.</p>
  <sec id="other-similar-software">
    <title>Other similar software</title>
    <fig>
      <caption><p>Speed comparison of Tokenizers on IMDB Movie Review
      Dataset</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="speed_compare.png" xlink:title="" />
    </fig>
    <p>There are various NLP libraries and toolkits written in other
    programming languages, available to Julia users for tokenization.
    <ext-link ext-link-type="uri" xlink:href="https://github.com/nltk/nltk">NLTK</ext-link>
    and
    <ext-link ext-link-type="uri" xlink:href="https://github.com/explosion/spaCy">SpaCy</ext-link>
    packages provide users with a variety of tokenizers, accessed to
    Julia users via <monospace>PyCall</monospace>. Shown above is a
    performance benchmark of using some of the WordTokenizers.jl
    tokenizers vs PyCalling the default tokenizers from NLTK and SpaCy.
    This was evaluated on the ~127,000 sentences of the IMDB Movie
    Review Dataset. It can be seen that the performance of
    WordTokenizers.jl is very strong.</p>
    <p>There are many more packages like
    <ext-link ext-link-type="uri" xlink:href="https://github.com/stanfordnlp/CoreNLP">Stanford
    CoreNLP</ext-link>,
    <ext-link ext-link-type="uri" xlink:href="https://github.com/allenai/allennlp/">AllenNLP</ext-link>
    providing a couple of basic tokenizers. However, WordTokenizers.jl
    is
    <ext-link ext-link-type="uri" xlink:href="https://github.com/Ayushk4/Tweet_tok_analyse/tree/master/speed">faster</ext-link>
    and simpler to use, providing with a wider variety of tokenizers and
    a means to build custom tokenizers.</p>
  </sec>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Julia">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
          <name><surname>Edelman</surname><given-names>Alan</given-names></name>
          <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
          <name><surname>Shah</surname><given-names>Viral B.</given-names></name>
        </person-group>
        <article-title>Julia: A fresh approach to numerical computing</article-title>
        <year iso-8601-date="2014">2014</year>
        <uri>http://arxiv.org/abs/1411.1607</uri>
        <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-NLTK1">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Bird</surname><given-names>Steven</given-names></name>
          <name><surname>Loper</surname><given-names>Edward</given-names></name>
        </person-group>
        <article-title>NLTK: The natural language toolkit</article-title>
        <source>Proceedings of the ACL 2004 on interactive poster and demonstration sessions</source>
        <publisher-name>Association for Computational Linguistics</publisher-name>
        <year iso-8601-date="2004">2004</year>
        <uri>http://www.aclweb.org/anthology/P04-3031</uri>
      </element-citation>
    </ref>
    <ref id="ref-NLTK2">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Bird</surname><given-names>Steven</given-names></name>
          <name><surname>Klein</surname><given-names>Ewan</given-names></name>
          <name><surname>Loper</surname><given-names>Edward</given-names></name>
        </person-group>
        <source>Natural language processing with Python</source>
        <publisher-name>Oâ€™Reilly Media, Inc.</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <uri>http://www.nltk.org/</uri>
      </element-citation>
    </ref>
    <ref id="ref-toktok">
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name><surname>Dehdari</surname><given-names>Jonathan</given-names></name>
        </person-group>
        <article-title>Tok-tok: A fast, simple, multilingual tokenizer</article-title>
        <year iso-8601-date="2015">2015</year>
        <uri>https://github.com/jonsafari/tok-tok</uri>
      </element-citation>
    </ref>
    <ref id="ref-toktokpub">
      <element-citation publication-type="thesis">
        <person-group person-group-type="author">
          <name><surname>Dehdari</surname><given-names>Jonathan</given-names></name>
        </person-group>
        <article-title>A neurophysiologically-inspired statistical language model</article-title>
        <publisher-name>The Ohio State University</publisher-name>
        <year iso-8601-date="2014">2014</year>
      </element-citation>
    </ref>
    <ref id="ref-reversibletok1">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mielke</surname><given-names>Sebastian J.</given-names></name>
          <name><surname>Eisner</surname><given-names>Jason</given-names></name>
        </person-group>
        <article-title>Spell once, summon anywhere: A two-level open-vocabulary language model</article-title>
        <source>CoRR</source>
        <year iso-8601-date="2018">2018</year>
        <volume>abs/1804.08205</volume>
        <uri>http://arxiv.org/abs/1804.08205</uri>
        <pub-id pub-id-type="doi">10.1609/aaai.v33i01.33016843</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-reversibletok2">
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name><surname>Mielke</surname><given-names>Sebastian J.</given-names></name>
        </person-group>
        <article-title>A simple, reversible, language-agnostic tokenizer</article-title>
        <year iso-8601-date="2019">2019</year>
        <uri>https://sjmielke.com/papers/tokenize/</uri>
      </element-citation>
    </ref>
    <ref id="ref-tweettok">
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name><surname>Potts</surname><given-names>Christopher</given-names></name>
        </person-group>
        <article-title>Sentiment symposium tutorial: tokenizing</article-title>
        <year iso-8601-date="2019">2019</year>
        <date-in-citation content-type="access-date"><year iso-8601-date="2011">2011</year></date-in-citation>
        <uri>http://sentiment.christopherpotts.net/tokenizing.html#sentiment</uri>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
