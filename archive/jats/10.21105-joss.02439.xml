<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2439</article-id>
<article-id pub-id-type="doi">10.21105/joss.02439</article-id>
<title-group>
<article-title>htmldate: A Python package to extract publication dates
from web pages</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-8079-8694</contrib-id>
<string-name>Adrien Barbaresi</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Berlin-Brandenburg Academy of Sciences</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-07-29">
<day>29</day>
<month>7</month>
<year>2020</year>
</pub-date>
<volume>5</volume>
<issue>51</issue>
<fpage>2439</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>metadata extraction</kwd>
<kwd>date parsing</kwd>
<kwd>web scraping</kwd>
<kwd>natural language processing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="introduction">
  <title>Introduction</title>
  <sec id="rationale">
    <title>Rationale</title>
    <p>Metadata extraction is part of data mining and knowledge
    extraction. Being able to better qualify content allows for insights
    based on descriptive or typological information (e.g., content type,
    authors, categories), better bandwidth control (e.g., by knowing
    when webpages have been updated), or optimization of indexing (e.g.,
    caches, language-based heuristics). It is useful for applications
    including database management, business intelligence, or data
    visualization. This particular effort is part of a methodological
    approach to derive information from web documents in order to build
    text databases for research, chiefly linguistics and natural
    language processing. Dates are critical components since they are
    relevant both from a philological standpoint and in the context of
    information technology.</p>
    <p>Although text is ubiquitous on the Web, extracting information
    from web pages can prove to be difficult. Web documents come in
    different shapes and sizes mostly because of the wide variety of
    genres, platforms, and content management systems, and not least
    because of greatly diverse publication goals. In most cases,
    immediately accessible data on retrieved webpages do not carry
    substantial or accurate information: neither the URL nor the server
    response provide a reliable way to date a web document, that is to
    find out when it has been published or possibly modified. In that
    case it is necessary to fully parse the document or apply robust
    scraping patterns on it. Improving extraction methods for web
    collections can hopefully allow for combining both the quantity
    resulting from broad web crawling and the quality obtained by
    accurately extracting text and metadata and by rejecting documents
    which do not match certain criteria.</p>
  </sec>
  <sec id="research-context">
    <title>Research context</title>
    <p>Fellow colleagues are working on a lexicographic information
    platform
    (<xref alt="Geyken et al., 2017" rid="ref-GeykenEtAlU003A2017" ref-type="bibr">Geyken
    et al., 2017</xref>) at the language center of the
    Berlin-Brandenburg Academy of Sciences
    (<ext-link ext-link-type="uri" xlink:href="https://www.dwds.de/">dwds.de</ext-link>).
    The platform hosts and provides access to a series of
    metadata-enhanced web corpora
    (<xref alt="Barbaresi, 2016" rid="ref-BarbaresiU003A2016" ref-type="bibr">Barbaresi,
    2016</xref>). Information on publication and modification dates is
    crucial to be able to make sense of linguistic data, that is, in the
    case of lexicography to determine precisely when a given word was
    used for the first time and how its use evolves through time.</p>
    <p>Large “offline” web text collections are now standard among the
    research community in linguistics and natural language processing.
    The construction of such text corpora notably involves “crawling,
    downloading, ‘cleaning’ and de-duplicating the data, then
    linguistically annotating it and loading it into a corpus query
    tool”
    (<xref alt="Kilgarriff, 2007" rid="ref-KilgarriffU003A2007" ref-type="bibr">Kilgarriff,
    2007</xref>). Web crawling
    (<xref alt="Olston &amp; Najork, 2010" rid="ref-OlstonU003A2010" ref-type="bibr">Olston
    &amp; Najork, 2010</xref>) involves a significant number of design
    decisions and turning points in data processing, without which data
    and applications turn into a “Wild West”
    (<xref alt="Jo &amp; Gebru, 2020" rid="ref-JoGebruU003A2020" ref-type="bibr">Jo
    &amp; Gebru, 2020</xref>). Researchers face a lack of information
    regarding the content, whose adequacy, focus, and quality are the
    object of a post hoc evaluation
    (<xref alt="Baroni et al., 2009" rid="ref-BaroniU003A2009" ref-type="bibr">Baroni
    et al., 2009</xref>). Comparably, web corpora (i.e., document
    collections) usually lack metadata gathered with or obtained from
    documents. Between opportunistic and restrained data collection
    (<xref alt="Barbaresi, 2015" rid="ref-BarbaresiU003A2015" ref-type="bibr">Barbaresi,
    2015</xref>), a significant challenge lies in the ability to extract
    and pre-process web data to meet scientific expectations with
    respect to corpus quality.</p>
  </sec>
</sec>
<sec id="functionality">
  <title>Functionality</title>
  <p><monospace>htmldate</monospace> finds original and updated
  publication dates of web pages using heuristics on HTML code and
  linguistic patterns. It operates both within Python and from the
  command-line. URLs, HTML files, or HTML trees are given as input, and
  the library outputs a date string in the desired format or
  <monospace>None</monospace> as the output is thouroughly verified in
  terms of plausibility and adequateness.</p>
  <p>The package features a combination of tree traversal and text-based
  extraction, and the following methods are used to date HTML
  documents:</p>
  <list list-type="order">
    <list-item>
      <p>Markup in header: common patterns are used to identify relevant
      elements (e.g., <monospace>link</monospace> and
      <monospace>meta</monospace> elements) including Open Graph
      protocol attributes and a large number of content management
      systems idiosyncrasies</p>
    </list-item>
    <list-item>
      <p>HTML code: The whole document is then searched for structural
      markers: <monospace>abbr</monospace> and
      <monospace>time</monospace> elements as well as a series of
      attributes (e.g. <monospace>postmetadata</monospace>)</p>
    </list-item>
    <list-item>
      <p>Bare HTML content: A series of heuristics is run on text and
      markup:</p>
      <list list-type="bullet">
        <list-item>
          <p>in <monospace>fast</monospace> mode the HTML page is
          cleaned and precise patterns are targeted</p>
        </list-item>
        <list-item>
          <p>in <monospace>extensive</monospace> mode all potential
          dates are collected and a disambiguation algorithm determines
          the best one</p>
        </list-item>
      </list>
    </list-item>
  </list>
  <p>Finally, a date is returned if a valid cue could be found in the
  document, corresponding to either the last update or the original
  publishing statement (the default), which allows for switching between
  original and updated dates. The output string defaults to ISO 8601 YMD
  format.</p>
  <p><monospace>htmldate</monospace> is compatible with all recent
  versions of Python (currently 3.4 to 3.9). It is designed to be
  computationally efficient and used in production on millions of
  documents. All the steps needed from web page download to HTML
  parsing, scraping, and text analysis are handled, including batch
  processing. It is distributed under the GNU General Public License
  v3.0. Markup-based extraction is multilingual by nature, and
  text-based refinements for better coverage currently support German,
  English and Turkish.</p>
</sec>
<sec id="state-of-the-art">
  <title>State of the art</title>
  <p>Diverse extraction and scraping techniques are routinely used on
  web document collections by companies and research institutions alike.
  Content extraction mostly draws on Document Object Model (DOM)
  examination, that is, on considering a given HTML document as a tree
  structure whose nodes represent parts of the document to be operated
  on. Less thorough and not necessarily faster alternatives use
  superficial search patterns such as regular expressions in order to
  capture desirable excerpts.</p>
  <sec id="alternatives">
    <title>Alternatives</title>
    <p>There are comparable software solutions in Python. The following
    date extraction packages are open-source and work
    out-of-the-box:</p>
    <list list-type="bullet">
      <list-item>
        <p><monospace>articleDateExtractor</monospace> detects,
        extracts, and normalizes the publication date of an online
        article or blog post
        (<xref alt="Geva, 2018" rid="ref-articleDateExtractor" ref-type="bibr">Geva,
        2018</xref>),</p>
      </list-item>
      <list-item>
        <p><monospace>date_guesser</monospace> extracts publication
        dates from a web pages along with an accuracy measure which is
        not tested here
        (<xref alt="Carroll &amp; Valiukas, 2019" rid="ref-dateguesser" ref-type="bibr">Carroll
        &amp; Valiukas, 2019</xref>),</p>
      </list-item>
      <list-item>
        <p><monospace>goose3</monospace> can extract information for
        embedded content
        (<xref alt="Grangier et al., 2019" rid="ref-goose3" ref-type="bibr">Grangier
        et al., 2019</xref>),</p>
      </list-item>
      <list-item>
        <p><monospace>htmldate</monospace> is the software package
        described here; it is designed to extract original and updated
        publication dates of web pages
        (<xref alt="Barbaresi, 2019" rid="ref-BarbaresiU003A2019" ref-type="bibr">Barbaresi,
        2019</xref>),</p>
      </list-item>
      <list-item>
        <p><monospace>newspaper</monospace> is mostly geared towards
        newspaper texts
        (<xref alt="Ou-Yang &amp; Prezument, 2019" rid="ref-newspaper" ref-type="bibr">Ou-Yang
        &amp; Prezument, 2019</xref>),</p>
      </list-item>
      <list-item>
        <p><monospace>news-please</monospace> is a news crawler that
        extracts structured information
        (<xref alt="Hamborg et al., 2017" rid="ref-HamborgEtAlU003A2017" ref-type="bibr">Hamborg
        et al., 2017</xref>),</p>
      </list-item>
    </list>
    <p>Two alternative packages are not tested here but that also could
    be used:</p>
    <list list-type="bullet">
      <list-item>
        <p><monospace>datefinder</monospace>
        (<xref alt="Koumjian et al., 2020" rid="ref-datefinder" ref-type="bibr">Koumjian
        et al., 2020</xref>) features pattern-based date extraction for
        texts written in English,</p>
      </list-item>
      <list-item>
        <p>if dates are nowhere to be found, using
        <monospace>CarbonDate</monospace>
        (<xref alt="Atkins et al., 2018" rid="ref-carbondate" ref-type="bibr">Atkins
        et al., 2018</xref>) can be an option, however this is
        computationally expensive.</p>
      </list-item>
    </list>
  </sec>
  <sec id="benchmark">
    <title>Benchmark</title>
    <sec id="test-set">
      <title>Test set</title>
      <p>The experiments below are run on a collection of documents that
      are either typical for Internet articles (news outlets, blogs,
      including smaller ones) or non-standard and thus harder to
      process. They were selected from large collections of web pages in
      German. For the sake of completeness, a few documents in other
      languages were added (English, European languages, Chinese, and
      Arabic).</p>
    </sec>
    <sec id="evaluation">
      <title>Evaluation</title>
      <p>The evaluation script is available in the project repository:
      <monospace>tests/comparison.py</monospace>. The tests can be
      reproduced by cloning the repository, installing all necessary
      packages and running the evaluation script with the data provided
      in the <monospace>tests</monospace> directory.</p>
      <p>Only documents with dates that are clearly able to be
      determined are considered for this benchmark. A given day is taken
      as unit of reference, meaning that results are converted to
      <monospace>%Y-%m-%d</monospace> format if necessary in order to
      make them comparable.</p>
    </sec>
    <sec id="time">
      <title>Time</title>
      <p>The execution time (best of 3 tests) cannot be easily compared
      in all cases as some solutions perform a whole series of
      operations which are irrelevant to this task.</p>
    </sec>
    <sec id="errors">
      <title>Errors</title>
      <p><monospace>goose3</monospace>’s output is not always meaningful
      and/or in a standardized format, so these cases were discarded.
      news-please seems to have trouble with some encodings (e.g., in
      Chinese), in which case it leads to an exception.</p>
    </sec>
  </sec>
  <sec id="results">
    <title>Results</title>
    <p>The results in Table 1 show that date extraction is not a
    completely solved task but one for which extractors have to resort
    to heuristics and guesses. The figures documenting recall and
    accuracy capture the real-world performance of the tools as the
    absence of a date output impacts the result.</p>
    <table-wrap>
      <caption>
        <p>225 web pages containing identifiable dates (as of
        2020-07-29)</p>
      </caption>
      <table>
        <thead>
          <tr>
            <th>Python Package</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>Accuracy</th>
            <th>F-Score</th>
            <th>Time</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>newspaper 0.2.8</td>
            <td>0.888</td>
            <td>0.407</td>
            <td>0.387</td>
            <td>0.558</td>
            <td>81.6</td>
          </tr>
          <tr>
            <td>goose3 3.1.6</td>
            <td>0.887</td>
            <td>0.441</td>
            <td>0.418</td>
            <td>0.589</td>
            <td>15.5</td>
          </tr>
          <tr>
            <td>date_guesser 2.1.4</td>
            <td>0.809</td>
            <td>0.553</td>
            <td>0.489</td>
            <td>0.657</td>
            <td>40.0</td>
          </tr>
          <tr>
            <td>news-please 1.5.3</td>
            <td>0.823</td>
            <td>0.660</td>
            <td>0.578</td>
            <td>0.732</td>
            <td>69.6</td>
          </tr>
          <tr>
            <td>articleDateExtractor 0.20</td>
            <td>0.817</td>
            <td>0.635</td>
            <td>0.556</td>
            <td>0.714</td>
            <td>6.8</td>
          </tr>
          <tr>
            <td>htmldate 0.7.0 <italic>(fast)</italic></td>
            <td><bold>0.903</bold></td>
            <td>0.907</td>
            <td>0.827</td>
            <td>0.905</td>
            <td><bold>2.4</bold></td>
          </tr>
          <tr>
            <td>htmldate[all] 0.7.0 <italic>(extensive)</italic></td>
            <td>0.889</td>
            <td><bold>1.000</bold></td>
            <td><bold>0.889</bold></td>
            <td><bold>0.941</bold></td>
            <td>3.8</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>Precision describes if the dates given as output are correct:
    <monospace>newspaper</monospace> and <monospace>goose3</monospace>
    fare well precision-wise but they fail to extract dates in a large
    majority of cases (poor recall). The difference in accuracy between
    <monospace>date_guesser</monospace> and
    <monospace>newspaper</monospace> is consistent with tests described
    on the website of the former.</p>
    <p>It turns out that <monospace>htmldate</monospace> performs better
    than the other solutions overall. It is also noticeably faster than
    the strictly comparable packages
    (<monospace>articleDateExtractor</monospace> and
    <monospace>date_guesser</monospace>). Despite being measured on a
    sample, the higher accuracy and faster processing time are highly
    significant. Especially for smaller news outlets, websites, and
    blogs, as well as pages written in languages other than English (in
    this case mostly but not exclusively German),
    <monospace>htmldate</monospace> greatly extends date extraction
    coverage without sacrificing precision.</p>
    <sec id="note-on-the-different-versions">
      <title>Note on the different versions:</title>
      <list list-type="bullet">
        <list-item>
          <p><monospace>htmldate[all]</monospace> means that additional
          components are added for performance and coverage. They can be
          installed with
          <monospace>pip/pip3/pipenv htmldate[all]</monospace> and
          result in differences with respect to accuracy (due to further
          linguistic analysis) and potentially speed (faster date
          parsing).</p>
        </list-item>
        <list-item>
          <p>The fast mode does not output as many dates (lower recall)
          but its guesses are more often correct (better precision).</p>
        </list-item>
      </list>
    </sec>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work has been supported by the ZDL research project
  (<italic>Zentrum für digitale Lexikographie der deutschen
  Sprache</italic>,
  <ext-link ext-link-type="uri" xlink:href="https://www.zdl.org/">zdl.org</ext-link>).
  Thanks to Yannick Kozmus (evaluation), user evolutionoftheuniverse
  (patterns for Turkish) and further
  <ext-link ext-link-type="uri" xlink:href="https://github.com/adbar/htmldate/graphs/contributors">contributors</ext-link>
  for testing and working on the package. Thanks to Daniel S. Katz,
  Geoff Bacon and Maarten van Gompel for reviewing this JOSS
  submission.</p>
  <p>The following Python modules have been of great help:
  <monospace>lxml</monospace>, <monospace>ciso8601</monospace>, and
  <monospace>dateparser</monospace>. A few patterns are derived from
  <monospace>python-goose</monospace>,
  <monospace>metascraper</monospace>, <monospace>newspaper</monospace>
  and <monospace>articleDateExtractor</monospace>; this package extends
  their coverage and robustness significantly.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-BarbaresiU003A2019">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Barbaresi</surname><given-names>Adrien</given-names></name>
        </person-group>
        <article-title>Generic Web Content Extraction with Open-Source Software</article-title>
        <source>Proceedings of KONVENS 2019, Kaleidoscope Abstracts</source>
        <publisher-name>GSCL</publisher-name>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-GeykenEtAlU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Geyken</surname><given-names>Alexander</given-names></name>
          <name><surname>Barbaresi</surname><given-names>Adrien</given-names></name>
          <name><surname>Didakowski</surname><given-names>Jörg</given-names></name>
          <name><surname>Jurish</surname><given-names>Bryan</given-names></name>
          <name><surname>Wiegand</surname><given-names>Frank</given-names></name>
          <name><surname>Lemnitzer</surname><given-names>Lothar</given-names></name>
        </person-group>
        <article-title>Die Korpusplattform des &quot;Digitalen Wörterbuchs der deutschen Sprache&quot; (DWDS)</article-title>
        <source>Zeitschrift für germanistische Linguistik</source>
        <publisher-name>De Gruyter</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>45</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1515/zgl-2017-0017</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BarbaresiU003A2015">
      <element-citation publication-type="thesis">
        <person-group person-group-type="author">
          <name><surname>Barbaresi</surname><given-names>Adrien</given-names></name>
        </person-group>
        <article-title>Ad hoc and general-purpose corpus construction from web sources</article-title>
        <publisher-name>École Normale Supérieure de Lyon</publisher-name>
        <year iso-8601-date="2015">2015</year>
      </element-citation>
    </ref>
    <ref id="ref-BarbaresiU003A2016">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Barbaresi</surname><given-names>Adrien</given-names></name>
        </person-group>
        <article-title>Efficient construction of metadata-enhanced web corpora</article-title>
        <source>Proceedings of the 10th Web as Corpus Workshop</source>
        <person-group person-group-type="editor">
          <name><surname>Cook</surname><given-names>Paul</given-names></name>
          <name><surname>Evert</surname><given-names>Stefan</given-names></name>
          <name><surname>Schäfer</surname><given-names>Roland</given-names></name>
          <name><surname>Stemle</surname><given-names>Egon</given-names></name>
        </person-group>
        <publisher-name>Association for Computational Linguistics</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <pub-id pub-id-type="doi">10.18653/v1/w16-2602</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-HamborgEtAlU003A2017">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Hamborg</surname><given-names>Felix</given-names></name>
          <name><surname>Meuschke</surname><given-names>Norman</given-names></name>
          <name><surname>Breitinger</surname><given-names>Corinna</given-names></name>
          <name><surname>Gipp</surname><given-names>Bela</given-names></name>
        </person-group>
        <article-title>news-please: A Generic News Crawler and Extractor</article-title>
        <source>Proceedings of the 15th International Symposium of Information Science</source>
        <person-group person-group-type="editor">
          <name><surname>Gaede</surname><given-names>Maria</given-names></name>
          <name><surname>Trkulja</surname><given-names>Violeta</given-names></name>
          <name><surname>Petra</surname><given-names>Vivien</given-names></name>
        </person-group>
        <publisher-loc>Berlin</publisher-loc>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-KilgarriffU003A2007">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kilgarriff</surname><given-names>Adam</given-names></name>
        </person-group>
        <article-title>Googleology is bad science</article-title>
        <source>Computational Linguistics</source>
        <publisher-name>MIT Press</publisher-name>
        <year iso-8601-date="2007">2007</year>
        <volume>33</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1162/coli.2007.33.1.147</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BaroniU003A2009">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Baroni</surname><given-names>Marco</given-names></name>
          <name><surname>Bernardini</surname><given-names>Silvia</given-names></name>
          <name><surname>Ferraresi</surname><given-names>Adriano</given-names></name>
          <name><surname>Zanchetta</surname><given-names>Eros</given-names></name>
        </person-group>
        <article-title>The WaCky Wide Web: a collection of very large linguistically processed web-crawled corpora</article-title>
        <source>Language Resources and Evaluation</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>43</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1007/s10579-009-9081-4</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-OlstonU003A2010">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Olston</surname><given-names>Christopher</given-names></name>
          <name><surname>Najork</surname><given-names>Marc</given-names></name>
        </person-group>
        <article-title>Web Crawling</article-title>
        <source>Foundations and Trends in Information Retrieval</source>
        <publisher-name>Now Publishers, Inc.</publisher-name>
        <year iso-8601-date="2010">2010</year>
        <volume>4</volume>
        <issue>3</issue>
      </element-citation>
    </ref>
    <ref id="ref-JoGebruU003A2020">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Jo</surname><given-names>Eun Seo</given-names></name>
          <name><surname>Gebru</surname><given-names>Timnit</given-names></name>
        </person-group>
        <article-title>Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning</article-title>
        <source>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</source>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-articleDateExtractor">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Geva</surname><given-names>Ran et al.</given-names></name>
        </person-group>
        <article-title>articleDateExtractor: Automatically extracts and normalizes an online article or blog post publication date</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <uri>https://github.com/Webhose/article-date-extractor</uri>
      </element-citation>
    </ref>
    <ref id="ref-dateguesser">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Carroll</surname><given-names>Colin D.</given-names></name>
          <name><surname>Valiukas</surname><given-names>Linas et al.</given-names></name>
        </person-group>
        <article-title>Date Guesser: A library to extract a publication date from a web page, along with a measure of the accuracy</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://github.com/mitmedialab/date_guesser</uri>
      </element-citation>
    </ref>
    <ref id="ref-goose3">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Grangier</surname><given-names>Xavier</given-names></name>
          <name><surname>Barrus</surname><given-names>Tyler</given-names></name>
          <name><surname>Sidorov</surname><given-names>Ilya et al.</given-names></name>
        </person-group>
        <article-title>Goose3: A Python 3 compatible version of goose</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://github.com/goose3/goose3</uri>
      </element-citation>
    </ref>
    <ref id="ref-newspaper">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Ou-Yang</surname><given-names>Lucas</given-names></name>
          <name><surname>Prezument</surname><given-names>Yuri et al.</given-names></name>
        </person-group>
        <article-title>Newspaper: News, full-text, and article metadata extraction in Python 3</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://github.com/codelucas/newspaper</uri>
      </element-citation>
    </ref>
    <ref id="ref-datefinder">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Koumjian</surname><given-names>Alec</given-names></name>
          <name><surname>sudobangbang</surname></name>
          <name><surname>Senecal</surname><given-names>Jonathan et al.</given-names></name>
        </person-group>
        <article-title>datefinder: Find dates inside text using Python and get back datetime objects</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <uri>https://github.com/akoumjian/datefinder</uri>
      </element-citation>
    </ref>
    <ref id="ref-carbondate">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Atkins</surname><given-names>Grant</given-names></name>
          <name><surname>DarkAngelZT</surname></name>
          <name><surname>Nwala</surname><given-names>Alexander et al.</given-names></name>
        </person-group>
        <article-title>CarbonDate: Estimating the age of web resources</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <uri>https://github.com/oduwsdl/CarbonDate</uri>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
