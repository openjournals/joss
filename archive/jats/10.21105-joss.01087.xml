<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1087</article-id>
<article-id pub-id-type="doi">10.21105/joss.01087</article-id>
<title-group>
<article-title>ReinforcementLearning: A Package to Perform Model-Free
Reinforcement Learning in R</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<string-name>Nicolas Pröellochs</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Stefan Feuerriegel</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Giessen</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University of Oxford</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>ETH Zurich</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-05-26">
<day>26</day>
<month>5</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>38</issue>
<fpage>1087</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Reinforcement Learning</kwd>
<kwd>Batch Learning</kwd>
<kwd>Experience Replay</kwd>
<kwd>Q-Learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Reinforcement learning refers to a group of methods from artificial
  intelligence where an agent performs learning through trial and error
  (<xref alt="Sutton &amp; Barto, 1998" rid="ref-Sutton.1998" ref-type="bibr">Sutton
  &amp; Barto, 1998</xref>). It differs from supervised learning, since
  reinforcement learning requires no explicit labels; instead, the agent
  interacts continuously with its environment. That is, the agent starts
  in a specific state and then performs an action, based on which it
  transitions to a new state and, depending on the outcome, receives a
  reward. Different strategies (e.g. Q-learning) have been proposed to
  maximize the overall reward, resulting in a so-called policy, which
  defines the best possible action in each state. As a main advantage,
  reinforcement learning is applicable to situations in which the
  dynamics of the environment are unknown or too complex to evaluate
  (e.g.
  <xref alt="Mnih et al., 2015" rid="ref-Mnih.2015" ref-type="bibr">Mnih
  et al., 2015</xref>). However, there is currently no package available
  for performing reinforcement learning in R. As a remedy, we introduce
  the <monospace>ReinforcementLearning</monospace> R package, which
  allows an agent to learn the optimal behavior based on sampling
  experience consisting of states, actions and rewards
  (<xref alt="Pröllochs &amp; Feuerriegel, 2017" rid="ref-ReinforcementLearning.2017" ref-type="bibr">Pröllochs
  &amp; Feuerriegel, 2017</xref>). Based on such training examples, the
  package allows a reinforcement learning agent to learn an optimal
  policy that defines the best possible action in each state. Main
  features of <monospace>ReinforcementLearning</monospace> include, but
  are not limited to:</p>
  <list list-type="bullet">
    <list-item>
      <p>Learning an optimal policy from a fixed set of a priori known
      transition samples</p>
    </list-item>
    <list-item>
      <p>Predefined learning rules and action selection modes</p>
    </list-item>
    <list-item>
      <p>A highly customizable framework for model-free reinforcement
      learning tasks</p>
    </list-item>
  </list>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Reinforcement learning techniques can primarily be categorized in
  two groups, namely, model-based and model-free approaches
  (<xref alt="Sutton &amp; Barto, 1998" rid="ref-Sutton.1998" ref-type="bibr">Sutton
  &amp; Barto, 1998</xref>). The former, algorithms, rely on explicit
  models of the environment that fully describe the probabilities of
  state transitions, as well as the consequences of actions and the
  associated rewards. Specifically, corresponding algorithms are built
  upon explicit representations of the environment given in the form of
  Markov decision processes. These MDPs can be solved by various,
  well-known algorithms, including value iteration and policy iteration,
  in order to derive the optimal behavior of an agent. These algorithms
  are also implemented within the statistical software R. For instance,
  the package <monospace>MDPtoolbox</monospace> solves such models based
  on an explicit formalization of the MDP, i.e. settings in which the
  transition probabilities and reward functions are known a priori
  (<xref alt="Chades et al., 2017" rid="ref-MDPtoolbox.2017" ref-type="bibr">Chades
  et al., 2017</xref>).</p>
  <p>The second category of reinforcement learning comprises approaches
  that forgo any explicit knowledge regarding the dynamics of the
  environment. These approaches learn the optimal behavior through
  trial-and-error by directly interacting with the environment. In this
  context, the learner has no explicit knowledge of either the reward
  function or the state transition function
  (<xref alt="Sutton &amp; Barto, 1998" rid="ref-Sutton.1998" ref-type="bibr">Sutton
  &amp; Barto, 1998</xref>). Instead, the optimal behavior is
  iteratively inferred from the consequences of actions on the basis of
  past experience. As a main advantage, this method is applicable to
  situations in which the dynamics of the environment are unknown or too
  complex to evaluate. However, the available tools in R are not yet
  living up to the needs of users in such cases. In fact, there is
  currently no package available that allows one to perform model-free
  reinforcement learning in R. Hence, users that aim to teach optimal
  behavior through trial-and-error learning must implement corresponding
  learning algorithms in a manual way.</p>
  <p>As a remedy, we introduce the
  <monospace>ReinforcementLearning</monospace> package for R, which
  allows an agent to learn the optimal behavior based on sampling
  experience consisting of states, actions and rewards
  (<xref alt="Pröllochs &amp; Feuerriegel, 2017" rid="ref-ReinforcementLearning.2017" ref-type="bibr">Pröllochs
  &amp; Feuerriegel, 2017</xref>). The training examples for
  reinforcement learning can originate from real-world applications,
  such as sensor data. In addition, the package is shipped with the
  built-in capability to sample experience from a function that defines
  the dynamics of the environment. In both cases, the result of the
  learning process is a highly interpretable reinforcement learning
  policy that defines the best possible action in each state. The
  package provides a remarkably flexible framework, which makes it
  readily applicable to a wide range of different problems. Among other
  functions, it allows one to customize a variety of learning parameters
  and elaborates on how to mitigate common performance in common
  solution algorithms (e.g. experience replay). The package
  <ext-link ext-link-type="uri" xlink:href="https://github.com/nproellochs/ReinforcementLearning/blob/master/vignettes/ReinforcementLearning.Rmd">vignette</ext-link>
  demonstrates its use by drawing upon common examples from the
  literature (e.g. finding optimal game strategies).</p>
</sec>
<sec id="functionality">
  <title>Functionality</title>
  <p>The <monospace>ReinforcementLearning</monospace> package utilizes
  different mechanisms for reinforcement learning, including Q-learning
  and experience replay. It thereby learns an optimal policy based on
  past experience in the form of sample sequences consisting of states,
  actions and rewards. Consequently, each training example consists of a
  state-transition tuple <inline-formula><alternatives>
  <tex-math><![CDATA[(s_i, a_i, r_{i+1}, s_{i+1})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  as follows:</p>
  <list list-type="bullet">
    <list-item>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[s_i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      is the current environment state.</p>
    </list-item>
    <list-item>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[a_i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      denotes the selected action in the current state.</p>
    </list-item>
    <list-item>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[r_{i+1}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
      specifies the immediate reward received after transitioning from
      the current state to the next state.</p>
    </list-item>
    <list-item>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[s_{i+1}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
      refers to the next environment state.</p>
    </list-item>
  </list>
  <p>The training examples for reinforcement learning can (1) be
  collected from an external source and inserted into a tabular data
  structure, or (2) generated dynamically by querying a function that
  defines the behavior of the environment. In both cases, the
  corresponding input must follow the same tuple structure
  <inline-formula><alternatives>
  <tex-math><![CDATA[(s_i, a_i, r_{i+1}, s_{i+1})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
  <p>Learning from pre-defined observations is beneficial when the input
  data is pre-determined or one wants to train an agent that replicates
  past behavior. In this case, one merely needs to insert a tabular data
  structure with past observations into the package. This might be the
  case when the state-transition tuples have been collected from an
  external source, such as sensor data, and one wants to learn an agent
  by eliminating further interaction with the environment.</p>
  <p>An alternative strategy is to define a function that mimics the
  behavior of the environment. One can then learn an agent that samples
  experience from this function. Here the environment function takes a
  state-action pair as input. It then returns a list containing the name
  of the next state and the reward. In this case, one can also utilize R
  to access external data sources, such as sensors, and execute actions
  via common interfaces. The structure of such a function is represented
  by the following pseudocode:</p>
  <preformat>environment &lt;- function(state, action) {
  ...
  return(list(&quot;NextState&quot; = newState,
              &quot;Reward&quot; = reward))
}</preformat>
  <p>After specifying the environment function, one can use
  <monospace>sampleExperience()</monospace> to collect random sequences
  from it. Thereby, the input specifies number of samples
  (<inline-formula><alternatives>
  <tex-math><![CDATA[N]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>),
  the environment function, the set of states
  (i.e. <inline-formula><alternatives>
  <tex-math><![CDATA[S]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>S</mml:mi></mml:math></alternatives></inline-formula>)
  and the set of actions (i.e. <inline-formula><alternatives>
  <tex-math><![CDATA[A]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>A</mml:mi></mml:math></alternatives></inline-formula>).
  The return value is then a data frame containing the experienced state
  transition tuples <inline-formula><alternatives>
  <tex-math><![CDATA[(s_i, a_i, r_{i+1}, s_{i+1})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  for <inline-formula><alternatives>
  <tex-math><![CDATA[i = 1, \ldots, N]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="notes-on-performance">
  <title>Notes on performance</title>
  <p>Q-learning is guaranteed to converge to an optimal policy. However,
  the method is computationally demanding as it relies on continuous
  interactions between an agent and its environment. To remedy this, the
  <monospace>ReinforcementLearning</monospace> package allows users to
  perform batch reinforcement learning. In most scenarios, this
  reinforcement learning variant benefits computational performance as
  it mitigates the ‘exploration overhead’ problem in pure online
  learning. In combination with experience replay, it speeds up
  convergence by collecting and replaying observed state transitions
  repeatedly to the agent as if they were new observations collected
  while interacting with the system. Nonetheless, due to the fact that
  the package is written purely in R, the applicability of the package
  to very large scale problems (such as applications from computer
  vision) is still limited. In the following, we briefly summarize
  scenarios the package is capable of handling and situations in which
  one should consider utilizing reinforcement learning implementations
  written in “faster” programming languages.</p>
  <p>What the <monospace>ReinforcementLearning</monospace> R package can
  do:</p>
  <list list-type="bullet">
    <list-item>
      <p>Learning optimal strategies for real-world problems with
      limited state and action sets (e.g. finding optimal strategies for
      simple games, training a simple stock market trading agent,
      learning polarity labels in applications from natural language
      processing).</p>
    </list-item>
    <list-item>
      <p>The packages allows one to speed up performance by adjusting
      learning parameters and making use of experience replay.</p>
    </list-item>
    <list-item>
      <p>The package allows one to train an agent from pre-defined
      observations without the need for modeling the dynamics of the
      environment. Typically, this approach drastically speeds up
      convergence and can be useful in situations in which the
      state-transition tuples have been collected from an external
      source, such as sensor data.</p>
    </list-item>
    <list-item>
      <p>The package provides a highly customizable framework for
      model-free reinforcement learning tasks in which the functionality
      can easily be extended. For example, users may attempt to speed up
      performance by defining alternative reinforcement learning
      algorithms and integrating them into the package code.</p>
    </list-item>
  </list>
  <p>What the <monospace>ReinforcementLearning</monospace> R package
  cannot do:</p>
  <list list-type="bullet">
    <list-item>
      <p>Solving large-scale problems with high-dimensional state-action
      spaces such as those from computer vision (users may consider
      reinforcement learning implementations written in “faster”
      programming languages)</p>
    </list-item>
    <list-item>
      <p>Solving reinforcement learning problems requiring real-time
      interaction (e.g. real-time interaction with a robot)</p>
    </list-item>
  </list>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-MDPtoolbox.2017">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Chades</surname><given-names>Iadine</given-names></name>
          <name><surname>Chapron</surname><given-names>Guillaume</given-names></name>
          <name><surname>Cros</surname><given-names>Marie-Josee</given-names></name>
          <name><surname>Garcia</surname><given-names>Frederick</given-names></name>
          <name><surname>Sabbadin</surname><given-names>Regis</given-names></name>
        </person-group>
        <source>MDPtoolbox</source>
        <year iso-8601-date="2017">2017</year>
        <uri>https://cran.r-project.org/web/packages/MDPtoolbox/index.html</uri>
        <pub-id pub-id-type="doi">10.1111/ecog.00888</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Mnih.2015">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mnih</surname><given-names>Volodymyr</given-names></name>
          <name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name>
          <name><surname>Silver</surname><given-names>David</given-names></name>
          <name><surname>Rusu</surname><given-names>Andrei A.</given-names></name>
          <name><surname>Veness</surname><given-names>Joel</given-names></name>
          <name><surname>Bellemare</surname><given-names>Marc G.</given-names></name>
          <name><surname>Graves</surname><given-names>Alex</given-names></name>
          <name><surname>Riedmiller</surname><given-names>Martin</given-names></name>
          <name><surname>Fidjeland</surname><given-names>Andreas K.</given-names></name>
          <name><surname>Ostrovski</surname><given-names>Georg</given-names></name>
          <name><surname>Petersen</surname><given-names>Stig</given-names></name>
          <name><surname>Beattie</surname><given-names>Charles</given-names></name>
          <name><surname>Sadik</surname><given-names>Amir</given-names></name>
          <name><surname>Antonoglou</surname><given-names>Ioannis</given-names></name>
          <name><surname>King</surname><given-names>Helen</given-names></name>
          <name><surname>Kumaran</surname><given-names>Dharshan</given-names></name>
          <name><surname>Wierstra</surname><given-names>Daan</given-names></name>
          <name><surname>Legg</surname><given-names>Shane</given-names></name>
          <name><surname>Hassabis</surname><given-names>Demis</given-names></name>
        </person-group>
        <article-title>Human-level control through deep reinforcement learning</article-title>
        <source>Nature</source>
        <year iso-8601-date="2015">2015</year>
        <volume>518</volume>
        <issue>7540</issue>
        <pub-id pub-id-type="doi">10.1038/nature14236</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-ReinforcementLearning.2017">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Pröllochs</surname><given-names>Nicolas</given-names></name>
          <name><surname>Feuerriegel</surname><given-names>Stefan</given-names></name>
        </person-group>
        <source>ReinforcementLearning</source>
        <year iso-8601-date="2017">2017</year>
        <uri>https://CRAN.R-project.org/package=ReinforcementLearning</uri>
      </element-citation>
    </ref>
    <ref id="ref-Sutton.1998">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Sutton</surname><given-names>Richard S.</given-names></name>
          <name><surname>Barto</surname><given-names>Andrew G.</given-names></name>
        </person-group>
        <source>Reinforcement learning: An introduction</source>
        <publisher-name>MIT Press</publisher-name>
        <publisher-loc>Cambridge, MA</publisher-loc>
        <year iso-8601-date="1998">1998</year>
        <pub-id pub-id-type="doi">10.1109/TNN.1998.712192</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
