<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3153</article-id>
<article-id pub-id-type="doi">10.21105/joss.03153</article-id>
<title-group>
<article-title>TRUNAJOD: A text complexity library to enhance natural
language processing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1540-7164</contrib-id>
<string-name>Diego A. Palma</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Christian Soto</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Mónica Veliz</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Bruno Karelovic</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Bernardo Riffo</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Universidad de Concepción</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-03-03">
<day>3</day>
<month>3</month>
<year>2020</year>
</pub-date>
<volume>6</volume>
<issue>60</issue>
<fpage>3153</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>natural language processing</kwd>
<kwd>machine learning</kwd>
<kwd>text complexity</kwd>
<kwd>text coherence</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>We present <monospace>TRUNAJOD</monospace>, a text complexity
  analysis tool that includes a wide variety of linguistics measurements
  that can be extracted from texts as an approximation for readability,
  coherence, and cohesion. The features that
  <monospace>TRUNAJOD</monospace> can extract from the text are based on
  the literature and can be separated into the following categories:
  discourse markers, emotions, entity grid-based measurements,
  givenness, lexical-semantic norms, semantic measures, surface proxies,
  etc. In this first version of <monospace>TRUNAJOD</monospace>, we
  mainly support the Spanish language, but several features support any
  language that has proper natural language processing POS tagging and
  dependency parsing capabilities. Finally, we show how TRUNAJOD could
  be used in applied research.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><monospace>TRUNAJOD</monospace> aims to address three
  challenges:</p>
  <list list-type="order">
    <list-item>
      <p>A standardized API for text complexity measurements</p>
    </list-item>
    <list-item>
      <p>An open-source code, so any researcher in the linguistics field
      could contribute to it</p>
    </list-item>
    <list-item>
      <p>Easy-to-build applications and tools that rely upon text
      complexity assessment</p>
    </list-item>
  </list>
  <p>Other tools aim to make it easy for the public to get coherence and
  cohesion metrics. One such tool is TAACO
  (<xref alt="Crossley et al., 2019" rid="ref-crossley2019tool" ref-type="bibr">Crossley
  et al., 2019</xref>), which is written in Python and can be freely
  downloaded. A problem with TAACO is that it is a desktop application,
  which encloses the code. This makes it impossible to contribute
  modifications or new features, as it is a closed system. Moreover, it
  does not implement other relevant features to assess cohesion and
  coherence of discourse, for example, entity grid-based features. One
  open-source project with this purpose is the
  <monospace>Cohere</monospace> framework
  (<xref alt="Smith et al., 2016" rid="ref-smith2016cohere" ref-type="bibr">Smith
  et al., 2016</xref>), which is written in a mixture of Java and
  Python. However, it does not seem to be actively maintained and it
  does not implement other measurements that could be used by other
  researchers. On the other hand, most of the tools only support English
  languages and do not provide support for a plethora of metrics
  available in a comprehensible API. <monospace>TRUNAJOD</monospace>
  aims to be different, in the sense that we do not present a closed
  system, but rather, an open-source project, trying to follow the best
  Python development patterns. Furthermore, we rely on
  <monospace>spaCy</monospace>, enabling us to support not only one
  language but multiple languages for coherence and cohesion tasks,
  which enables <monospace>TRUNAJOD</monospace> to improve its
  performance when <monospace>spaCy</monospace> does, promoting
  collaboration.</p>
  <p>Moreover, <monospace>TRUNAJOD</monospace> not only implements
  state-of-the-art measurements for text complexity assessment, but also
  bundles new sets of predictors for this task. In this sense,
  <monospace>TRUNAJOD</monospace> ́s contributions are:</p>
  <list list-type="bullet">
    <list-item>
      <p>Fixing periphrasis of texts, because many NLP tools have issues
      dealing with periphrasis. In this release, this only applies to
      Spanish.</p>
    </list-item>
    <list-item>
      <p>Adding heuristics for measurements based on clause count.
      TRUNAJOD provides a new algorithm for clause segmentation.</p>
    </list-item>
    <list-item>
      <p>TRUNAJOD provides several approximations to narrativity in
      these new clause segmentation-dependent indices.</p>
    </list-item>
  </list>
  <p>Text complexity assessment is a natural language processing task
  that can be applied to multiple problems, such as automatic
  summarization, automatic essay scoring, automatic summary evaluation,
  intelligent tutoring systems, and so on. Text complexity is usually
  related to the readability of a text, which is dependent on several of
  its intrinsic properties, mainly cohesion and coherence.</p>
  <p>Automatic coherence evaluation is an open problem, and there have
  been several studies addressing it. On one side, text coherence
  assessment has been related to how sentences connect either
  semantically, or by co-referencing noun phrases. In the semantic view
  of coherence, Latent Semantic Analysis (LSA)
  (<xref alt="Foltz et al., 1998" rid="ref-foltz1998measurement" ref-type="bibr">Foltz
  et al., 1998</xref>) has been widely used because of its simplicity.
  In essence, sentences are represented as vectors, and the coherence of
  the text is computed as the average sentence similarity, using
  similarity vector measurements (such as cosine distance). This
  approach has drawbacks, such as that sentence ordering does not
  matter. To solve this issue, other methodologies have been proposed
  based on discourse theory, in particular the centering theory. One
  such approach is entity grids
  (<xref alt="Barzilay &amp; Lapata, 2008" rid="ref-barzilay2008modeling" ref-type="bibr">Barzilay
  &amp; Lapata, 2008</xref>) and entity graphs
  (<xref alt="Guinaudeau &amp; Strube, 2013" rid="ref-guinaudeau2013graph" ref-type="bibr">Guinaudeau
  &amp; Strube, 2013</xref>) that treat coherence as to how are entities
  take different roles between sentences and how are they connected in
  the text. <monospace>TRUNAJOD</monospace> implements all these models,
  and thus <monospace>TRUNAJOD</monospace> can compute coherence based
  on sentence similarities using word vectors.
  <monospace>TRUNAJOD</monospace> also provides an API for dealing with
  entity grids and entity graphs, to extract such measurements.</p>
  <p>A downside of the previous approaches for text complexity is that
  they only capture either CORPUS-based semantics or relationships
  between entities. Additionally, entity grids rely heavily on the
  dependency parser at hand and the co-reference resolution used because
  an entity might be mentioned in several ways across a text. The
  problem with this is that these measurements might be noisy depending
  on the use case, and simpler measurements would fit better in such
  cases. In <monospace>TRUNAJOD</monospace>, we compute several surface
  proxies that have been used by several state-of-the-art text
  assessment tools
  (<xref alt="McNamara et al., 2014" rid="ref-mcnamara2014automated" ref-type="bibr">McNamara
  et al., 2014</xref>)
  (<xref alt="Page, 1994" rid="ref-page1994computer" ref-type="bibr">Page,
  1994</xref>). Such surface proxies try to approximate intrinsic
  properties of the text such as narrativity, connectiveness, givenness,
  cohesion, and coherence. <monospace>TRUNAJOD</monospace> includes
  classical measurements such as word count, sentence count,
  pronoun-noun ratio, type-token ratios, frequency index, etc. Moreover,
  <monospace>TRUNAJOD</monospace> comes bundled with heuristics to
  compute clause count-based metrics, such as subordination and clause
  length, among others. To achieve this, <monospace>TRUNAJOD</monospace>
  adds periphrasis tags to the text to heuristically segment
  clauses.</p>
  <p>One drawback of using surface proxies (shallow measurements) is
  that they do not capture all the properties of the text, and just rely
  on approximations that are captured from raw text. In some use cases,
  this is not desirable (e.g., automatic essay scoring, intelligent
  tutoring systems), and these measurements should be complemented with
  other measurements that are desirable in those use cases, such as
  lexical-semantic norms and word emotions. Lexical semantic norms are
  norms for words that are related to the psychological degree of
  activation of a word in the reader
  (<xref alt="Guasch et al., 2016" rid="ref-guasch2016spanish" ref-type="bibr">Guasch
  et al., 2016</xref>). Some examples of such variables are
  concreteness, imageability, familiarity, arousal, valence. These
  variables might be used for reading comprehension tools (e.g., in
  reading comprehension assessments, it is desirable that feedback is
  concrete). Moreover, emotions could be used in such cases, and even in
  opinion mining
  (<xref alt="Sidorov et al., 2012" rid="ref-sidorov2012empirical" ref-type="bibr">Sidorov
  et al., 2012</xref>). <monospace>TRUNAJOD</monospace> comes bundled
  with both types of features, and thus, average lexical-semantic norms
  and emotions could be extracted from the text.</p>
  <p><monospace>TRUNAJOD</monospace> architectural design is shown in
  Figure 1.</p>
  <fig>
    <caption><p><monospace>TRUNAJOD</monospace> architectural
    design.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="imgs/figure1.png" xlink:title="" />
  </fig>
  <p>Basically, <monospace>TRUNAJOD</monospace> API takes as input a
  spaCy Doc and <monospace>TRUNAJOD</monospace> models (lemmatizer,
  synonym map, lexical-semantic norm map, etc.) and then it can compute
  supported text complexity metrics. It is worth noting that
  <monospace>TRUNAJOD</monospace> has available downloadable models from
  its GitHub repository, but currently only Spanish models are
  available. Nevertheless, it should be straightforward adding models
  for different languages.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This research was supported by FONDEF (Chile) under Grant IT17I0051
  “Desarrollo de una herramienta computacional para evaluación
  automática de textos del Sistema escolar chileno” (“Development of a
  computational tool for automatic assessment of Chilean school
  texts”)</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-foltz1998measurement">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Foltz</surname><given-names>Peter W.</given-names></name>
          <name><surname>Kintsch</surname><given-names>Walter</given-names></name>
          <name><surname>Landauer</surname><given-names>Thomas K</given-names></name>
        </person-group>
        <article-title>The measurement of textual coherence with latent semantic analysis</article-title>
        <source>Discourse Processes</source>
        <publisher-name>Routledge</publisher-name>
        <year iso-8601-date="1998">1998</year>
        <volume>25</volume>
        <issue>2-3</issue>
        <uri> 
                  https://doi.org/10.1080/01638539809545029
              
          </uri>
        <pub-id pub-id-type="doi">10.1080/01638539809545029</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-barzilay2008modeling">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Barzilay</surname><given-names>Regina</given-names></name>
          <name><surname>Lapata</surname><given-names>Mirella</given-names></name>
        </person-group>
        <article-title>Modeling local coherence: An entity-based approach</article-title>
        <source>Computational Linguistics</source>
        <publisher-name>MIT Press</publisher-name>
        <year iso-8601-date="2008">2008</year>
        <volume>34</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
    <ref id="ref-guinaudeau2013graph">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Guinaudeau</surname><given-names>Camille</given-names></name>
          <name><surname>Strube</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>Graph-based local coherence modeling</article-title>
        <source>Proceedings of the 51st annual meeting of the association for computational linguistics (Volume 1: Long papers)</source>
        <year iso-8601-date="2013">2013</year>
      </element-citation>
    </ref>
    <ref id="ref-mcnamara2014automated">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>McNamara</surname><given-names>Danielle S</given-names></name>
          <name><surname>Graesser</surname><given-names>Arthur C</given-names></name>
          <name><surname>McCarthy</surname><given-names>Philip M</given-names></name>
          <name><surname>Cai</surname><given-names>Zhiqiang</given-names></name>
        </person-group>
        <source>Automated evaluation of text and discourse with Coh-Metrix</source>
        <publisher-name>Cambridge University Press</publisher-name>
        <year iso-8601-date="2014">2014</year>
      </element-citation>
    </ref>
    <ref id="ref-page1994computer">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Page</surname><given-names>Ellis Batten</given-names></name>
        </person-group>
        <article-title>Computer grading of student prose, using modern concepts and software</article-title>
        <source>The Journal of Experimental Education</source>
        <publisher-name>Routledge</publisher-name>
        <year iso-8601-date="1994">1994</year>
        <volume>62</volume>
        <issue>2</issue>
        <uri> 
                  https://doi.org/10.1080/00220973.1994.9943835
              
          </uri>
        <pub-id pub-id-type="doi">10.1080/00220973.1994.9943835</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-smith2016cohere">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Smith</surname><given-names>Karin Sim</given-names></name>
          <name><surname>Aziz</surname><given-names>Wilker</given-names></name>
          <name><surname>Specia</surname><given-names>Lucia</given-names></name>
        </person-group>
        <article-title>Cohere: A toolkit for local coherence</article-title>
        <source>Proceedings of the tenth international conference on language resources and evaluation (LREC’16)</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-guasch2016spanish">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Guasch</surname><given-names>Marc</given-names></name>
          <name><surname>Ferré</surname><given-names>Pilar</given-names></name>
          <name><surname>Fraga</surname><given-names>Isabel</given-names></name>
        </person-group>
        <article-title>Spanish norms for affective and lexico-semantic variables for 1,400 words</article-title>
        <source>Behavior Research Methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <volume>48</volume>
        <issue>4</issue>
        <uri> 
                  https://doi.org/10.3758/s13428-015-0684-y
              
          </uri>
        <pub-id pub-id-type="doi">10.3758/s13428-015-0684-y</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-crossley2019tool">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Crossley</surname><given-names>Scott A</given-names></name>
          <name><surname>Kyle</surname><given-names>Kristopher</given-names></name>
          <name><surname>Dascalu</surname><given-names>Mihai</given-names></name>
        </person-group>
        <article-title>The tool for the automatic analysis of Cohesion 2.0: Integrating semantic similarity and text overlap</article-title>
        <source>Behavior research methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>51</volume>
        <issue>1</issue>
        <uri>
            https://doi.org/10.3758/s13428-018-1142-4
          </uri>
        <pub-id pub-id-type="doi">10.3758/s13428-018-1142-4</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-sidorov2012empirical">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Sidorov</surname><given-names>G</given-names></name>
          <name><surname>Miranda-Jiménez</surname><given-names>S</given-names></name>
          <name><surname>Viveros-Jiménez</surname><given-names>F</given-names></name>
          <name><surname>Gelbukh</surname><given-names>A</given-names></name>
          <name><surname>Castro-Sánchez</surname><given-names>N</given-names></name>
          <name><surname>Velásquez</surname><given-names>F</given-names></name>
          <name><surname>Dıaz-Rangel</surname><given-names>I</given-names></name>
          <name><surname>Suárez-Guerra</surname><given-names>S</given-names></name>
          <name><surname>Trevino</surname><given-names>A</given-names></name>
          <name><surname>Gordon</surname><given-names>J</given-names></name>
        </person-group>
        <article-title>Empirical study of opinion mining in Spanish tweets. MICAI 2012</article-title>
        <source>Lect. Notes Comput. Sci.</source>
        <year iso-8601-date="2012">2012</year>
        <volume>7629</volume>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
