<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3545</article-id>
<article-id pub-id-type="doi">10.21105/joss.03545</article-id>
<title-group>
<article-title>sorn: A Python package for Self Organizing Recurrent
Neural Network</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7314-0261</contrib-id>
<string-name>Saranraj Nambusubramaniyan</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Indian center for Robotics Innovation and
Smart-intelligence(IRIS-i), India</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Institute of Cognitive Science, Universität Osnabrück,
Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-07-24">
<day>24</day>
<month>7</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>65</issue>
<fpage>3545</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Spiking Neural Network</kwd>
<kwd>OpenAI Gym</kwd>
<kwd>Neuromorphic computing</kwd>
<kwd>Neuroscience</kwd>
<kwd>Self Organizing Networks</kwd>
<kwd>Hebbian Learning</kwd>
<kwd>Associative Networks</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The self-organizing recurrent neural (SORN) network is a class of
  neuro-inspired artificial networks. This class of networks has been
  shown to mimic the ability of neocortical circuits to learn and adapt
  through neuroplasticity mechanisms. Structurally, the SORN network
  consists of a pool of excitatory neurons and a small population of
  inhibitory neurons. The network uses five basic plasticity mechanisms
  found in the neocortex of the brain, namely, spike-timing-dependent
  plasticity, intrinsic plasticity, synaptic scaling, inhibitory
  spike-timing-dependent plasticity, and structural plasticity
  (<xref alt="Lazar et al., 2009" rid="ref-lazar2009sorn" ref-type="bibr">Lazar
  et al., 2009</xref>;
  <xref alt="Papa et al., 2017" rid="ref-papa2017criticality" ref-type="bibr">Papa
  et al., 2017</xref>;
  <xref alt="Zheng et al., 2013" rid="ref-zheng2013network" ref-type="bibr">Zheng
  et al., 2013</xref>) to optimize its parameters. Using mathematical
  tools, a SORN network simplifies the underlying structural and
  functional connectivity mechanisms responsible for learning and memory
  in the brain.</p>
  <fig>
    <caption><p>SORN network</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="../imgs/SORN1.png" xlink:title="" />
  </fig>
  <p><monospace>sorn</monospace> is a Python package designed for Self
  Organizing Recurrent Neural Networks. While it was originally
  developed for SORN networks, it can also serve as an ideal research
  package for Liquid State Machines
  (<xref alt="Jaeger, 2002" rid="ref-jaeger2002adaptive" ref-type="bibr">Jaeger,
  2002</xref>;
  <xref alt="Jaeger et al., 2007" rid="ref-jaeger2007special" ref-type="bibr">Jaeger
  et al., 2007</xref>) in general. The detailed documentation can be
  found at
  <ext-link ext-link-type="uri" xlink:href="self-organizing-recurrent-neural-networks.readthedocs.io">https://self-organizing-recurrent-neural-networks.readthedocs.io/en/latest/</ext-link>.
  To extend the potential applications of this network architecture, a
  demonstrative example of a neuro-robotics experiment using OpenAI Gym
  (<xref alt="Brockman et al., 2016" rid="ref-brockman2016openai" ref-type="bibr">Brockman
  et al., 2016</xref>) is provided at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Saran-nns/sorn/">sorn
  package</ext-link>.</p>
  <sec id="statement-of-need">
    <title>Statement of need</title>
    <p>Reservoir computing (RC) models are neuroinspired artificial
    neural networks. RC networks have either sparsely or densely
    connected units with fixed connection weights. Unlike other RC
    models, SORN has synaptic weights controlled by neuroinspired
    plasticity mechanisms. The network has two distinct pools of
    excitatory and inhibitory reservoirs that compete to remain in a
    subcritical state suitable for learning. The subcritical state is a
    state between chaos and order, also called the “edge of chaos”. In
    this state, the network has momentum with a strong affinity for
    order, but is sensitive to external perturbations. Through
    plasticity mechanisms, the network has the ability to overcome the
    perturbations and return to its subcritical dynamics. This
    self-adaptive behavior is also referred to as self-organization. To
    build such a network with a synergistic combination of plasticity
    mechanisms from scratch requires a deeper understanding of
    neurophysiology and soft computing. <monospace>sorn</monospace>
    reduces the cognitive load of theorists, experimenters or
    researchers by encapsulating all plasticity mechanisms with a high
    degree of reliability and flexibility.</p>
    <p>There are few other open source codes
    <ext-link ext-link-type="uri" xlink:href="https://github.com/delpapa/SORN">sorn
    v1</ext-link>,
    <ext-link ext-link-type="uri" xlink:href="https://github.com/delpapa/SORN_V2">sorn
    v2</ext-link>, for SORN networks, but they are application-specific
    and are not general-purpose software packages. However,
    <monospace>sorn</monospace> is a flexible package that allows
    researchers to develop the network of their interest, providing them
    the freedom to choose the combination of plasticity rules of their
    choice. Moreover, it is easy to integrate
    <monospace>sorn</monospace> with machine learning frameworks such as
    PyTorch and reinforcement learning toolkits such as OpenAI Gym.
    Overall, <monospace>sorn</monospace> provides a research environment
    for computational neuroscientists to study self-organization,
    adaptation, learning, memory, and behavior of brain circuits by
    reverse-engineering neural plasticity mechanisms.</p>
  </sec>
  <sec id="library-overview">
    <title>Library Overview</title>
    <p>The package <monospace>sorn</monospace> is heavily dependent on
    numpy
    (<xref alt="Harris et al., 2020" rid="ref-harris2020array" ref-type="bibr">Harris
    et al., 2020</xref>) for numerical computation and analysis methods,
    seaborn and matplotlib
    (<xref alt="Barrett et al., 2005" rid="ref-barrett2005matplotlib" ref-type="bibr">Barrett
    et al., 2005</xref>) for visualization. The network is constructed
    in five classes; the object <monospace>SORN</monospace> encapsulates
    all the required functions that instantiate network variables such
    as connection weights and thresholds.
    <monospace>Plasticity</monospace> inherits objects from
    <monospace>SORN</monospace> and implements plasticity rules with
    methods <monospace>stdp()</monospace>, <monospace>ip()</monospace>,
    <monospace>ss()</monospace>, <monospace>sp()</monospace> and
    <monospace>istdp()</monospace>. <monospace>NetworkState</monospace>
    has methods that evaluate excitatory and inhibitory network states
    at each time step and finally
    <monospace>MatrixCollection</monospace> objects behave like a memory
    cache. It collects the network states and keeps track of variables
    such as weights and thresholds as the network evolves during
    simulation and training.</p>
    <p>The network can be instantiated, simulated and trained using two
    classes: <monospace>Simulator</monospace> and
    <monospace>Trainer</monospace>, which inherit objects from
    <monospace>SORN</monospace>.</p>
  </sec>
  <sec id="sorn-network-model">
    <title>SORN Network Model</title>
    <p>As defined in
    (<xref alt="Lazar et al., 2009" rid="ref-lazar2009sorn" ref-type="bibr">Lazar
    et al., 2009</xref>;
    <xref alt="Zheng et al., 2013" rid="ref-zheng2013network" ref-type="bibr">Zheng
    et al., 2013</xref>) the activity of neurons in the excitatory and
    inhibitory pool is given by the following state equations,</p>
    <p><named-content id="es" content-type="equation"><disp-formula><alternatives>
    <tex-math><![CDATA[\label{es}
    x_i(t+1) =  \Theta\left (\sum_{j=1}^{N^\text{E}} {W_{ij}^\text{EE}(t)} {x_{j}(t)} - \sum_{j=1}^{N^\text{I}}W_{ik}^\text{EI}(t) y_{k}(t)+u_{i}(t) - T_{i}^\text{E}(t)+\xi_\text{E}(t)\right)]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Θ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext mathvariant="normal">E</mml:mtext></mml:msup></mml:munderover><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EE</mml:mtext></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext mathvariant="normal">I</mml:mtext></mml:msup></mml:munderover><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EI</mml:mtext></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">E</mml:mtext></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mtext mathvariant="normal">E</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
    <p><named-content id="is" content-type="equation"><disp-formula><alternatives>
    <tex-math><![CDATA[\label{is}
    y_i(t+1)=\Theta\left(\sum_{j=1}^{N_i}W_{ij}^\text{IE}(t) x_j(t)-T_i^\text{I}+ \xi_\text{I}(t)\right)]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>Θ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:munderover><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">IE</mml:mtext></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">I</mml:mtext></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mtext mathvariant="normal">I</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
    <p><inline-formula><alternatives>
    <tex-math><![CDATA[W_{ij}^\text{EE}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EE</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>
    - Connection strength between excitatory neurons</p>
    <p><inline-formula><alternatives>
    <tex-math><![CDATA[W_{ik}^\text{EI}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EI</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>
    - Synaptic strenght from Inhibitory to excitatory network</p>
    <p><inline-formula><alternatives>
    <tex-math><![CDATA[W_{ki}^\text{IE}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mtext mathvariant="normal">IE</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>
    - Synaptic strenght from Exciatory to inhibitory network</p>
    <p><inline-formula><alternatives>
    <tex-math><![CDATA[x_{j}(t)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    - Presynaptic excitatory neuron state at
    <inline-formula><alternatives>
    <tex-math><![CDATA[t]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula></p>
    <p><inline-formula><alternatives>
    <tex-math><![CDATA[y_{k}(t)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    - Presynaptic inhibitory neuron state at
    <inline-formula><alternatives>
    <tex-math><![CDATA[t]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula></p>
    <p><inline-formula><alternatives>
    <tex-math><![CDATA[x_{i}(t)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    - Postsynaptic neuron state at <inline-formula><alternatives>
    <tex-math><![CDATA[t]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula></p>
    <p><inline-formula><alternatives>
    <tex-math><![CDATA[u_{i}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    - External stimuli</p>
    <p><inline-formula><alternatives>
    <tex-math><![CDATA[T_{i}(t)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    - Firing threshold of the neuron <inline-formula><alternatives>
    <tex-math><![CDATA[i]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>
    at time <inline-formula><alternatives>
    <tex-math><![CDATA[t]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula></p>
  </sec>
  <sec id="plasticity-rules">
    <title>Plasticity Rules</title>
    <sec id="spike-timing-dependent-plasticity">
      <title>Spike Timing Dependent Plasticity</title>
      <p>Spike Timing Dependent Plasticity (STDP) alters synaptic
      efficacy between excitatory neurons based on the spike timing
      between presynaptic neuron <inline-formula><alternatives>
      <tex-math><![CDATA[j]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>j</mml:mi></mml:math></alternatives></inline-formula>
      and postsynaptic neuron <inline-formula><alternatives>
      <tex-math><![CDATA[i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>.</p>
      <p><named-content id="stdp" content-type="equation"><disp-formula><alternatives>
      <tex-math><![CDATA[\label{stdp}
      \Delta W_{ij}^\text{EE}=\eta_\text{STDP}\left(x_i(t)x_j(t-1)-x_i(t-1)x_j(t)\right)]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EE</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mtext mathvariant="normal">STDP</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
      <p>where,</p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[W_{ij}^\text{EE}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EE</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>
      - Connection strength between excitatory neurons</p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[\eta_\text{STDP}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>η</mml:mi><mml:mtext mathvariant="normal">STDP</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      - STDP learning rate</p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[x_j(t-1)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      - Presynaptic neuron state at <inline-formula><alternatives>
      <tex-math><![CDATA[t-1]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[x_i(t)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      - Postsynaptic neuron state at <inline-formula><alternatives>
      <tex-math><![CDATA[t]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula></p>
    </sec>
    <sec id="intrinsic-plasticity">
      <title>Intrinsic Plasticity</title>
      <p>Intrinsic Plasticity (IP) updates the firing threshold of
      excitatory neurons based on the state of the neuron at each time
      step. It increases the threshold if the neuron is firing and
      decreases it otherwise.</p>
      <p><named-content id="ip" content-type="equation"><disp-formula><alternatives>
      <tex-math><![CDATA[\label{ip}
      T_i(t+1)=T_i(t)+\eta_\text{IP}(x_i(t)-H_\text{IP})]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mtext mathvariant="normal">IP</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mtext mathvariant="normal">IP</mml:mtext></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
      <p>where,</p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[T_i(t)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      - Firing threshold of the neuron <inline-formula><alternatives>
      <tex-math><![CDATA[i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>
      at time <inline-formula><alternatives>
      <tex-math><![CDATA[t]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula></p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[\eta_\text{IP}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>η</mml:mi><mml:mtext mathvariant="normal">IP</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      - Intrinsic plasticity step size</p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[H_\text{IP}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>H</mml:mi><mml:mtext mathvariant="normal">IP</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      - Target firing rate of the neuron</p>
    </sec>
    <sec id="structural-plasticity">
      <title>Structural Plasticity</title>
      <p>Structural Plasticity (SP) is responsible for creating new
      synapses between excitatory neurons at a rate of about 1
      connection per 10th time step.</p>
    </sec>
    <sec id="synaptic-scaling">
      <title>Synaptic Scaling</title>
      <p>Synaptic Scaling (SS) normalizes the synaptic strengths of
      presynaptic neurons and prevents network activity from declining
      or exploding.</p>
      <p><named-content id="ss" content-type="equation"><disp-formula><alternatives>
      <tex-math><![CDATA[\label{ss}
      W_{ij}^\text{EE}(t)=W_{ij}^\text{EE}(t)/\sum{W_{ij}^\text{EE}(t)}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EE</mml:mtext></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EE</mml:mtext></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>/</mml:mi><mml:mo>∑</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EE</mml:mtext></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
    </sec>
    <sec id="inhibitory-spike-timing-dependent-plasticity">
      <title>Inhibitory Spike Timing Dependent Plasticity</title>
      <p>Inhibitory Spike Timing Dependent Plasticity (iSTDP) is
      responsible for controlling synaptic strengths from the inhibitory
      to the excitatory network.</p>
      <p><named-content id="istdp" content-type="equation"><disp-formula><alternatives>
      <tex-math><![CDATA[\label{istdp}
      \Delta W_{ij}^\text{EI}=\eta_\text{iSTDP}\left(y_j(t-1)(1-x_i(t)(1+\frac{1}{\mu_\text{IP}}))\right)]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EI</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mtext mathvariant="normal">iSTDP</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>μ</mml:mi><mml:mtext mathvariant="normal">IP</mml:mtext></mml:msub></mml:mfrac><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
      <p>where,</p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[W_{ij}^\text{EI}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">EI</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>
      - Synaptic strength from Inhibitory to excitatory network</p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[\eta_\text{iSTDP}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>η</mml:mi><mml:mtext mathvariant="normal">iSTDP</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      - Inhibitory STDP learning rate</p>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[\mu_\text{IP}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>μ</mml:mi><mml:mtext mathvariant="normal">IP</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      - Mean firing rate of the neuron</p>
      <p>Note that the connection strength from excitatory to inhibitory
      (<inline-formula><alternatives>
      <tex-math><![CDATA[W_{ij}^\text{IE}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mtext mathvariant="normal">IE</mml:mtext></mml:msubsup></mml:math></alternatives></inline-formula>)
      remains fixed at the initial state and also the connections
      between inhibitory neurons are not allowed.</p>
    </sec>
  </sec>
  <sec id="sample-simulation-methods">
    <title>Sample Simulation methods</title>
    <code language="python">
# Sample input
num_features = 10
time_steps = 200
inputs = numpy.random.rand(num_features,time_steps)

state_dict,E,I,R,C=Simulator.simulate_sorn(inputs=inputs,phase='plasticity',

                                        matrices=None,noise=True,

                                        time_steps=time_steps,ne=200,

                                        nu=num_features)</code>
    <p><monospace>simulate_sorn</monospace> returns the dictionary of
    network state variables of the previous time steps, the excitatory
    and inhibitory network activity of the whole simulation period, and
    also the recurrent activity and the number of active connections at
    each time step. To continue the simulation, load the matrices
    returned in the previous step as,</p>
    <code language="python">state_dict,E,I,R,C=Simulator.simulate_sorn(inputs=inputs,phase='plasticity',

                                        matrices=state_dict, noise=True,

                                        time_steps=time_steps,

                                        ne = 200,nu=num_features)
    </code>
    <sec id="network-output-descriptions">
      <title>Network Output Descriptions</title>
      <p><monospace>state_dict</monospace> - Dictionary of connection
      weights (‘Wee’, ‘Wei’, ‘Wie’) ,</p>
      <preformat>           Excitatory network activity ('X'),

           Inhibitory network activities('Y'),

           Threshold values ('Te', 'Ti')</preformat>
      <p><monospace>E</monospace> - Collection of Excitatory network
      activity of entire simulation period</p>
      <p><monospace>I</monospace> - Collection of Inhibitory network
      activity of entire simulation period</p>
      <p><monospace>R</monospace> - Collection of Recurrent network
      activity of entire simulation period</p>
      <p><monospace>C</monospace> - List of number of active connections
      in the Excitatory pool at each time step</p>
    </sec>
  </sec>
  <sec id="sample-training-methods">
    <title>Sample Training methods</title>
    <code language="python">from sorn import Trainer
inputs = np.random.rand(num_features,1)

# Under all plasticity mechanisms
state_dict,E,I,R,C=Trainer.train_sorn(inputs=inputs,phase='plasticity',

                                    matrices=state_dict,

                                    nu=num_features,time_steps=1)



# Resume the training without any plasticity mechanisms

state_dict,E,I,R,C=Trainer.train_sorn(inputs=inputs,phase='training',

                                    matrices=state_dict,

                                    nu=num_features,time_steps=1)</code>
    <p>To turn off any plasticity mechanisms during the simulation or
    training phase, you can use the argument
    <monospace>freeze</monospace>. For example, to stop intrinsic
    plasticity during the training phase,</p>
    <code language="python">
state_dict,E,I,R,C=Trainer.train_sorn(inputs=inputs,phase='plasticity',

                                    matrices=None,noise=True,

                                    time_steps=1,ne=200,

                                    nu=num_features,freeze=['ip'])
    </code>
    <p>The other options for <monospace>freeze</monospace> argument
    are,</p>
    <p><monospace>stdp</monospace> - Spike Timing Dependent
    Plasticity</p>
    <p><monospace>ss</monospace> - Synaptic Scaling</p>
    <p><monospace>sp</monospace> - Structural Plasticity</p>
    <p><monospace>istdp</monospace> - Inhibitory Spike Timing Dependent
    Plasticity</p>
    <p>The <monospace>simulate_sorn</monospace> and
    <monospace>train_sorn</monospace> methods accepts the following
    keyword arguments:</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="21%" />
          <col width="79%" />
        </colgroup>
        <thead>
          <tr>
            <th>kwargs</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><monospace>inputs</monospace></td>
            <td>External stimulus</td>
          </tr>
          <tr>
            <td><monospace>phase</monospace></td>
            <td><monospace>plasticity</monospace> or
            <monospace>training</monospace></td>
          </tr>
          <tr>
            <td><monospace>matrices</monospace></td>
            <td><monospace>state_dict</monospace> to resume simulation
            otherwise <monospace>None</monospace> to intialize new
            network</td>
          </tr>
          <tr>
            <td><monospace>time_steps</monospace></td>
            <td><monospace>simulaton</monospace> total time steps. For
            <monospace>training</monospace> should be 1</td>
          </tr>
          <tr>
            <td><monospace>noise</monospace></td>
            <td>If <monospace>True</monospace>, Gaussian white noise
            will be added to excitatory field potentials</td>
          </tr>
          <tr>
            <td><monospace>freeze</monospace></td>
            <td>To drop any given plasticity mechanism(s) among
            [<monospace>'ip'</monospace>,<monospace>'stdp'</monospace>,<monospace>'istdp'</monospace>,<monospace>'ss'</monospace>,
            <monospace>'sp'</monospace>]</td>
          </tr>
          <tr>
            <td><monospace>ne</monospace></td>
            <td>Number of Excitatory neurons in the network</td>
          </tr>
          <tr>
            <td><monospace>nu</monospace></td>
            <td>Number of input units among excitatory neurons</td>
          </tr>
          <tr>
            <td><monospace>network_type_ee</monospace></td>
            <td><monospace>sparse</monospace> or
            <monospace>dense</monospace> connection between excitatory
            neurons</td>
          </tr>
          <tr>
            <td><monospace>network_type_ei</monospace></td>
            <td><monospace>sparse</monospace> or
            <monospace>dense</monospace> connection from inhibitory and
            excitatory neurons</td>
          </tr>
          <tr>
            <td><monospace>network_type_ie</monospace></td>
            <td><monospace>sparse</monospace> or
            <monospace>dense</monospace> connection from excitatory and
            inhibitory neurons</td>
          </tr>
          <tr>
            <td><monospace>lambda_ee</monospace></td>
            <td>Connection density between excitatory networks if
            network type is <monospace>sparse</monospace></td>
          </tr>
          <tr>
            <td><monospace>lambda_ei</monospace></td>
            <td>Density of connections from inhibitory to excitatory
            networks if network type is
            <monospace>sparse</monospace></td>
          </tr>
          <tr>
            <td><monospace>lambda_ie</monospace></td>
            <td>Density of connections from inhibitory to excitatory
            networks if network type is
            <monospace>sparse</monospace></td>
          </tr>
          <tr>
            <td><monospace>eta_stdp</monospace></td>
            <td>Hebbian learning rate of excitatory synapses</td>
          </tr>
          <tr>
            <td><monospace>eta_inhib</monospace></td>
            <td>Hebbian learning rate synapses from inhibitory to
            excitatory</td>
          </tr>
          <tr>
            <td><monospace>eta_ip</monospace></td>
            <td>Learning rate of excitatory neuron threshold</td>
          </tr>
          <tr>
            <td><monospace>te_max</monospace></td>
            <td>Maximum of excitatory neuron threshold range</td>
          </tr>
          <tr>
            <td><monospace>ti_max</monospace></td>
            <td>Maximum of inhibitory neuron threshold range</td>
          </tr>
          <tr>
            <td><monospace>ti_min</monospace></td>
            <td>Minimum of inhibitory neuron threshold range</td>
          </tr>
          <tr>
            <td><monospace>te_min</monospace></td>
            <td>Minimum of excitatory neuron threshold range</td>
          </tr>
          <tr>
            <td><monospace>mu_ip</monospace></td>
            <td>Target Mean firing rate of excitatory neuron</td>
          </tr>
          <tr>
            <td><monospace>sigma_ip</monospace></td>
            <td>Target Standard deviation of firing rate of excitatory
            neuron</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="analysis-functions">
      <title>Analysis functions</title>
      <p>The <monospace>sorn</monospace> package also includes necessary
      methods to investigate network properties. A few of the methods in
      the <monospace>Statistics</monospace> module are:</p>
      <table-wrap>
        <table>
          <colgroup>
            <col width="28%" />
            <col width="72%" />
          </colgroup>
          <thead>
            <tr>
              <th>methods</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><monospace>autocorr</monospace></td>
              <td>t-lagged auto correlation between neural activity</td>
            </tr>
            <tr>
              <td><monospace>fanofactor</monospace></td>
              <td>To verify poissonian process in spike generation of
              neuron(s)</td>
            </tr>
            <tr>
              <td><monospace>spike_source_entropy</monospace></td>
              <td>Measure the uncertainty about the origin of spike from
              the network using entropy</td>
            </tr>
            <tr>
              <td><monospace>firing_rate_neuron</monospace></td>
              <td>Spike rate of specific neuron</td>
            </tr>
            <tr>
              <td><monospace>firing_rate_network</monospace></td>
              <td>Spike rate of entire network</td>
            </tr>
            <tr>
              <td><monospace>avg_corr_coeff</monospace></td>
              <td>Average Pearson correlation coeffecient between
              neurons</td>
            </tr>
            <tr>
              <td><monospace>spike_times</monospace></td>
              <td>Time instants at which neuron spikes</td>
            </tr>
            <tr>
              <td><monospace>spike_time_intervals</monospace></td>
              <td>Inter spike intervals for each neuron</td>
            </tr>
            <tr>
              <td><monospace>hamming_distance</monospace></td>
              <td>Hamming distance between two network states</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>More details about the statistical and plotting tools in the
      package can be found at
      (<ext-link ext-link-type="uri" xlink:href="self-organizing-recurrent-neural-networks.readthedocs.io">https://self-organizing-recurrent-neural-networks.readthedocs.io/en/latest/</ext-link>)</p>
    </sec>
  </sec>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-lazar2009sorn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lazar</surname><given-names>Andreea</given-names></name>
          <name><surname>Pipa</surname><given-names>Gordon</given-names></name>
          <name><surname>Triesch</surname><given-names>Jochen</given-names></name>
        </person-group>
        <article-title>SORN: A self-organizing recurrent neural network</article-title>
        <source>Frontiers in computational neuroscience</source>
        <publisher-name>Frontiers</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>3</volume>
        <pub-id pub-id-type="doi">10.3389/neuro.10.019.2009</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-zheng2013network">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Zheng</surname><given-names>Pengsheng</given-names></name>
          <name><surname>Dimitrakakis</surname><given-names>Christos</given-names></name>
          <name><surname>Triesch</surname><given-names>Jochen</given-names></name>
        </person-group>
        <article-title>Network self-organization explains the statistics and dynamics of synaptic connection strengths in cortex</article-title>
        <source>PLoS computational biology</source>
        <publisher-name>Public Library of Science San Francisco, USA</publisher-name>
        <year iso-8601-date="2013">2013</year>
        <volume>9</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002848</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-jaeger2002adaptive">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Jaeger</surname><given-names>Herbert</given-names></name>
        </person-group>
        <article-title>Adaptive nonlinear system identification with echo state networks</article-title>
        <source>Advances in neural information processing systems</source>
        <year iso-8601-date="2002">2002</year>
        <volume>15</volume>
      </element-citation>
    </ref>
    <ref id="ref-jaeger2007special">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Jaeger</surname><given-names>Herbert</given-names></name>
          <name><surname>Maass</surname><given-names>Wolfgang</given-names></name>
          <name><surname>Principe</surname><given-names>Jose</given-names></name>
        </person-group>
        <article-title>Special issue on echo state networks and liquid state machines.</article-title>
        <publisher-name>Elsevier Science</publisher-name>
        <year iso-8601-date="2007">2007</year>
        <pub-id pub-id-type="doi">10.1016/j.neunet.2007.04.001</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-papa2017criticality">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Papa</surname><given-names>Bruno Del</given-names></name>
          <name><surname>Priesemann</surname><given-names>Viola</given-names></name>
          <name><surname>Triesch</surname><given-names>Jochen</given-names></name>
        </person-group>
        <article-title>Criticality meets learning: Criticality signatures in a self-organizing recurrent neural network</article-title>
        <source>PloS One</source>
        <publisher-name>Public Library of Science</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>12</volume>
        <issue>5</issue>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0178683</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-harris2020array">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Harris</surname><given-names>Charles R</given-names></name>
          <name><surname>Millman</surname><given-names>K Jarrod</given-names></name>
          <name><surname>Walt</surname><given-names>Stéfan J van der</given-names></name>
          <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
          <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
          <name><surname>Cournapeau</surname><given-names>David</given-names></name>
          <name><surname>Wieser</surname><given-names>Eric</given-names></name>
          <name><surname>Taylor</surname><given-names>Julian</given-names></name>
          <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
          <name><surname>Smith</surname><given-names>Nathaniel J</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Array programming with NumPy</article-title>
        <source>Nature</source>
        <publisher-name>Nature Publishing Group</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>585</volume>
        <issue>7825</issue>
      </element-citation>
    </ref>
    <ref id="ref-barrett2005matplotlib">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Barrett</surname><given-names>Paul</given-names></name>
          <name><surname>Hunter</surname><given-names>John</given-names></name>
          <name><surname>Miller</surname><given-names>J Todd</given-names></name>
          <name><surname>Hsu</surname><given-names>J-C</given-names></name>
          <name><surname>Greenfield</surname><given-names>Perry</given-names></name>
        </person-group>
        <article-title>Matplotlib–a portable python plotting package</article-title>
        <source>Astronomical data analysis software and systems XIV</source>
        <year iso-8601-date="2005">2005</year>
        <volume>347</volume>
      </element-citation>
    </ref>
    <ref id="ref-brockman2016openai">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Brockman</surname><given-names>Greg</given-names></name>
          <name><surname>Cheung</surname><given-names>Vicki</given-names></name>
          <name><surname>Pettersson</surname><given-names>Ludwig</given-names></name>
          <name><surname>Schneider</surname><given-names>Jonas</given-names></name>
          <name><surname>Schulman</surname><given-names>John</given-names></name>
          <name><surname>Tang</surname><given-names>Jie</given-names></name>
          <name><surname>Zaremba</surname><given-names>Wojciech</given-names></name>
        </person-group>
        <article-title>Openai gym</article-title>
        <source>arXiv preprint arXiv:1606.01540</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
