<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2761</article-id>
<article-id pub-id-type="doi">10.21105/joss.02761</article-id>
<title-group>
<article-title>pystiche: A Framework for Neural Style
Transfer</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5184-1622</contrib-id>
<string-name>Philip Meier</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-3325-7887</contrib-id>
<string-name>Volker Lohweg</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>inIT –- Institute Industrial IT, Technische Hochschule
Ostwestfalen-Lippe (TH-OWL)</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-10-15">
<day>15</day>
<month>10</month>
<year>2020</year>
</pub-date>
<volume>5</volume>
<issue>54</issue>
<fpage>2761</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Neural Style Transfer</kwd>
<kwd>framework</kwd>
<kwd>PyTorch</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The seminal work of Gatys, Ecker, and Bethge gave birth to the
  field of <italic>Neural Style Transfer</italic> (NST) in 2016
  (<xref alt="Gatys et al., 2016" rid="ref-GEB2016" ref-type="bibr">Gatys
  et al., 2016</xref>). The general idea behind an NST can be conveyed
  with only three images and two symbols:</p>
  <graphic mimetype="image" mime-subtype="jpeg" xlink:href="banner.jpg" />
  <p>In words: an NST describes the merger between the content and
  artistic style of two arbitrary images. This idea is nothing new in
  the field of <italic>Non-photorealistic rendering</italic> (NPR)
  (<xref alt="Gooch &amp; Gooch, 2001" rid="ref-GG2001" ref-type="bibr">Gooch
  &amp; Gooch, 2001</xref>). What distinguishes NST from traditional NPR
  approaches is its generality: an NST only needs a single arbitrary
  content and style image as input and thus “makes – for the first time
  – a generalized style transfer practicable”
  (<xref alt="Semmo et al., 2017" rid="ref-SID2017" ref-type="bibr">Semmo
  et al., 2017</xref>).</p>
  <p>Besides peripheral tasks, an NST at its core is the definition of
  an optimization criterion called <italic>perceptual loss</italic>,
  which estimates the perceptual quality of the stylized image. Usually
  the perceptual loss comprises a deep neural network that needs to
  supply encodings of images from various depths.</p>
  <p><monospace>pystiche</monospace> is a framework for NST written in
  Python and built upon the <italic>Deep Learning</italic> (DL)
  framework PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-PGMU002B2019" ref-type="bibr">Paszke
  et al., 2019</xref>). It provides modular and efficient
  implementations for commonly used perceptual losses
  (<xref alt="Gatys et al., 2016" rid="ref-GEB2016" ref-type="bibr">Gatys
  et al., 2016</xref>;
  <xref alt="Li &amp; Wand, 2016" rid="ref-LW2016" ref-type="bibr">Li
  &amp; Wand, 2016</xref>;
  <xref alt="Mahendran &amp; Vedaldi, 2015" rid="ref-MV2015" ref-type="bibr">Mahendran
  &amp; Vedaldi, 2015</xref>) as well as neural net architectures
  (<xref alt="Krizhevsky et al., 2012" rid="ref-KSH2012" ref-type="bibr">Krizhevsky
  et al., 2012</xref>;
  <xref alt="Simonyan &amp; Zisserman, 2014" rid="ref-SZ2014" ref-type="bibr">Simonyan
  &amp; Zisserman, 2014</xref>). This enables users to mix current
  state-of-the-art techniques with new ideas with ease.</p>
  <p>Due to its vivid nature, the field of NST gained a lot of traction
  in the short time after its emergence
  (<xref alt="Jing et al., 2019" rid="ref-JYFU002B2019" ref-type="bibr">Jing
  et al., 2019</xref>). While many new techniques or augmentations have
  been developed, the field lacks standardization, which is especially
  evident in the reference implementations of the authors.
  <monospace>pystiche</monospace> aims to fill this gap.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Currently, unlike DL, there exist no library or framework for
  implementing NST. Thus, authors of new NST techniques either implement
  everything from scratch or base their implementation upon existing
  ones of other authors. Both ways have their downsides: while the
  former dampens innovation due to the lengthy implementation of
  reusable parts, with the latter the author inherits the technical debt
  due to the rapid development pace of DL hard- and software. In order
  to overcome this, <monospace>pystiche</monospace> pursues similar
  goals as DL frameworks:</p>
  <list list-type="order">
    <list-item>
      <p><bold>Accessibility</bold> Starting off with NST can be quite
      overwhelming due to the sheer amount of techniques one has to know
      and be able to deploy. <monospace>pystiche</monospace> aims to
      provide an easy-to-use interface that reduces the necessary prior
      knowledge about NST and DL to a minimum.</p>
    </list-item>
    <list-item>
      <p><bold>Reproducibility</bold> Implementing NST from scratch is
      not only inconvenient but also error-prone.
      <monospace>pystiche</monospace> aims to provide reusable tools
      that let developers focus on their ideas rather than worrying
      about everything around it.</p>
    </list-item>
  </list>
  <p><monospace>pystiche</monospace>s core audience are researchers, but
  its easy-to-use user interface opens up the field of NST for
  recreational use by laypersons.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This contribution is part of the project <italic>Fused Security
  Features</italic>, which is funded by the <italic>Ministry for Culture
  and Science of North Rhine-Westphalia</italic> (MKW NRW) under the
  Grant ID <monospace>005-1703-0013</monospace>. The authors thank
  Julian Bültemeier for extensive internal testing.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-GEB2016">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Gatys</surname><given-names>Leon A.</given-names></name>
          <name><surname>Ecker</surname><given-names>Alexander. S.</given-names></name>
          <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
        </person-group>
        <article-title>Image style transfer using convolutional neural networks</article-title>
        <source>IEEE conference on computer vision and pattern recognition (CVPR)</source>
        <year iso-8601-date="2016">2016</year>
        <uri>https://arxiv.org/abs/1508.06576</uri>
        <pub-id pub-id-type="doi">10.1109/CVPR.2016.265</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-SZ2014">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Simonyan</surname><given-names>Karen</given-names></name>
          <name><surname>Zisserman</surname><given-names>Andrew</given-names></name>
        </person-group>
        <article-title>Very deep convolutional networks for large-scale image recognition</article-title>
        <source>Computing Research Repository (CoRR)</source>
        <year iso-8601-date="2014">2014</year>
        <volume>abs/1409</volume>
        <uri>https://arxiv.org/abs/1409.1556</uri>
      </element-citation>
    </ref>
    <ref id="ref-MV2015">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Mahendran</surname><given-names>Aravindh</given-names></name>
          <name><surname>Vedaldi</surname><given-names>Andrea</given-names></name>
        </person-group>
        <article-title>Understanding deep image representations by inverting them</article-title>
        <source>IEEE conference on computer vision and pattern recognition (CVPR)</source>
        <year iso-8601-date="2015">2015</year>
        <uri>https://arxiv.org/abs/1412.0035</uri>
        <pub-id pub-id-type="doi">10.1109/CVPR.2015.7299155</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-LW2016">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Li</surname><given-names>Chuan</given-names></name>
          <name><surname>Wand</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>Combining markov random fields and convolutional neural networks for image synthesis</article-title>
        <source>IEEE conference on computer vision and pattern recognition (CVPR)</source>
        <year iso-8601-date="2016">2016</year>
        <uri>https://arxiv.org/abs/1601.04589</uri>
        <pub-id pub-id-type="doi">10.1109/CVPR.2016.272</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-SID2017">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Semmo</surname><given-names>Amir</given-names></name>
          <name><surname>Isenberg</surname><given-names>Tobias</given-names></name>
          <name><surname>Döllner</surname><given-names>Jürgen</given-names></name>
        </person-group>
        <article-title>Neural style transfer: A paradigm shift for image-based artistic rendering?</article-title>
        <source>Proceedings of the symposium on non-photorealistic animation and rendering (NPAR)</source>
        <year iso-8601-date="2017">2017</year>
        <pub-id pub-id-type="doi">10.1145/3092919.3092920</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-KSH2012">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Krizhevsky</surname><given-names>Alex</given-names></name>
          <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
          <name><surname>Hinton</surname><given-names>Geoffrey E.</given-names></name>
        </person-group>
        <article-title>Imagenet classification with deep convolutional neural networks</article-title>
        <source>Advances in neural information processing systems 25 (NIPS)</source>
        <year iso-8601-date="2012">2012</year>
        <uri>https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-PGMU002B2019">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Paszke</surname><given-names>Adam</given-names></name>
          <name><surname>Gross</surname><given-names>Sam</given-names></name>
          <name><surname>Massa</surname><given-names>Francisco</given-names></name>
          <name><surname>Lerer</surname><given-names>Adam</given-names></name>
          <name><surname>Bradbury</surname><given-names>James</given-names></name>
          <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
          <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
          <name><surname>Lin</surname><given-names>Zeming</given-names></name>
          <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
          <name><surname>Antiga</surname><given-names>Luca</given-names></name>
          <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
          <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
          <name><surname>Yang</surname><given-names>Edward</given-names></name>
          <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
          <name><surname>Raison</surname><given-names>Martin</given-names></name>
          <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
          <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
          <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
          <name><surname>Fang</surname><given-names>Lu</given-names></name>
          <name><surname>Bai</surname><given-names>Junjie</given-names></name>
          <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
        </person-group>
        <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
        <source>Advances in neural information processing systems 32 (NIPS)</source>
        <year iso-8601-date="2019">2019</year>
        <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-GG2001">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Gooch</surname><given-names>Bruce</given-names></name>
          <name><surname>Gooch</surname><given-names>Amy</given-names></name>
        </person-group>
        <source>Non-photorealistic rendering</source>
        <publisher-name>A. K. Peters, Ltd.</publisher-name>
        <year iso-8601-date="2001">2001</year>
        <isbn>1568811330</isbn>
      </element-citation>
    </ref>
    <ref id="ref-JYFU002B2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Jing</surname><given-names>Yongcheng</given-names></name>
          <name><surname>Yang</surname><given-names>Yezhou</given-names></name>
          <name><surname>Feng</surname><given-names>Zunlei</given-names></name>
          <name><surname>Ye</surname><given-names>Jingwen</given-names></name>
          <name><surname>Yu</surname><given-names>Yizhou</given-names></name>
          <name><surname>Song</surname><given-names>Mingli</given-names></name>
        </person-group>
        <article-title>Neural style transfer: A review</article-title>
        <source>IEEE Transactions on Visualization and Computer Graphics</source>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.1109/TVCG.2019.2921336</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
