<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2741</article-id>
<article-id pub-id-type="doi">10.21105/joss.02741</article-id>
<title-group>
<article-title>ParaMonte: A high-performance serial/parallel Monte Carlo
simulation library for C, C++, Fortran</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9720-8937</contrib-id>
<string-name>Amir Shahmoradi</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Fatemeh Bagheri</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Physics, The University of Texas, Arlington,
TX</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Data Science Program, The University of Texas, Arlington,
TX</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-09-28">
<day>28</day>
<month>9</month>
<year>2020</year>
</pub-date>
<volume>6</volume>
<issue>61</issue>
<fpage>2741</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>C</kwd>
<kwd>C++</kwd>
<kwd>Monte Carlo</kwd>
<kwd>Markov Chain Monte Carlo</kwd>
<kwd>Uncertainty Quantification</kwd>
<kwd>Metropolis-Hastings</kwd>
<kwd>adaptive sampling</kwd>
<kwd>MCMC</kwd>
<kwd>DRAM</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>ParaMonte (which stands for Parallel Monte Carlo) is a serial and
  MPI/Coarray-parallelized library of Monte Carlo routines for sampling
  mathematical objective functions of arbitrary-dimensions, in
  particular, the posterior distributions of Bayesian models in data
  science, machine learning, and scientific inference. The ParaMonte
  library has been developed with the design goal of unifying the
  <bold>automation</bold>, <bold>accessibility</bold>,
  <bold>high-performance</bold>, <bold>scalability</bold>, and
  <bold>reproducibility</bold> of Monte Carlo simulations. The current
  implementation of the library includes <bold>ParaDRAM</bold>, a
  <bold>Para</bold>llel <bold>D</bold>elayed-<bold>R</bold>ejection
  <bold>A</bold>daptive <bold>M</bold>etropolis Markov Chain Monte Carlo
  sampler, accessible from a wide range of programming languages
  including C, C++, Fortran, with a unified Application Programming
  Interface and simulation environment across all supported programming
  languages. The ParaMonte library is MIT-licensed and is permanently
  located and maintained at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/cdslaborg/paramonte">https://github.com/cdslaborg/paramonte</ext-link>.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Monte Carlo simulation techniques
  (<xref alt="Metropolis &amp; Ulam, 1949" rid="ref-metropolis1949monte" ref-type="bibr">Metropolis
  &amp; Ulam, 1949</xref>), in particular, Markov Chain Monte Carlo
  (MCMC)
  (<xref alt="Metropolis et al., 1953" rid="ref-metropolis1953equation" ref-type="bibr">Metropolis
  et al., 1953</xref>) are among the most popular methods of quantifying
  uncertainty in scientific inference problems. Extensive work has been
  done over the past decades to develop Monte Carlo simulation
  programming environments that aim to partially or fully automate the
  problem of uncertainty quantification via Markov Chain Monte Carlo
  simulations. Example open-source libraries in C/C++/Fortran include
  <monospace>MCSim</monospace> in C
  (<xref alt="Bois, 2009" rid="ref-BoisU003A2009" ref-type="bibr">Bois,
  2009</xref>), <monospace>MCMCLib</monospace> and
  <monospace>QUESO</monospace>
  (<xref alt="Prudencio &amp; Schulz, 2012" rid="ref-AlexanderU003A2012" ref-type="bibr">Prudencio
  &amp; Schulz, 2012</xref>) libraries in C++, and
  <monospace>mcmcf90</monospace> in Fortran
  (<xref alt="Haario et al., 2006" rid="ref-HaarioU003A2006" ref-type="bibr">Haario
  et al., 2006</xref>). These packages, however, mostly serve the users
  of one particular programming language environment. Some can perform
  only serial simulations while others are inherently parallelized.
  Furthermore, the majority of the existing packages have significant
  dependencies on other external libraries. Such dependencies can
  potentially make the build process of the packages extremely complex
  and arduous due to software version incompatibilities, a phenomenon
  that has become known as the <italic>dependency-hell</italic> among
  software developers.</p>
  <p>The ParaMonte library presented in this work aims to address the
  aforementioned problems by providing a standalone high-performance
  serial/parallel Monte Carlo simulation environment with the following
  principal design goals:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Full automation</bold> of the library’s build process and
      all Monte Carlo simulations to ensure the highest level of
      user-friendliness of the library and minimal time investment
      requirements for building the library as well as running and
      post-processing the Monte Carlo simulations
      </p>
    </list-item>
    <list-item>
      <p><bold>Interoperability</bold> of the core of the library with
      as many programming languages as currently possible, including C,
      C++, Fortran, as well as MATLAB and Python via the
      <monospace>ParaMonte::MATLAB</monospace>
      (<xref alt="Kumbhare &amp; Shahmoradi, 2020a" rid="ref-2020arXiv201004190S" ref-type="bibr">Kumbhare
      &amp; Shahmoradi, 2020a</xref>) and
      <monospace>ParaMonte::Python</monospace>
      (<xref alt="Shahmoradi et al., 2020" rid="ref-2020arXiv201000724S" ref-type="bibr">Shahmoradi
      et al., 2020</xref>) libraries
      </p>
    </list-item>
    <list-item>
      <p><bold>High-Performance</bold>, meticulously-low-level
      implementation of the library that guarantees the fastest-possible
      Monte Carlo simulations <bold>without</bold> compromising the
      reproducibility of the simulations or the extensive external
      reporting of the simulation progress and results
      </p>
    </list-item>
    <list-item>
      <p><bold>Parallelizability</bold> of all simulations via both MPI
      and PGAS/Coarray communication paradigms while <bold>requiring
      zero-parallel-coding efforts from the user</bold>
      </p>
    </list-item>
    <list-item>
      <p><bold>Zero external-library dependencies</bold> to ensure
      hassle-free library builds and Monte Carlo simulation runs
      </p>
    </list-item>
    <list-item>
      <p><bold>Fully-deterministic reproducibility</bold> and
      <bold>automatically-enabled restart functionality</bold> for all
      ParaMonte simulations (up to 16 digits of decimal precision if
      requested by the user)
      </p>
    </list-item>
    <list-item>
      <p><bold>Comprehensive-reporting and post-processing</bold> of
      each simulation and its results as well as their efficient compact
      storage in external files to ensure the reproducibility and
      comprehensibility of the simulation results at any time in the
      future</p>
    </list-item>
  </list>
</sec>
<sec id="the-origins-of-paramonte">
  <title>The origins of ParaMonte</title>
  <p>The ParaMonte library grew out of the need for a free user-friendly
  high-performance parallel software for stochastic optimization,
  sampling, and integration problems in scientific inference and Data
  Science applications. The project started in 2012 to aid the research
  goals of the primary author of the package in the field of High Energy
  Astrophysics and Bioinformatics
  (<xref alt="Osborne et al., 2020" rid="ref-osborne2020multilevelapj" ref-type="bibr">Osborne
  et al., 2020</xref>;
  <xref alt="Shahmoradi, 2013a" rid="ref-ShahmoradiAU003A2013" ref-type="bibr">Shahmoradi,
  2013a</xref>,
  <xref alt="2013b" rid="ref-ShahmoradiU003A2013" ref-type="bibr">2013b</xref>;
  <xref alt="Shahmoradi &amp; Nemiroff, 2014" rid="ref-ShahmoradiU003A2014" ref-type="bibr">Shahmoradi
  &amp; Nemiroff, 2014</xref>;
  <xref alt="Shahmoradi &amp; Nemiroff, 2019" rid="ref-ShahmoradiU003A2019" ref-type="bibr">Shahmoradi
  &amp; Nemiroff, 2019</xref>;
  <xref alt="Shahmoradi &amp; Nemiroff, 2015" rid="ref-ShahmoradiU003A2015" ref-type="bibr">Shahmoradi
  &amp; Nemiroff, 2015</xref>). It remained in private usage over the
  years until summer 2020, when version 1.0 of the library was released
  as an open-source project for public usage and contributions.</p>
  <p>Many contemporary research problems are computationally demanding
  and require the analysis of vast amounts of high-dimensional data by a
  community of non-computer-science domain researchers who might be
  neither familiar with details of software dependencies, build
  processes, and complexities nor even with the inner-workings of the
  various stochastic optimization and sampling techniques. As such,
  since its inception, the ParaMonte library has been built upon the two
  pillars of user-friendliness and high-performance.</p>
</sec>
<sec id="the-build-process">
  <title>The build process</title>
  <p>The ParaMonte library is permanently located on GitHub and is
  available to view at:
  <ext-link ext-link-type="uri" xlink:href="https://github.com/cdslaborg/paramonte">https://github.com/cdslaborg/paramonte</ext-link>.
  The build process of the library is fully automated. Extensive
  detailed instructions are also available on the
  <ext-link ext-link-type="uri" xlink:href="https://www.cdslab.org/paramonte/">documentation
  website of the library</ext-link>.</p>
  <p>For the convenience of users, each versioned release of the
  library’s source code also includes prebuilt, ready-to-use, copies of
  the library for <monospace>x64</monospace> architecture on Windows,
  Linux, macOS, in all supported programming languages, including C,
  C++, Fortran. These prebuilt libraries automatically ship with the
  language-specific example codes and build scripts that fully automate
  the process of building and running the examples. Users can either
  adapt the example’s source files and build scripts to their own needs
  or in more sophisticated scenarios, they can simply link their
  applications against the supplied prebuilt library.</p>
  <p>Where the prebuilt libraries cannot be used, users can readily call
  the Unix-Bash and Windows-Batch build-scripts that are provided with
  the source code of the library to fully automate the build process of
  the library. These build scripts have been developed to automate the
  installation of any missing components that may be required for the
  successful build of the library, including the
  <monospace>cmake</monospace> build software, the GNU C/C++/Fortran
  compilers, as well as the MPI/Coarray libraries. All of these tasks
  are performed with the explicit permission granted by the user. The
  ParaMonte build scripts are heavily inspired by the impressive
  <monospace>OpenCoarrays</monospace> open-source software
  (<xref alt="Fanfarillo et al., 2014" rid="ref-FanfarilloU003A2014" ref-type="bibr">Fanfarillo
  et al., 2014</xref>) developed and maintained by the
  <ext-link ext-link-type="uri" xlink:href="http://www.sourceryinstitute.org/">Sourcery
  Institute</ext-link>.</p>
</sec>
<sec id="the-paramonte-samplers">
  <title>The ParaMonte samplers</title>
  <p>The current implementation of the ParaMonte library includes the
  <bold>Para</bold>llel <bold>D</bold>elayed-<bold>R</bold>ejection
  <bold>A</bold>daptive <bold>M</bold>etropolis Markov Chain Monte Carlo
  (<bold><monospace>ParaDRAM</monospace></bold>) sampler
  (<xref alt="Kumbhare &amp; Shahmoradi, 2020b" rid="ref-KumbhareU003A2020" ref-type="bibr">Kumbhare
  &amp; Shahmoradi, 2020b</xref>;
  <xref alt="Shahmoradi &amp; Bagheri, 2020a" rid="ref-ShahmoradiADSU003A2020" ref-type="bibr">Shahmoradi
  &amp; Bagheri, 2020a</xref>,
  <xref alt="2020b" rid="ref-ShahmoradiASCLU003A2020" ref-type="bibr">2020b</xref>,
  <xref alt="2020c" rid="ref-ShahmoradiGSU003A2020" ref-type="bibr">2020c</xref>),
  and several other samplers whose development is in progress as of
  writing this manuscript. The ParaDRAM algorithm is a variant of the
  DRAM algorithm of
  (<xref alt="Haario et al., 2006" rid="ref-HaarioU003A2006" ref-type="bibr">Haario
  et al., 2006</xref>) and can be used in either serial or parallel
  mode.</p>
  <p>In brief, the ParaDRAM sampler continuously adapts the shape and
  scale of the proposal distribution throughout the simulation to
  increase the efficiency of the sampler. This is in contrast to
  traditional MCMC samplers where the proposal distribution remains
  fixed throughout the simulation. The ParaDRAM sampler provides a
  highly customizable MCMC simulation environment whose complete
  description goes beyond the scope and limits of this manuscript. All
  of these simulation specifications are, however, expensively explained
  and discussed on
  <ext-link ext-link-type="uri" xlink:href="https://www.cdslab.org/paramonte/">the
  documentation website of the ParaMonte library</ext-link>. The
  description of all of these specifications are also automatically
  provided in the output <monospace>*_report.txt</monospace> files of
  every simulation performed by the ParaMonte samplers.</p>
  <p>Several additional more advanced samplers are also currently under
  development and are scheduled for release in 2021.</p>
  <sec id="parallelism">
    <title>Parallelism</title>
    <p>Two modes of parallelism are currently implemented for all
    ParaMonte samplers,</p>
    <list list-type="bullet">
      <list-item>
        <p>The <bold>Perfect Parallelism</bold> (multi-Chain): In this
        mode, independent instances of a particular ParaMonte sampler of
        choice run concurrently. Once all simulations are complete, the
        sampler compares the output samples from all processors with
        each other to calculate various statistics and to ensure that no
        evidence for a lack of convergence to the target density exists
        in any of the output chains.</p>
      </list-item>
      <list-item>
        <p>The <bold>Fork-Join Parallelism</bold> (single-Chain): In
        this mode, a single processor is responsible for collecting and
        dispatching information, generated by all processors, to
        construct the full sample from the target density function.</p>
      </list-item>
    </list>
    <p>For each parallel simulation in the Fork-Join mode, the ParaMonte
    samplers automatically compute the speedup gained compared to the
    serial mode. The speedup for a wide range of the number of
    processors is also automatically computed and reported in the output
    <monospace>*_report.txt</monospace> files that are automatically
    generated for all simulations. The processor contributions to the
    construction of the final output sample are also reported along with
    all visited states in the output <monospace>*_chain.*</monospace>
    files. Such information is particularly useful for finding the
    optimal number of processors for a given problem at hand, by first
    running a short simulation to predict the optimal number of
    processors from the sampler’s output information, followed by the
    full production run using the optimal number of processors. For a
    comprehensive description and algorithmic details see
    (<xref alt="Shahmoradi &amp; Bagheri, 2020a" rid="ref-ShahmoradiADSU003A2020" ref-type="bibr">Shahmoradi
    &amp; Bagheri, 2020a</xref>,
    <xref alt="2020c" rid="ref-ShahmoradiGSU003A2020" ref-type="bibr">2020c</xref>).</p>
    <fig>
      <caption><p>An illustration of the contributions of 512 Intel Xeon
      Phi 7250 processors to a ParaMonte-ParaDRAM simulation
      parallelized via the Fork-Join paradigm. The predicted best-fit
      Geometric distribution from the post-processing phase of the
      ParaDRAM simulation is shown by the black line. The data used in
      this figure is automatically generated for each parallel
      simulation performed via any of the ParaMonte samplers.
      <styled-content id="figU003AprocContribution512"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="procContribution512.png" xlink:title="" />
    </fig>
    <p>As we argue in
    (<xref alt="Shahmoradi &amp; Bagheri, 2020a" rid="ref-ShahmoradiADSU003A2020" ref-type="bibr">Shahmoradi
    &amp; Bagheri, 2020a</xref>,
    <xref alt="2020c" rid="ref-ShahmoradiGSU003A2020" ref-type="bibr">2020c</xref>)
    for the particular case of MCMC simulations, the distribution of
    processor contributions to the construction of a final sample from
    an objective function in the Fork-Join parallelism paradigm follows
    a Geometric curve.
    <xref alt="Figure 1" rid="figU003AprocContribution512">Figure 1</xref>
    depicts the processor contributions to an example ParaDRAM
    simulation of a variant of Himmelblau’s function. Superimposed on
    the distribution of processor contributions is the Geometric fit to
    the data.
    <xref alt="Figure 2" rid="figU003APredictedSpeedupActualSpeedup">Figure 2</xref>
    illustrates an example strong-scaling behavior of the sampler and
    the predicted speedup by the sampler for a range of processor
    counts.</p>
    <p>The exact strong scaling behavior of Fork-Join parallel
    simulations is highly dependent on the sampling efficiency of the
    problem. For a fixed number of processors, the parallel efficiency
    of the samplers monotonically increases toward the maximum
    theoretical speedup with decreasing sampling efficiency. This
    theoretical maximum parallel speedup and the corresponding predicted
    optimal number of processors are also computed for each parallel
    simulation and reported in the final output
    <monospace>*_report.txt</monospace> files.</p>
    <fig>
      <caption><p>A comparison of the actual strong scaling behavior of
      an example ParaMonte-ParaDRAM simulation from 1 to 1088 processors
      with the strong-scaling behavior predicted during the
      post-processing phases of the corresponding individual ParaDRAM
      simulations. The data used in this figure is automatically
      generated for each parallel simulation performed via any of the
      ParaMonte samplers. The black dashed line represents the perfect
      parallelism.<styled-content id="figU003APredictedSpeedupActualSpeedup"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="PredictedSpeedupActualSpeedup.png" xlink:title="" />
    </fig>
  </sec>
  <sec id="efficient-compact-storage-of-the-output-samples">
    <title>Efficient compact storage of the output samples</title>
    <p>Efficient continuous external storage of the output of Monte
    Carlo simulations is essential for both post-processing of the
    results and the restart functionality of the simulations, should any
    interruptions happen at runtime. However, as the number of
    dimensions or the complexity of the target density increases, such
    external storage of the output can easily become a challenge and a
    bottleneck in the speed of an otherwise high-performance sampler.
    Given the currently-available computational technologies,
    input/ouput (IO) to external hard-drives can be 2-3 orders of
    magnitude slower than the Random Access Memory (RAM) storage.</p>
    <p>To alleviate the effects of such external-IO speed bottlenecks,
    the ParaMonte samplers have been devised to carefully store the
    resulting samples in a small <italic>compact</italic>, yet ASCII
    human-readable format in external output files. This
    <bold>compact</bold> storage, as opposed to the <bold>verbose</bold>
    (or in the case of the ParaDRAM sampler, the
    <bold>Markov-chain</bold>) storage format, leads to significant
    speedup of the simulations while requiring 4-100 times less external
    memory to store the output samples in the external output files. The
    exact amount of reduction in the external memory usage depends on
    the runtime sampling efficiency of the samplers. Additionally, the
    format of output files can be set by the user to
    <monospace>binary</monospace>, further reducing the memory
    foot-print of the simulations while increasing the simulation speed.
    The implementation details of this compact storage format are
    extensively discussed in
    (<xref alt="Shahmoradi &amp; Bagheri, 2020a" rid="ref-ShahmoradiADSU003A2020" ref-type="bibr">Shahmoradi
    &amp; Bagheri, 2020a</xref>,
    <xref alt="2020c" rid="ref-ShahmoradiGSU003A2020" ref-type="bibr">2020c</xref>).</p>
  </sec>
  <sec id="restart-functionality">
    <title>Restart functionality</title>
    <p>Each ParaMonte sampler is automatically capable of restarting an
    existing interrupted simulation, whether in serial or parallel. All
    that is required is to rerun the interrupted simulation with the
    same prefix for the simulation output file names. The ParaMonte
    samplers automatically detect the presence of an incomplete
    simulation in the output files and restart the simulation from where
    it was left off.</p>
    <p>Furthermore, if the user sets the seed of the random number
    generator of the sampler before running the simulation, <italic>the
    ParaMonte samplers are capable of regenerating the same output
    sample that would have been produced if the simulation had not been
    interrupted in the first place</italic>. Such
    <bold>fully-deterministic reproducibility into-the-future</bold> is
    guaranteed with 16 digits of decimal precision for the results of
    any ParaMonte simulation, whether serial or in parallel. To our
    knowledge, this is a unique feature of the ParaMonte library that
    does not appear to exist in any of the contemporary libraries for
    Monte Carlo simulations.</p>
  </sec>
</sec>
<sec id="the-paramonte-application-programming-interface">
  <title>The ParaMonte Application Programming Interface</title>
  <p>Special care has been taken to develop highly-similar (if not the
  same) Application Programming Interface (API) to the ParaMonte library
  samplers across all supported programming languages. Ensuring
  backward-compatibility has been also one of the high priorities in the
  development of the library and will remain a priority for any future
  development and expansion of the project.</p>
  <p>The project currently uses semantic versioning to readily inform
  the users about the backward-compatibility of each release. This
  semantic versioning, however, only applies to the ParaMonte samplers’
  API. Conversely, the API to the foundational procedures of the
  ParaMonte kernel tend to be dynamic and subject to changes without
  incrementing the library’s major version. This is line with the
  primary purpose of the library as a Monte Carlo optimization and
  sampling toolbox.</p>
  <p>Nevertheless, the kernel documentation is well developed and
  accessible to all, including the end-users, should they decide to use
  any of the basic routines in the library beyond the samplers. The
  kernel API documentation is permanently available on GitHub at its
  dedicated repository: and is accessible from
  <ext-link ext-link-type="uri" xlink:href="https://www.cdslab.org/paramonte/">the
  ParaMonte’s main documentation portal</ext-link>.</p>
</sec>
<sec id="tests-and-code-coverage">
  <title>Tests and code coverage</title>
  <p>The entire kernel routines of the ParaMonte library are currently
  tested with over 866 individual tests. Out of these, 510 test units
  currently cover and verify the functionalities of the basic
  building-block routines of the library and 156 tests verify the
  semantics and different aspects of the ParaMonte samplers built on top
  of the basic routines.</p>
  <p>The code coverage report analyses of the ParaMonte kernel library
  can be generated for a wide range of input build configurations. For
  example, corresponding to each programming language (C, C++, Fortran,
  MATLAB, Python, R, …), a separate code coverage report for the kernel
  routines can be generated. Similarly, different library builds (debug,
  testing, release), library types (static, dynamic), and memory
  allocation schemes (stack or heap) can lead to different code coverage
  reports. This is primarily because the kernel routines heavily utilize
  compiler preprocessor directives to accommodate the support for
  multiple programming languages, Operating Systems and architectures,
  as well as multiple parallelism paradigms.</p>
  <p>Among all build configurations, however, the single most important
  specification that yields the largest differences in code coverage
  reports is the parallelism paradigm (serial, MPI, and Coarray
  parallelism). The code coverage reports for the serial, MPI, and
  Coarray modes are remarkably different since each parallelism paradigm
  activates a different set of codes in the ParaMonte kernel
  routines.</p>
  <p>Thus, with each tagged release of the library, the code coverage
  reports for the three parallelism paradigms are also generated and
  stored in a separate repository dedicated to the code coverage
  analyses of the ParaMonte library. This repository is permanently
  located on GitHub at: and the reports are automatically and
  conveniently accessible from
  <ext-link ext-link-type="uri" xlink:href="https://www.cdslab.org/paramonte/notes/codecov/">the
  ParaMonte documentation website</ext-link>.</p>
  <p>The current set of 866 unit tests in version 1.5.0 release of the
  ParaMonte library collectively cover 47565 out of 48384 lines of code
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\sim 98\%]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>98</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  line coverage) and 4467 out of 4476 procedures
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\sim 100\%]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>100</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  function coverage) in the library’s kernel for the three serial, MPI,
  and Coarray parallelism paradigms.</p>
</sec>
<sec id="documentation-and-repository">
  <title>Documentation and Repository</title>
  <p>Extensive documentation and examples in C, C++, Fortran (as well as
  other programming languages) are available on the documentation
  website of the library at:
  <ext-link ext-link-type="uri" xlink:href="https://www.cdslab.org/paramonte/">https://www.cdslab.org/paramonte/</ext-link>.
  The ParaMonte library is MIT-licensed and is permanently located and
  maintained at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/cdslaborg/paramonte">https://github.com/cdslaborg/paramonte</ext-link>.</p>
  <p>A high number of comments in a codebase are frequently indicative
  of an organized well-documented project and considered a sign of a
  helpful and disciplined development team. As of January 2021, comments
  approximately make up <inline-formula><alternatives>
  <tex-math><![CDATA[31\%]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>31</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  of the entire ParaMonte codebase. Currently, the Open Hub indexer
  service ranks the ParaMonte project’s level of documentation as
  “impressive” and among the top <inline-formula><alternatives>
  <tex-math><![CDATA[10\%]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>10</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  of all projects in the same category on Open Hub.</p>
</sec>
<sec id="funding-and-acknowledgements">
  <title>Funding and Acknowledgements</title>
  <p>The development cost of ParaMonte is estimated over 1 million
  dollars by
  <ext-link ext-link-type="uri" xlink:href="https://www.openhub.net/p/paramonte/estimated_cost">Open
  Hub</ext-link>. Considering the presence of multiple high-level
  dynamic languages such as MATLAB, Python, and R, which together
  comprise approximately <inline-formula><alternatives>
  <tex-math><![CDATA[40\%]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>40</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  of the repository, an estimate of <inline-formula><alternatives>
  <tex-math><![CDATA[500-600]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>500</mml:mn><mml:mo>−</mml:mo><mml:mn>600</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  thousand dollars seems more realist for the development costs of the
  library’s kernel routines. The current state of the library has
  resulted from multiple years of continuous development predominantly
  by the core developers of the library as graduate student, postdoc,
  and faculty in their free time. The project was also partially
  supported in its final stages of development by the Peter O’Donnell,
  Jr. Postdoctoral Fellowship awarded by the Oden Institute for
  Computational Engineering and Sciences at The University of Texas at
  Austin to the primary developer of the library, Amir Shahmoradi. We
  thank the Texas Advanced Computing Center for providing the
  supercomputer time for testing and development of this library.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-osborne2020multilevelapj">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Osborne</surname><given-names>Joshua A.</given-names></name>
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
          <name><surname>Nemiroff</surname><given-names>Robert J.</given-names></name>
        </person-group>
        <article-title>A Multilevel Empirical Bayesian Approach to Estimating the Unknown Redshifts of 1366 BATSE Catalog Long-duration Gamma-Ray Bursts</article-title>
        <source></source>
        <year iso-8601-date="2020-11">2020</year><month>11</month>
        <volume>903</volume>
        <issue>1</issue>
        <uri>https://arxiv.org/abs/2006.01157</uri>
        <pub-id pub-id-type="doi">10.3847/1538-4357/abb9b7</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-2020arXiv201004190S">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kumbhare</surname><given-names>Shashank</given-names></name>
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
        </person-group>
        <article-title>MatDRAM: A pure-MATLAB Delayed-Rejection Adaptive Metropolis-Hastings Markov Chain Monte Carlo Sampler</article-title>
        <source>arXiv e-prints</source>
        <year iso-8601-date="2020-09">2020</year><month>09</month>
        <uri>https://arxiv.org/abs/2010.04190</uri>
      </element-citation>
    </ref>
    <ref id="ref-2020arXiv201000724S">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
          <name><surname>Bagheri</surname><given-names>Fatemeh</given-names></name>
          <name><surname>Osborne</surname><given-names>Joshua Alexander</given-names></name>
        </person-group>
        <article-title>Fast fully-reproducible serial/parallel Monte Carlo and MCMC simulations and visualizations via ParaMonte::Python library</article-title>
        <source>arXiv e-prints</source>
        <year iso-8601-date="2020-10">2020</year><month>10</month>
        <uri>https://arxiv.org/abs/2010.00724</uri>
      </element-citation>
    </ref>
    <ref id="ref-metropolis1953equation">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Metropolis</surname><given-names>Nicholas</given-names></name>
          <name><surname>Rosenbluth</surname><given-names>Arianna W</given-names></name>
          <name><surname>Rosenbluth</surname><given-names>Marshall N</given-names></name>
          <name><surname>Teller</surname><given-names>Augusta H</given-names></name>
          <name><surname>Teller</surname><given-names>Edward</given-names></name>
        </person-group>
        <article-title>Equation of state calculations by fast computing machines</article-title>
        <source>The journal of chemical physics</source>
        <publisher-name>AIP Publishing</publisher-name>
        <year iso-8601-date="1953">1953</year>
        <volume>21</volume>
        <issue>6</issue>
      </element-citation>
    </ref>
    <ref id="ref-metropolis1949monte">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Metropolis</surname><given-names>Nicholas</given-names></name>
          <name><surname>Ulam</surname><given-names>Stanislaw</given-names></name>
        </person-group>
        <article-title>The Monte Carlo method</article-title>
        <source>Journal of the American statistical association</source>
        <publisher-name>Taylor &amp; Francis Group</publisher-name>
        <year iso-8601-date="1949">1949</year>
        <volume>44</volume>
        <issue>247</issue>
      </element-citation>
    </ref>
    <ref id="ref-ShahmoradiU003A2013">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
        </person-group>
        <article-title>A multivariate fit luminosity function and world model for long gamma-ray bursts</article-title>
        <source>The Astrophysical Journal</source>
        <publisher-name>IOP Publishing</publisher-name>
        <year iso-8601-date="2013">2013</year>
        <volume>766</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1088/0004-637x/766/2/111</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-ShahmoradiAU003A2013">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
        </person-group>
        <article-title>Gamma-Ray bursts: Energetics and Prompt Correlations</article-title>
        <source>arXiv e-prints</source>
        <year iso-8601-date="2013-08">2013</year><month>08</month>
        <uri>https://arxiv.org/abs/1308.1097</uri>
      </element-citation>
    </ref>
    <ref id="ref-ShahmoradiU003A2014">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
          <name><surname>Nemiroff</surname><given-names>RJ</given-names></name>
        </person-group>
        <article-title>Classification and energetics of cosmological gamma-ray bursts</article-title>
        <source>American astronomical society meeting abstracts# 223</source>
        <year iso-8601-date="2014">2014</year>
        <volume>223</volume>
      </element-citation>
    </ref>
    <ref id="ref-ShahmoradiU003A2015">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
          <name><surname>Nemiroff</surname><given-names>Robert J</given-names></name>
        </person-group>
        <article-title>Short versus long gamma-ray bursts: A comprehensive study of energetics and prompt gamma-ray correlations</article-title>
        <source>Monthly Notices of the Royal Astronomical Society</source>
        <publisher-name>Oxford University Press</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>451</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1093/mnras/stv714</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-ShahmoradiU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
          <name><surname>Nemiroff</surname><given-names>Robert J.</given-names></name>
        </person-group>
        <article-title>A Catalog of Redshift Estimates for 1366 BATSE Long-Duration Gamma-Ray Bursts: Evidence for Strong Selection Effects on the Phenomenological Prompt Gamma-Ray Correlations</article-title>
        <source>arXiv e-prints</source>
        <year iso-8601-date="2019-03">2019</year><month>03</month>
        <uri>https://arxiv.org/abs/1903.06989</uri>
      </element-citation>
    </ref>
    <ref id="ref-ShahmoradiGSU003A2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
          <name><surname>Bagheri</surname><given-names>Fatemeh</given-names></name>
        </person-group>
        <article-title>ParaDRAM: A Cross-Language Toolbox for Parallel High-Performance Delayed-Rejection Adaptive Metropolis Markov Chain Monte Carlo Simulations</article-title>
        <source>arXiv preprint arXiv:2008.09589</source>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-ShahmoradiADSU003A2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
          <name><surname>Bagheri</surname><given-names>Fatemeh</given-names></name>
        </person-group>
        <article-title>ParaDRAM: A Cross-Language Toolbox for Parallel High-Performance Delayed-Rejection Adaptive Metropolis Markov Chain Monte Carlo Simulations</article-title>
        <source>arXiv e-prints</source>
        <year iso-8601-date="2020-08">2020</year><month>08</month>
        <uri>https://arxiv.org/abs/2008.09589</uri>
      </element-citation>
    </ref>
    <ref id="ref-KumbhareU003A2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kumbhare</surname><given-names>Shashank</given-names></name>
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
        </person-group>
        <article-title>Parallel adapative Monte Carlo optimization, sampling, and integration in C/C++, Fortran, MATLAB, and Python</article-title>
        <source>Bulletin of the American Physical Society</source>
        <publisher-name>APS</publisher-name>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-ShahmoradiASCLU003A2020">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Shahmoradi</surname><given-names>Amir</given-names></name>
          <name><surname>Bagheri</surname><given-names>Fatemeh</given-names></name>
        </person-group>
        <article-title>ParaMonte: Parallel Monte Carlo library</article-title>
        <year iso-8601-date="2020-08">2020</year><month>08</month>
      </element-citation>
    </ref>
    <ref id="ref-FanfarilloU003A2014">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Fanfarillo</surname><given-names>Alessandro</given-names></name>
          <name><surname>Burnus</surname><given-names>Tobias</given-names></name>
          <name><surname>Cardellini</surname><given-names>Valeria</given-names></name>
          <name><surname>Filippone</surname><given-names>Salvatore</given-names></name>
          <name><surname>Nagle</surname><given-names>Dan</given-names></name>
          <name><surname>Rouson</surname><given-names>Damian</given-names></name>
        </person-group>
        <article-title>OpenCoarrays: Open-source transport layers supporting coarray Fortran compilers</article-title>
        <source>Proceedings of the 8th international conference on partitioned global address space programming models</source>
        <year iso-8601-date="2014">2014</year>
      </element-citation>
    </ref>
    <ref id="ref-HaarioU003A2006">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Haario</surname><given-names>Heikki</given-names></name>
          <name><surname>Laine</surname><given-names>Marko</given-names></name>
          <name><surname>Mira</surname><given-names>Antonietta</given-names></name>
          <name><surname>Saksman</surname><given-names>Eero</given-names></name>
        </person-group>
        <article-title>DRAM: Efficient adaptive MCMC</article-title>
        <source>Statistics and computing</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2006">2006</year>
        <volume>16</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1007/s11222-006-9438-0</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BoisU003A2009">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bois</surname><given-names>Frédéric Y</given-names></name>
        </person-group>
        <article-title>GNU MCSim: Bayesian statistical inference for SBML-coded systems biology models</article-title>
        <source>Bioinformatics</source>
        <publisher-name>Oxford University Press</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>25</volume>
        <issue>11</issue>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp162</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-AlexanderU003A2012">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Prudencio</surname><given-names>ErnestoE.</given-names></name>
          <name><surname>Schulz</surname><given-names>KarlW.</given-names></name>
        </person-group>
        <article-title>The parallel C++ statistical library QUESO: Quantification of uncertainty for estimation, simulation and optimization</article-title>
        <source>Euro-par 2011: Parallel processing workshops</source>
        <person-group person-group-type="editor">
          <name><surname>Alexander</surname><given-names>Michael</given-names></name>
          <name><surname>D’Ambra</surname><given-names>Pasqua</given-names></name>
          <name><surname>Belloum</surname><given-names>Adam</given-names></name>
          <name><surname>Bosilca</surname><given-names>George</given-names></name>
          <name><surname>Cannataro</surname><given-names>Mario</given-names></name>
          <name><surname>Danelutto</surname><given-names>Marco</given-names></name>
          <name><surname>Martino</surname><given-names>Beniamino</given-names></name>
          <name><surname>Gerndt</surname><given-names>Michael</given-names></name>
          <name><surname>Jeannot</surname><given-names>Emmanuel</given-names></name>
          <name><surname>Namyst</surname><given-names>Raymond</given-names></name>
          <name><surname>Roman</surname><given-names>Jean</given-names></name>
          <name><surname>Scott</surname><given-names>StephenL.</given-names></name>
          <name><surname>Traff</surname><given-names>JesperLarsson</given-names></name>
          <name><surname>Vallée</surname><given-names>Geoffroy</given-names></name>
          <name><surname>Weidendorfer</surname><given-names>Josef</given-names></name>
        </person-group>
        <publisher-name>Springer Berlin Heidelberg</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <volume>7155</volume>
        <isbn>978-3-642-29736-6</isbn>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
