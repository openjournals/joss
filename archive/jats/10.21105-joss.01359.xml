<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1359</article-id>
<article-id pub-id-type="doi">10.21105/joss.01359</article-id>
<title-group>
<article-title>VBLinLogit: Variational Bayesian linear and logistic
regression</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7846-0408</contrib-id>
<string-name>Jan Drugowitsch</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Neurobiology, Harvard Medical School, Boston,
MA 02115, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-03-10">
<day>10</day>
<month>3</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>38</issue>
<fpage>1359</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>MATLAB</kwd>
<kwd>Octave</kwd>
<kwd>linear regression</kwd>
<kwd>logistic regression</kwd>
<kwd>Variational Bayes</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Linear and logistic regression are essential workhorses of
  statistical analysis, whose Bayesian treatment has received much
  recent attention
  (<xref alt="Bishop, 2006" rid="ref-BishopU003A2006" ref-type="bibr">Bishop,
  2006</xref>;
  <xref alt="Gelman et al., 2013" rid="ref-GelmanU003A2013" ref-type="bibr">Gelman
  et al., 2013</xref>;
  <xref alt="Hastie et al., 2011" rid="ref-HastieU003A2011" ref-type="bibr">Hastie
  et al., 2011</xref>;
  <xref alt="Murphy, 2012" rid="ref-MurphyU003A2012" ref-type="bibr">Murphy,
  2012</xref>). Using Bayesian statistics for linear and logistic
  regression allows specifying prior beliefs over certain model
  parameters, which makes it particularly useful for small and/or
  high-dimensional datasets. Bayesian regression furthermore provides an
  estimate of the uncertainty about estimated regression coefficients,
  as well as uncertainty about predictions arising from the regression.
  Both are again particularly important for small and/or
  high-dimensional datasets, as the use of such data might result in
  highly uncertain predictions, and allow the user to be explicit about
  this uncertainty.</p>
  <p><monospace>VBLinLogit</monospace> is a MATLAB/Octave library that
  provides a variational Bayesian implementation of Bayesian models for
  both linear and logistic regression. It uses variational Bayesian
  inference
  (<xref alt="Beal, 2003" rid="ref-BealU003A2003" ref-type="bibr">Beal,
  2003</xref>;
  <xref alt="Bishop, 2006" rid="ref-BishopU003A2006" ref-type="bibr">Bishop,
  2006</xref>;
  <xref alt="Murphy, 2012" rid="ref-MurphyU003A2012" ref-type="bibr">Murphy,
  2012</xref>) as a method for approximating Bayesian computations, as
  these computations would otherwise be intractable for the used
  regression models. It is significantly faster than Markov Chain Monte
  Carlo (MCMC) methods
  (<xref alt="Gilks et al., 1995" rid="ref-GilksU003A1995" ref-type="bibr">Gilks
  et al., 1995</xref>), another form of approximate Bayesian inference,
  which makes it applicable to high-dimensional problems for which
  standard MCMC might be too slow.</p>
  <p>A specific regression variant implemented by this library is
  automatic relevance determination (ARD), which uses a model that
  automatically determines which data dimensions are relevant for the
  regression, discarding the others
  (<xref alt="Wipf &amp; Nagarajan, 2008" rid="ref-WipfU003A2007" ref-type="bibr">Wipf
  &amp; Nagarajan, 2008</xref>). It does so without a separate
  “validation set”, as would be required by alternative methods, like
  the Lasso
  (<xref alt="Tibshirani, 1996" rid="ref-TibshiraniU003A1996" ref-type="bibr">Tibshirani,
  1996</xref>). Therefore, it can be used when the small size of the
  dataset makes the use of such a separate “validation set”
  prohibitive.</p>
  <p>The scripts encompassing the library were written to be
  light-weight and thus do not depend on external libraries. They
  include variants with and without ARD. Their use is deliberately kept
  simple. The core arguments to the regression scripts are a matrix of
  predictors, as well as a vector of response variable. Additional
  parameters specifying the prior and hyper-prior parameters are usually
  optional. Details about the specifics of the used models, the
  variational Bayes derivation, and detailed use of the scripts included
  in the library can be found in Drugowitsch
  (<xref alt="2013" rid="ref-DrugowitschU003A2013" ref-type="bibr">2013</xref>).</p>
</sec>
<sec id="additional-details-novelty-and-relation-to-other-approaches">
  <title>Additional details, novelty, and relation to other
  approaches</title>
  <p>Use of the library is particularly beneficial if the data is
  sparse. Sparsity can occur either if few training examples are
  available, or if the dimensionality of the input (i.e., the number of
  dependent variables) is large. Data sparsity becomes particularly
  challenging if the input dimensionality exceeds the number of training
  examples. In this case, the regression is underdetermined: multiple
  solutions exist that fit the training set equally well. However, only
  some of them yield good predictions on a separate test set.</p>
  <p>A common approach to handle underdetermination is to make
  additional assumptions about potential solutions. Specifically, it is
  commonly assumed that the regression weights (i.e., the regression
  coefficients) that form the solution and describe how the output
  (i.e., the independent variable) varies with the inputs, take small
  values. This is known as <italic>regularization</italic>.
  Regularization isn’t only beneficial if the regression is
  underdetermined, but also if the data is noisy. The different methods
  discussed here differ in how exactly they introduce regularization by
  making different a-priori assumptions about the regression
  weights.</p>
  <p>The Bayesian methods provided in this library implement two
  different sets of assumptions. They either assume that all regression
  coefficients are equally small and tunes how small they are overall
  (that’s the variant without ARD), or they adjust “smallness” of each
  regression coefficient individually (that’s the variant with ARD). The
  latter is particularly beneficial if the data includes some spurious
  input dimensions that don’t determine the outputs. In this case, the
  ARD variant might be able to set the associated regression weights to
  zero, effectively ignoring these input dimensions. As mentioned
  further above, another benefit of Bayesian regularization is that it
  doesn’t need a separate “validation set” to tune its parameters. How
  well this actually works depends on how close the data matches the
  assumptions underlying the different methods. Thus, different
  regularization approaches might work better or worse on different
  datasets. There currently exists no single best method that works best
  for all datasets.</p>
  <p>More specifically, the models underlying variational Bayesian
  linear and logistic regression implemented in this library are
  Bayesian hierarchical models with priors on the regression
  coefficient, as well as hyper-priors on the prior parameters. For the
  ARD variants, the hyper-priors are assigned to each of the regressors
  separately, which supports pruning eventually irrelevant coefficients
  (<xref alt="Wipf &amp; Nagarajan, 2008" rid="ref-WipfU003A2007" ref-type="bibr">Wipf
  &amp; Nagarajan, 2008</xref>). This happens without the need for a
  separate validation set, unlike comparable sparsity-inducing methods
  like the Lasso
  (<xref alt="Tibshirani, 1996" rid="ref-TibshiraniU003A1996" ref-type="bibr">Tibshirani,
  1996</xref>). Bishop
  (<xref alt="2006" rid="ref-BishopU003A2006" ref-type="bibr">2006</xref>)
  describes ARD only in the context of type-II maximum likelihood
  (<xref alt="MacKay, 1992" rid="ref-MacKayU003A1992" ref-type="bibr">MacKay,
  1992</xref>;
  <xref alt="Neal, 1996" rid="ref-NealU003A1996" ref-type="bibr">Neal,
  1996</xref>;
  <xref alt="Tipping, 2001" rid="ref-TippingU003A2001" ref-type="bibr">Tipping,
  2001</xref>), in which case the (hyper-)parameters are tuned by
  maximizing the marginal likelihood (or model evidence). The library
  instead provides implementations for the full Bayesian treatment, that
  finds the ARD hyper-posteriors by variational Bayesian inference.</p>
  <p>Since release <monospace>R2017a</monospace>, MATLAB also provides
  some functions for Bayesian linear regression, but none for Bayesian
  logistic regression. For linear regression, it provides variants with
  and without variable selection. Neither of the variants without
  variable selection use or infer hyper-priors. Therefore, they do not
  support inferring prior parameters in hierarchical models, unlike this
  library. The variants with variable selection either use a
  straight-foward Bayesian formulation of Lasso, or Stochastic Search
  Variable Selection (SSVS)
  (<xref alt="George &amp; McCulloch, 1993" rid="ref-GeorgeU003A1993" ref-type="bibr">George
  &amp; McCulloch, 1993</xref>). Both are different from ARD, and the
  advantages of either method remain to be clarified.</p>
  <p>Some of the scripts provided by this library have been included in
  the <monospace>pmtk3</monospace> software accompanying Murphy
  (<xref alt="2012" rid="ref-MurphyU003A2012" ref-type="bibr">2012</xref>).
  Part of the library has furthermore been used for various scientific
  publications across multiple domains (e.g.,
  <xref alt="Kanitscheider et al., 2015" rid="ref-KanitscheiderU003A2015" ref-type="bibr">Kanitscheider
  et al., 2015</xref>;
  <xref alt="Oh et al., 2016" rid="ref-OhU003A2016" ref-type="bibr">Oh
  et al., 2016</xref>;
  <xref alt="Ruxanda et al., 2018" rid="ref-RuxandaU003A2018" ref-type="bibr">Ruxanda
  et al., 2018</xref>;
  <xref alt="Wang &amp; Rehder, 2017" rid="ref-WangU003A2017" ref-type="bibr">Wang
  &amp; Rehder, 2017</xref>).</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-BealU003A2003">
      <element-citation publication-type="thesis">
        <person-group person-group-type="author">
          <name><surname>Beal</surname><given-names>Matthew J.</given-names></name>
        </person-group>
        <article-title>Variational algorithms for approximate Bayesian inference</article-title>
        <publisher-name>Gatsby Computational Neuroscience Unit, University College London</publisher-name>
        <year iso-8601-date="2003">2003</year>
        <pub-id pub-id-type="doi">10.1214/06-ba126</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BishopU003A2006">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Bishop</surname><given-names>Christopher M.</given-names></name>
        </person-group>
        <source>Pattern recognition and machine learning</source>
        <publisher-name>Springer-Verlag</publisher-name>
        <year iso-8601-date="2006">2006</year>
        <isbn>9780387310732</isbn>
      </element-citation>
    </ref>
    <ref id="ref-GelmanU003A2013">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
          <name><surname>Carlin</surname><given-names>John B.</given-names></name>
          <name><surname>Sern</surname><given-names>Hal S.</given-names></name>
          <name><surname>Dunson</surname><given-names>David B.</given-names></name>
          <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
          <name><surname>Rubin</surname><given-names>Donald B.</given-names></name>
        </person-group>
        <source>Bayesian data analysis</source>
        <publisher-name>Chapman; Hall</publisher-name>
        <year iso-8601-date="2013">2013</year>
        <isbn>9781439840955</isbn>
      </element-citation>
    </ref>
    <ref id="ref-GilksU003A1995">
      <element-citation publication-type="book">
        <source>Markov chain monte carlo in practice</source>
        <person-group person-group-type="editor">
          <name><surname>Gilks</surname><given-names>W. R.</given-names></name>
          <name><surname>Richardson</surname><given-names>S.</given-names></name>
          <name><surname>Spiegelhalter</surname><given-names>David</given-names></name>
        </person-group>
        <publisher-name>Chapman; Hall</publisher-name>
        <year iso-8601-date="1995">1995</year>
        <isbn>9780412055515</isbn>
        <pub-id pub-id-type="doi">10.1201/b14835</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-HastieU003A2011">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Hastie</surname><given-names>Trevor</given-names></name>
          <name><surname>Tibishirani</surname><given-names>Robert</given-names></name>
          <name><surname>Friedman</surname><given-names>Jerome</given-names></name>
        </person-group>
        <source>The elements of statistical learning</source>
        <publisher-name>Springer-Verlag</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <pub-id pub-id-type="doi">10.1007/978-0-387-84858-7</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MurphyU003A2012">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Murphy</surname><given-names>Kevin P.</given-names></name>
        </person-group>
        <source>Machine learning: A probabilistic perspective</source>
        <publisher-name>The MIT Press</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <isbn>9780262306164</isbn>
      </element-citation>
    </ref>
    <ref id="ref-WipfU003A2007">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Wipf</surname><given-names>David P.</given-names></name>
          <name><surname>Nagarajan</surname><given-names>Srikantan S.</given-names></name>
        </person-group>
        <article-title>A new view of automatic relevance determination</article-title>
        <source>Advances in neural information processing systems 20</source>
        <person-group person-group-type="editor">
          <name><surname>Platt</surname><given-names>J. C.</given-names></name>
          <name><surname>Koller</surname><given-names>D.</given-names></name>
          <name><surname>Singer</surname><given-names>Y.</given-names></name>
          <name><surname>Roweis</surname><given-names>S. T.</given-names></name>
        </person-group>
        <publisher-name>Curran Associates, Inc.</publisher-name>
        <year iso-8601-date="2008">2008</year>
        <uri>http://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-TibshiraniU003A1996">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
        </person-group>
        <article-title>Regression shrinkage and selection via the Lasso</article-title>
        <source>Journal of the Royal Statistical Society B</source>
        <year iso-8601-date="1996">1996</year>
        <volume>58</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MacKayU003A1992">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>MacKay</surname><given-names>David J. C.</given-names></name>
        </person-group>
        <article-title>Bayesian interpolation</article-title>
        <source>Neural Computation</source>
        <year iso-8601-date="1992">1992</year>
        <volume>4</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1162/neco.1992.4.3.415</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-GeorgeU003A1993">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>George</surname><given-names>Edward I.</given-names></name>
          <name><surname>McCulloch</surname><given-names>Robert E.</given-names></name>
        </person-group>
        <article-title>Variable selection via Gibbs sampling</article-title>
        <source>Journal of the American Statistical Association</source>
        <year iso-8601-date="1993">1993</year>
        <volume>88</volume>
        <issue>423</issue>
        <pub-id pub-id-type="doi">10.1080/01621459.1993.10476353</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-NealU003A1996">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Neal</surname><given-names>Radford M.</given-names></name>
        </person-group>
        <source>Bayesian learning for neural networks</source>
        <publisher-name>Springer-Verlag</publisher-name>
        <year iso-8601-date="1996">1996</year>
        <pub-id pub-id-type="doi">10.1007/978-1-4612-0745-0</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-TippingU003A2001">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tipping</surname><given-names>M. E.</given-names></name>
        </person-group>
        <article-title>Sparse Bayesian learning and the relevance vector machine ensembles</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2001">2001</year>
        <volume>1</volume>
        <pub-id pub-id-type="doi">10.1109/icdm.2012.58</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-DrugowitschU003A2013">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Drugowitsch</surname><given-names>Jan</given-names></name>
        </person-group>
        <article-title>Variational Bayesian inference for linear and logistic regression</article-title>
        <publisher-name>arXiv:1310.5438 [stat.ML]</publisher-name>
        <year iso-8601-date="2013">2013</year>
      </element-citation>
    </ref>
    <ref id="ref-KanitscheiderU003A2015">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kanitscheider</surname><given-names>Ingmar</given-names></name>
          <name><surname>Coen-Cagli</surname><given-names>Ruben</given-names></name>
          <name><surname>Kohn</surname><given-names>Adam</given-names></name>
          <name><surname>Pouget</surname><given-names>Alexandre</given-names></name>
        </person-group>
        <article-title>Measuring Fisher information accurately in correlated neural populations</article-title>
        <source>PLoS Computational Biology</source>
        <year iso-8601-date="2015">2015</year>
        <volume>11</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004218</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-OhU003A2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Oh</surname><given-names>Hanna</given-names></name>
          <name><surname>Beck</surname><given-names>Jeffrey M.</given-names></name>
          <name><surname>Zhu</surname><given-names>Pingping</given-names></name>
          <name><surname>Sommer</surname><given-names>Marc A.</given-names></name>
          <name><surname>Ferrari</surname><given-names>Silvia</given-names></name>
          <name><surname>Egner</surname><given-names>Tobias</given-names></name>
        </person-group>
        <article-title>Satisficing in split-second decision making is characterized by strategic cue discounting</article-title>
        <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>
        <year iso-8601-date="2016">2016</year>
        <volume>42</volume>
        <issue>12</issue>
        <pub-id pub-id-type="doi">10.1037/xlm0000284</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-WangU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wang</surname><given-names>Shaoming</given-names></name>
          <name><surname>Rehder</surname><given-names>Bob</given-names></name>
        </person-group>
        <article-title>Multi-attribute decision-making is best characterized by an attribute-wide reinforcement learning model</article-title>
        <source>bioRxiv</source>
        <year iso-8601-date="2017">2017</year>
        <pub-id pub-id-type="doi">10.1101/234732</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-RuxandaU003A2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Ruxanda</surname><given-names>Gheorghe</given-names></name>
          <name><surname>Zahfir</surname><given-names>Catalina</given-names></name>
          <name><surname>Muraru</surname><given-names>Andreea</given-names></name>
        </person-group>
        <article-title>Predicting financial distress from Romanian companies</article-title>
        <source>Technological and Economic Development of Economy</source>
        <year iso-8601-date="2018">2018</year>
        <volume>24</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.3846/tede.2018.6736</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
