<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2135</article-id>
<article-id pub-id-type="doi">10.21105/joss.02135</article-id>
<title-group>
<article-title>audiomate: A Python package for working with audio
datasets</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0207-5711</contrib-id>
<string-name>Matthias Büchi</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Andreas Ahlenstorf</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>ZHAW Zurich University of Applied Sciences, Winterthur,
Switzerland</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-12-20">
<day>20</day>
<month>12</month>
<year>2019</year>
</pub-date>
<volume>5</volume>
<issue>52</issue>
<fpage>2135</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>audio</kwd>
<kwd>speech</kwd>
<kwd>music</kwd>
<kwd>corpus</kwd>
<kwd>dataset</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Machine learning tasks in the audio domain frequently require large
  datasets with training data. Over the last years, numerous datasets
  have been made available for various purposes, for example,
  (<xref alt="Snyder et al., 2015" rid="ref-musan2015" ref-type="bibr">Snyder
  et al., 2015</xref>) and
  (<xref alt="Ardila et al., 2019" rid="ref-ardila2019common" ref-type="bibr">Ardila
  et al., 2019</xref>). Unfortunately, most of the datasets are stored
  in widely differing formats. As a consequence, machine learning
  practitioners have to convert datasets into other formats before they
  can be used or combined. Furthermore, common tasks like reading,
  partitioning, or shuffling of datasets have to be developed over and
  over again for each format and require intimate knowledge of the
  formats. We purpose Audiomate, a Python toolkit, to solve this
  problem.</p>
  <p>Audiomate provides a uniform programming interface to work with
  numerous datasets. Knowledge about the structure or on-disk format of
  the datasets is not necessary. Audiomate facilitates and simplifies a
  wide range of tasks:</p>
  <list list-type="bullet">
    <list-item>
      <p>Reading and writing of numerous dataset formats using a uniform
      programming interface, for example
      (<xref alt="Snyder et al., 2015" rid="ref-musan2015" ref-type="bibr">Snyder
      et al., 2015</xref>),
      (<xref alt="Panayotov et al., 2015" rid="ref-Panayotov2015LibrispeechAA" ref-type="bibr">Panayotov
      et al., 2015</xref>) and
      (<xref alt="Ardila et al., 2019" rid="ref-ardila2019common" ref-type="bibr">Ardila
      et al., 2019</xref>)</p>
    </list-item>
    <list-item>
      <p>Accessing metadata, like speaker information and labels</p>
    </list-item>
    <list-item>
      <p>Reading audio data (single files, batches of files)</p>
    </list-item>
    <list-item>
      <p>Retrieval of information about the data (e.g., number of
      speakers, total duration).</p>
    </list-item>
    <list-item>
      <p>Merging of multiple datasets (e.g., combine two speech
      datasets).</p>
    </list-item>
    <list-item>
      <p>Splitting data into smaller subsets (e.g., create training,
      validation, and test sets with a reasonable distribution of
      classes).</p>
    </list-item>
    <list-item>
      <p>Validation of data for specific requirements (e.g., check
      whether all samples were assigned a label)</p>
    </list-item>
  </list>
</sec>
<sec id="use-cases">
  <title>Use Cases</title>
  <p>To illustrate Audiomate’s capabilities, we present two typical
  applications where Audiomate significantly simplifies the task of a
  developer: Training a speech recognition model with Mozilla’s
  implementation of DeepSpeech and training a deep neural network to
  recognize music.</p>
  <sec id="converting-datasets">
    <title>Converting Datasets</title>
    <p>In this example, we illustrate how to employ Audiomate to convert
    the LibriSpeech dataset
    (<xref alt="Panayotov et al., 2015" rid="ref-Panayotov2015LibrispeechAA" ref-type="bibr">Panayotov
    et al., 2015</xref>) into the CSV-format expected by Mozilla’s
    implementation (https://github.com/mozilla/DeepSpeech) of DeepSpeech
    (<xref alt="Hannun et al., 2014" rid="ref-deepspeech" ref-type="bibr">Hannun
    et al., 2014</xref>) which can, in turn, be used to train an
    automatic speech recognition model.</p>
    <code language="python">import audiomate
from audiomate.corpus import io

# Download LibriSpeech corpus
downloader = io.LibriSpeechDownloader()
downloader.download('/local/data/librispeech')

# Read LibriSpeech
reader = io.LibriSpeechReader()
librispeech = reader.load('/local/data/librispeech')

# Save in DeepSpeech format
writer = io.MozillaDeepSpeechWriter()
writer.save(librispeech, '/local/data/librispeech_ds')</code>
    <p>Knowledge of the on-disk formats of the datasets is not
    required.</p>
    <p>Some datasets contain invalid or corrupted files. If those are
    known, Audiomate tries to rectify the problems or automatically
    excludes those files before processing any data.</p>
  </sec>
  <sec id="merging-and-partitioning-datasets">
    <title>Merging and Partitioning Datasets</title>
    <p>Another area where Audiomate excels is mixing datasets and
    partitioning them into training, test, and validation sets. Assume
    that the task is to train a neural network to detect segments in
    audio streams that are music. MUSAN
    (<xref alt="Snyder et al., 2015" rid="ref-musan2015" ref-type="bibr">Snyder
    et al., 2015</xref>) and GTZAN
    (<xref alt="GTZAN Music/Speech Collection, n.d." rid="ref-GTZAN" ref-type="bibr"><italic>GTZAN
    Music/Speech Collection</italic>, n.d.</xref>) are two suitable
    datasets for this task because they provide a wide selection of
    music, speech, and noise samples. In the example below, we first
    download MUSAN and GTZAN to the local disk before creating
    <monospace>Loader</monospace> instances for each format that allow
    Audiomate to access both datasets using a unified interface. Then,
    we instruct Audiomate to merge both datasets. Afterwards, we use a
    <monospace>Splitter</monospace> to partition the merged dataset into
    a train and test set. By merely creating views, Audiomate avoids
    creating unnecessary disk I/O and is therefore ideally suited to
    work with large datasets in the range of tens or hundreds of
    gigabytes. Ultimately, we load the samples and labels by iterating
    over all utterances. Audio samples are numpy arrays. They allow for
    fast access, high processing speed and ensure interoperability with
    third-party programs that can operate on numpy arrays, for example
    TensorFlow or PyTorch. Alternatively, it is possible to load the
    samples in batches, which is ideal for feeding them to a deep
    learning toolkit like PyTorch.</p>
    <code language="python">import audiomate
from audiomate.corpus import io
from audiomate.corpus import subset

musan_dl = io.MusanDownloader()
musan_dl.download('/local/data/musan')

gtzan_dl = io.GtzanDownloader()
gtzan_dl.download('/local/data/gtzan')

musan = audiomate.Corpus.load('/local/data/musan', reader='musan')
gtzan = audiomate.Corpus.load('/local/data/gtzan', reader='gtzan')

full = audiomate.Corpus.merge_corpora([musan, gtzan])

splitter = subset.Splitter(full, random_seed=222)
subviews = splitter.split(proportions={
    'train': 0.8,
    'test': 0.2,
})

for utterance in subviews['train'].utterances.values():
    samples = utterance.read_samples()
    labels = utterance.label_lists[audiomate.corpus.LL_DOMAIN]</code>
  </sec>
</sec>
<sec id="implementation">
  <title>Implementation</title>
  <p>Audiomate was designed with extensibility in mind. Therefore, it is
  straightforward to add support for additional data formats. Support
  for another format can be added by implementing at least one of three
  available abstract interfaces.</p>
  <list list-type="bullet">
    <list-item>
      <p>Reader: A <monospace>Reader</monospace> defines the procedure
      to load data that is structured in a specific format. It converts
      it into a Audiomate-specific data structure.</p>
    </list-item>
    <list-item>
      <p>Writer: A <monospace>Writer</monospace> defines the procedure
      to store data in a specific format. It does that by converting the
      data from the Audiomate-specific data structure into the target
      format.</p>
    </list-item>
    <list-item>
      <p>Downloader: A <monospace>Downloader</monospace> can be used to
      download a dataset. It downloads all required files
      automatically.</p>
    </list-item>
  </list>
  <p>Rarely all interfaces are implemented for a particular format.
  Usually, <monospace>Reader</monospace> and
  <monospace>Downloader</monospace> are implemented for datasets, while
  <monospace>Writer</monospace> is implemented for machine learning
  toolkits.</p>
  <p>Audiomate supports more than a dozen datasets and half as many
  toolkits.</p>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>A variety of frameworks and tools offer functionality similar to
  Audiomate.</p>
  <p><bold>Data loaders</bold> Data loaders are libraries that focus on
  downloading and preprocessing data sets to make them easily accessible
  without requiring a specific tool or framework. In contrast to
  Audiomate, they cannot convert between formats, split or merge data
  sets. Examples of libraries in that category are
  (<xref alt="Mirdata, 2020" rid="ref-mirdata" ref-type="bibr"><italic>Mirdata</italic>,
  2020</xref>),
  (<xref alt="Speech Corpus Downloader, 2020" rid="ref-speechcorpusdownloader" ref-type="bibr"><italic>Speech
  Corpus Downloader</italic>, 2020</xref>), and
  (<xref alt="Audio Datasets, 2020" rid="ref-audiodatasets" ref-type="bibr"><italic>Audio
  Datasets</italic>, 2020</xref>). Furthermore, some of these libraries
  focus on a particular kind of data, such as music, and do not assist
  with speech data sets.</p>
  <p><bold>Tools for specific frameworks</bold> Various machine learning
  tools and deep learning frameworks include the necessary
  infrastructure to make various datasets readily available to their
  users. One notable example is TensorFlow
  (<xref alt="Abadi et al., 2016" rid="ref-tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>), which includes data loaders for different kinds
  of data, including image, speech, and music data sets, such as
  (<xref alt="Ardila et al., 2019" rid="ref-ardila2019common" ref-type="bibr">Ardila
  et al., 2019</xref>). Another one is torchaudio
  (<xref alt="TORCHAUDIO, 2020" rid="ref-torchaudio" ref-type="bibr"><italic>TORCHAUDIO</italic>,
  2020</xref>) for PyTorch, which not only offers data loaders but is
  also capable of converting between various formats. In contrast to
  Audiomate, those tools or libraries support a specific machine
  learning or deep learning framework (TensorFlow or PyTorch,
  respectively), whereas Audiomate is framework agnostic.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-musan2015">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Snyder</surname><given-names>David</given-names></name>
          <name><surname>Chen</surname><given-names>Guoguo</given-names></name>
          <name><surname>Povey</surname><given-names>Daniel</given-names></name>
        </person-group>
        <article-title>MUSAN: A Music, Speech, and Noise Corpus</article-title>
        <year iso-8601-date="2015">2015</year>
      </element-citation>
    </ref>
    <ref id="ref-GTZAN">
      <element-citation>
        <article-title>GTZAN music/speech collection</article-title>
        <publisher-name>http://marsyas.info/downloads/datasets.html</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-Panayotov2015LibrispeechAA">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Panayotov</surname><given-names>Vassil</given-names></name>
          <name><surname>Chen</surname><given-names>Guoguo</given-names></name>
          <name><surname>Povey</surname><given-names>Daniel</given-names></name>
          <name><surname>Khudanpur</surname><given-names>Sanjeev</given-names></name>
        </person-group>
        <article-title>Librispeech: An ASR corpus based on public domain audio books</article-title>
        <source>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>
        <year iso-8601-date="2015">2015</year>
        <pub-id pub-id-type="doi">10.1109/icassp.2015.7178964</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-deepspeech">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hannun</surname><given-names>Awni Y.</given-names></name>
          <name><surname>Case</surname><given-names>Carl</given-names></name>
          <name><surname>Casper</surname><given-names>Jared</given-names></name>
          <name><surname>Catanzaro</surname><given-names>Bryan</given-names></name>
          <name><surname>Diamos</surname><given-names>Greg</given-names></name>
          <name><surname>Elsen</surname><given-names>Erich</given-names></name>
          <name><surname>Prenger</surname><given-names>Ryan</given-names></name>
          <name><surname>Satheesh</surname><given-names>Sanjeev</given-names></name>
          <name><surname>Sengupta</surname><given-names>Shubho</given-names></name>
          <name><surname>Coates</surname><given-names>Adam</given-names></name>
          <name><surname>Ng</surname><given-names>Andrew Y.</given-names></name>
        </person-group>
        <article-title>Deep speech: Scaling up end-to-end speech recognition</article-title>
        <source>CoRR</source>
        <year iso-8601-date="2014">2014</year>
        <volume>abs/1412.5567</volume>
        <uri>http://arxiv.org/abs/1412.5567</uri>
      </element-citation>
    </ref>
    <ref id="ref-ardila2019common">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Ardila</surname><given-names>Rosana</given-names></name>
          <name><surname>Branson</surname><given-names>Megan</given-names></name>
          <name><surname>Davis</surname><given-names>Kelly</given-names></name>
          <name><surname>Henretty</surname><given-names>Michael</given-names></name>
          <name><surname>Kohler</surname><given-names>Michael</given-names></name>
          <name><surname>Meyer</surname><given-names>Josh</given-names></name>
          <name><surname>Morais</surname><given-names>Reuben</given-names></name>
          <name><surname>Saunders</surname><given-names>Lindsay</given-names></name>
          <name><surname>Tyers</surname><given-names>Francis M.</given-names></name>
          <name><surname>Weber</surname><given-names>Gregor</given-names></name>
        </person-group>
        <article-title>Common voice: A massively-multilingual speech corpus</article-title>
        <year iso-8601-date="2019">2019</year>
        <uri>https://arxiv.org/abs/1912.06670</uri>
      </element-citation>
    </ref>
    <ref id="ref-torchaudio">
      <element-citation>
        <article-title>TORCHAUDIO</article-title>
        <publisher-name>https://pytorch.org/audio/</publisher-name>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-audiodatasets">
      <element-citation>
        <article-title>Audio datasets</article-title>
        <publisher-name>https://github.com/mcfletch/audiodatasets</publisher-name>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-mirdata">
      <element-citation>
        <article-title>Mirdata</article-title>
        <publisher-name>https://github.com/mir-dataset-loaders/mirdata</publisher-name>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-speechcorpusdownloader">
      <element-citation>
        <article-title>Speech corpus downloader</article-title>
        <publisher-name>https://github.com/mdangschat/speech-corpus-dl</publisher-name>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-tensorflow">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
          <name><surname>Barham</surname><given-names>Paul</given-names></name>
          <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
          <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
          <name><surname>Davis</surname><given-names>Andy</given-names></name>
          <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
          <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
          <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
          <name><surname>Isard</surname><given-names>Michael</given-names></name>
          <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
          <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
          <name><surname>Monga</surname><given-names>Rajat</given-names></name>
          <name><surname>Moore</surname><given-names>Sherry</given-names></name>
          <name><surname>Murray</surname><given-names>Derek G.</given-names></name>
          <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
          <name><surname>Tucker</surname><given-names>Paul</given-names></name>
          <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
          <name><surname>Warden</surname><given-names>Pete</given-names></name>
          <name><surname>Wicke</surname><given-names>Martin</given-names></name>
          <name><surname>Yu</surname><given-names>Yuan</given-names></name>
          <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
        </person-group>
        <article-title>TensorFlow: A system for large-scale machine learning</article-title>
        <source>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</source>
        <publisher-name>USENIX Association</publisher-name>
        <publisher-loc>Savannah, GA</publisher-loc>
        <year iso-8601-date="2016-11">2016</year><month>11</month>
        <isbn>978-1-931971-33-1</isbn>
        <uri>https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi</uri>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
