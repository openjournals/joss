<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1478</article-id>
<article-id pub-id-type="doi">10.21105/joss.01478</article-id>
<title-group>
<article-title>Fireworks: Reproducible Machine Learning and
Preprocessing with PyTorch</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4686-4923</contrib-id>
<string-name>Saad M. Khan</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Libusha Kelly</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Systems &amp; Computational Biology, Albert Einstein
College of Medicine</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-04-23">
<day>23</day>
<month>4</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>39</issue>
<fpage>1478</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>PyTorch</kwd>
<kwd>batch processing</kwd>
<kwd>machine learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Here, we present a batch-processing library for constructing
  machine learning pipelines using PyTorch and dataframes. It is meant
  to provide an easy method to stream data from a dataset into a machine
  learning model while performing reprocessing steps such as
  randomization, train/test split, batch normalization, etc. along the
  way. Fireworks offers more flexibility and structure for constructing
  input pipelines than the built-in dataset modules in PyTorch
  (<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>), but is also meant to be easier to use than
  frameworks such as Apache Spark
  (<xref alt="Zaharia et al., 2016" rid="ref-spark" ref-type="bibr">Zaharia
  et al., 2016</xref>).</p>
  <p>Steps in a pipeline are represented using objects that can be
  composed in a reusable manner with a standard input and output. These
  input/outputs are performed via a DataFrame-like object called a
  Message which can have PyTorch Tensor-valued columns in addition to
  all of the functionality of a Pandas DataFrame
  (<xref alt="McKinney, 2010" rid="ref-mckinney-proc-scipy-2010" ref-type="bibr">McKinney,
  2010</xref>). Each of these pipeline objects can be serialized,
  enabling one to save the entire state of their workflow rather than
  just an individual model’s when creating checkpoints. Additionally, we
  provide a suite of extensions and tools built on top of this library
  to facilitate common tasks such as relational database access, model
  training, hyperparameter optimization, saving/loading of pipelines,
  and more. A pain point in all deep learning frameworks is data entry;
  one has to take data in its original form and convert it into the form
  that the model expects (for example tensors, TFRecords
  (<xref alt="Abadi et al., 2015" rid="ref-tensorflow2015-whitepaper" ref-type="bibr">Abadi
  et al., 2015</xref>)). Data conversion can include acquiring the data
  from a database or an API call, formatting data, applying
  preprocessing transforms, and preparing mini batches of data for
  training. At each step, one must be cognizant of memory, storage,
  bandwidth, and compute limitations. For example, if a dataset is too
  large to fit in memory, then it must be streamed or broken into
  chunks. In practice, dealing with these issues is time consuming.
  Moreover, ad-hoc solutions often introduce biases. For example, if one
  chooses not to shuffle their dataset before sampling it due to its
  size, then the order of elements in that dataset becomes a source of
  bias. Ideally, proper statistical methodology should not have to be
  compromised for performance and memory considerations, but in
  practice, that often becomes the case.</p>
  <p>We developed Fireworks to to enable reproducible machine learning
  pipelines by overcoming some of these challenges in data handling and
  processing. Pipelines are made up of objects called Pipes which can
  represent some transformation. Each Pipe has an input and an output.
  As data flows from Pipe to Pipe, transforms are applied one at a time,
  and because each Pipe is independent, they can be stacked and reused.
  Operations are performed on a pipeline by performing method calls on
  the most downstream Pipe. These method calls are then recursively
  chained up the pipeline. So for example, if one calls iterator methods
  on the most downstream Pipe (‘iter’ and ‘next’), the iteration will
  trigger a cascade by which the most upstream Pipe produces its next
  output and each successive Pipe applies a transformation before
  handing it off. The same phenomena will apply when calling any method,
  such as ‘getitem’ for data access. This recursive architecture enables
  each Pipe to abstract away the inner workings of its upstream Pipe and
  present a view of the dataset that a downstream Pipe can build
  additional abstractions off of. For example, a Pipe could cache data
  from its upstream sources so that whenever a downstream data access
  call is made, the caching Pipe can retrieve the item from its cache.
  As another example, a Pipe could virtually shuffle its inputs so that
  every time an element at a given index is requested, a random element
  at some other index is returned instead. This framework also enables
  just-in-time evaluation of data transformations. A downstream Pipe
  could call a method only implemented by an upstream Pipe, enabling one
  to delay applying a transformation until a certain point in a
  pipeline. Lastly, there are additional primitives for creating
  branching pipelines and representing PyTorch models as Pipes that are
  described in the documentation.</p>
  <fig>
    <caption><p>Illustration of the core primitives in
    Fireworks.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="Models.png" xlink:title="" />
  </fig>
  <p>Because of this modular architecture and the standardization of
  input/output via the Message object, one can adapt traditional machine
  learning pipelines built around the Pandas library, because Messages
  behave like DataFrames. As a result, Fireworks is useful for any data
  processing task, not just for deep learning pipelines, as described
  here. It can be used, for example, to interact with a database and
  construct statistical models with Pandas as well. Additionally, we
  provides modules for database querying, saving/loading checkpoints,
  hyperparameter optimization, and model training in addition premade
  functions for common preprocessing tasks such as batch normalization
  and one-hot vector mapping using Pipes and Messages. All of these
  features are designed to improve the productivity of researchers and
  the reproducibility of their work by reducing the amount of time spent
  on these tasks and providing a standardized means to do so. The
  overall design of Fireworks is aimed at transparency and
  backwards-compatibility with respect to the underlying libraries. This
  library is designed to be easy to incorporate into a project without
  requiring one to change the other aspects of their workflow in order
  to help researchers implement cleaner machine learning pipelines in a
  reproducible manner to enhance the scientific rigor of their work.</p>
  <p>We currently use this library in our lab to implement neural
  network models for studying the human microbiome and for finding phage
  genes within bacterial genomes. In the future, we will implement
  integration with other popular data science libraries in order to
  facilitate the inclusion of Fireworks in common data workflows. For
  example, distributed data processing systems such as Apache Spark
  could be used to acquire initial data from a larger data-set that can
  then be pre-processed and mini-batched using Fireworks to train a
  PyTorch Model. The popular python library SciKit-Learn
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) also has numerous pre-processing tools and models
  that could be used within a pipeline. There is also a growing interest
  in conducting data science research on the cloud in a distributed
  environment. Libraries such as KubeFlow
  (<xref alt="Kubeflow, 2019" rid="ref-kubeflow_2019" ref-type="bibr"><italic>Kubeflow</italic>,
  2019</xref>) enable one to spawn experiments onto a cloud environment
  so that multiple runs can be performed and tracked simultaneously. In
  the future, we will integrate these libraries and features with
  Fireworks.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-paszke2017automatic">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Paszke</surname><given-names>Adam</given-names></name>
          <name><surname>Gross</surname><given-names>Sam</given-names></name>
          <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
          <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
          <name><surname>Yang</surname><given-names>Edward</given-names></name>
          <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
          <name><surname>Lin</surname><given-names>Zeming</given-names></name>
          <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
          <name><surname>Antiga</surname><given-names>Luca</given-names></name>
          <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        </person-group>
        <article-title>Automatic differentiation in PyTorch</article-title>
        <source>NIPS Autodiff Workshop</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-tensorflow2015-whitepaper">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
          <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
          <name><surname>Barham</surname><given-names>Paul</given-names></name>
          <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
          <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
          <name><surname>Citro</surname><given-names>Craig</given-names></name>
          <name><surname>Corrado</surname><given-names>Greg S.</given-names></name>
          <name><surname>Davis</surname><given-names>Andy</given-names></name>
          <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
          <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
          <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
          <name><surname>Harp</surname><given-names>Andrew</given-names></name>
          <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
          <name><surname>Isard</surname><given-names>Michael</given-names></name>
          <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
          <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
          <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
          <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
          <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
          <name><surname>Mané</surname><given-names>Dandelion</given-names></name>
          <name><surname>Monga</surname><given-names>Rajat</given-names></name>
          <name><surname>Moore</surname><given-names>Sherry</given-names></name>
          <name><surname>Murray</surname><given-names>Derek</given-names></name>
          <name><surname>Olah</surname><given-names>Chris</given-names></name>
          <name><surname>Schuster</surname><given-names>Mike</given-names></name>
          <name><surname>Shlens</surname><given-names>Jonathon</given-names></name>
          <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
          <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
          <name><surname>Talwar</surname><given-names>Kunal</given-names></name>
          <name><surname>Tucker</surname><given-names>Paul</given-names></name>
          <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
          <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
          <name><surname>Viégas</surname><given-names>Fernanda</given-names></name>
          <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
          <name><surname>Warden</surname><given-names>Pete</given-names></name>
          <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
          <name><surname>Wicke</surname><given-names>Martin</given-names></name>
          <name><surname>Yu</surname><given-names>Yuan</given-names></name>
          <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
        </person-group>
        <article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title>
        <year iso-8601-date="2015">2015</year>
        <uri>https://www.tensorflow.org/</uri>
      </element-citation>
    </ref>
    <ref id="ref-mckinney-proc-scipy-2010">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>McKinney</surname><given-names>Wes</given-names></name>
        </person-group>
        <article-title>Data Structures for Statistical Computing in Python</article-title>
        <source>Proceedings of the 9th Python in Science Conference</source>
        <person-group person-group-type="editor">
          <name><surname>Walt</surname><given-names>Stéfan van der</given-names></name>
          <name><surname>Millman</surname><given-names>Jarrod</given-names></name>
        </person-group>
        <year iso-8601-date="2010">2010</year>
      </element-citation>
    </ref>
    <ref id="ref-spark">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Zaharia</surname><given-names>Matei</given-names></name>
          <name><surname>Xin</surname><given-names>Reynold S.</given-names></name>
          <name><surname>Wendell</surname><given-names>Patrick</given-names></name>
          <name><surname>Das</surname><given-names>Tathagata</given-names></name>
          <name><surname>Armbrust</surname><given-names>Michael</given-names></name>
          <name><surname>Dave</surname><given-names>Ankur</given-names></name>
          <name><surname>Meng</surname><given-names>Xiangrui</given-names></name>
          <name><surname>Rosen</surname><given-names>Josh</given-names></name>
          <name><surname>Venkataraman</surname><given-names>Shivaram</given-names></name>
          <name><surname>Franklin</surname><given-names>Michael J.</given-names></name>
          <name><surname>Ghodsi</surname><given-names>Ali</given-names></name>
          <name><surname>Gonzalez</surname><given-names>Joseph</given-names></name>
          <name><surname>Shenker</surname><given-names>Scott</given-names></name>
          <name><surname>Stoica</surname><given-names>Ion</given-names></name>
        </person-group>
        <article-title>Apache spark: A unified engine for big data processing</article-title>
        <source>Commun. ACM</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year iso-8601-date="2016-10">2016</year><month>10</month>
        <volume>59</volume>
        <issue>11</issue>
        <issn>0001-0782</issn>
        <uri>http://doi.acm.org/10.1145/2934664</uri>
        <pub-id pub-id-type="doi">10.1145/2934664</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-scikit-learn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
          <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
          <name><surname>Gramfort</surname><given-names>A.</given-names></name>
          <name><surname>Michel</surname><given-names>V.</given-names></name>
          <name><surname>Thirion</surname><given-names>B.</given-names></name>
          <name><surname>Grisel</surname><given-names>O.</given-names></name>
          <name><surname>Blondel</surname><given-names>M.</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
          <name><surname>Weiss</surname><given-names>R.</given-names></name>
          <name><surname>Dubourg</surname><given-names>V.</given-names></name>
          <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
          <name><surname>Passos</surname><given-names>A.</given-names></name>
          <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
          <name><surname>Brucher</surname><given-names>M.</given-names></name>
          <name><surname>Perrot</surname><given-names>M.</given-names></name>
          <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
      </element-citation>
    </ref>
    <ref id="ref-kubeflow_2019">
      <element-citation>
        <article-title>Kubeflow</article-title>
        <publisher-name>https://www.kubeflow.org/</publisher-name>
        <year iso-8601-date="2019-01">2019</year><month>01</month>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
