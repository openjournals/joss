<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1800</article-id>
<article-id pub-id-type="doi">10.21105/joss.01800</article-id>
<title-group>
<article-title>Distant Viewing Toolkit: A Python Package for the
Analysis of Visual Culture</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0576-0669</contrib-id>
<string-name>Taylor Arnold</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4629-8888</contrib-id>
<string-name>Lauren Tilton</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Richmond, Department of Mathematics and
Computer Science</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University of Richmond, Department of Rhetoric and
Communication Studies</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-08-04">
<day>4</day>
<month>8</month>
<year>2019</year>
</pub-date>
<volume>5</volume>
<issue>45</issue>
<fpage>1800</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>digital humanities</kwd>
<kwd>media studies</kwd>
<kwd>computational social science</kwd>
<kwd>time-based media</kwd>
<kwd>visual culture</kwd>
<kwd>computer vision</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The Distant Viewing Toolkit is a Python package for the
  computational analysis of visual culture. It addresses the challenges
  of working with moving images through the automated extraction and
  visualization of metadata summarizing the content (e.g.,
  people/actors, dialogue, scenes, objects) and style (e.g., shot angle,
  shot length, lighting, framing, sound) of time-based media. This
  toolkit is optimized for two purposes: (1) scholarly inquiry of visual
  culture from the humanities and social sciences, and (2) search and
  discovery of collections within libraries, archives, and museums.</p>
  <p>Many open-source projects provide implementations of
  state-of-the-art computer vision algorithms. However, there are
  limited options for users looking to quickly build end-to-end
  pipelines that link together common visual annotations. Those tools
  that do exist focus on specific subtasks, such as as FaceNet’s
  recognition of faces
  (<xref alt="Schroff et al., 2015" rid="ref-schroff2015facenet" ref-type="bibr">Schroff
  et al., 2015</xref>), visual embedding with PixPlot
  (<xref alt="Duhaime, 2019" rid="ref-duhaime2020pixplot" ref-type="bibr">Duhaime,
  2019</xref>), and FilmColors’ analysis of color
  (<xref alt="Flueckiger &amp; Halter, 2018" rid="ref-flueckiger2018building" ref-type="bibr">Flueckiger
  &amp; Halter, 2018</xref>). Different algorithms require varying
  dependencies, different input formats, and produce outputs using
  different schemas. Because of the rapid pace of scholarship across the
  many sub-fields of computer vision, it can be difficult to determine
  which algorithms to use and a significant amount of work to manually
  test every option. These challenges are exacerbated when working with
  moving images because most available computer vision libraries take
  still images as inputs.</p>
  <p>The Distant Viewing Toolkit addresses these needs by (1)
  constructing an object-oriented framework for applying a collection of
  algorithms to moving images, (2) packaging together common sound and
  computer vision algorithms in order to provide out-of-the-box
  functionality for common tasks in the computational analysis of moving
  images, and (3) allowing video files alongside still images as an
  input. Currently provided algorithms include functionality for: shot
  detection
  (<xref alt="Pal et al., 2015" rid="ref-pal2015video" ref-type="bibr">Pal
  et al., 2015</xref>), object detection
  (<xref alt="Li et al., 2018" rid="ref-li2018recurrent" ref-type="bibr">Li
  et al., 2018</xref>), face detection
  (<xref alt="Jiang &amp; Learned-Miller, 2017" rid="ref-jiang2017face" ref-type="bibr">Jiang
  &amp; Learned-Miller, 2017</xref>), face identification
  (<xref alt="Cao et al., 2018" rid="ref-cao2018vggface2" ref-type="bibr">Cao
  et al., 2018</xref>), color analysis
  (<xref alt="Karasek et al., 2015" rid="ref-karasek2015color" ref-type="bibr">Karasek
  et al., 2015</xref>), image similarity
  (<xref alt="Szegedy et al., 2017" rid="ref-szegedy2017inception" ref-type="bibr">Szegedy
  et al., 2017</xref>), optical flow
  (<xref alt="Farnebäck, 2003" rid="ref-farneback2003two" ref-type="bibr">Farnebäck,
  2003</xref>), and shot distance analysis
  (<xref alt="Butler, 2012" rid="ref-butler2012television" ref-type="bibr">Butler,
  2012</xref>).</p>
  <p>The Distant Viewing Toolkit provides two interfaces. The low-level
  Python API provides for customized and advanced processing of visual
  data. The high-level command line interface is designed to be
  accessible to users with limited programming experience. Metadata
  produced by either interface can also be further aggregated and
  analyzed to find patterns across a corpus. Drawing on theories of
  exploratory data analysis, the package includes a custom JavaScript
  visualization engine that can be run on a user’s machine to visualize
  the metadata for search and discovery. Together, these provide tools
  for the increasingly popular application of computational methods to
  the study of visual culture
  (<xref alt="Wevers &amp; Smits, 2019" rid="ref-wevers2019visual" ref-type="bibr">Wevers
  &amp; Smits, 2019</xref>). The following sections provide further
  explaination of the interfaces and visualization engine followed by a
  section on the process and development of the package. Detailed
  documentation and tutorials are provided in the package’s
  documentation:
  <ext-link ext-link-type="uri" xlink:href="https://distant-viewing.github.io/dvt/">https://distant-viewing.github.io/dvt/</ext-link>.</p>
  <fig>
    <caption><p>Schematic of the Distant Viewing Toolkit’s internal
    architecture. Algorithms are split into two types: annotators that
    have access to small chunks of the raw inputs and aggregators that
    have access to all of the extracted annotations but not the input
    data itself.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./img/process.png" xlink:title="" />
  </fig>
</sec>
<sec id="low-level-python-api">
  <title>Low-level Python API</title>
  <p>The full functionality of the Distant Viewing Toolkit is available
  when using the full Python API. The toolkit starts by constructing a
  <monospace>DataExtraction</monospace> object that is associated with
  input data (either a video file or a collection of still images).
  Algorithms are then applied to the extraction object, with results
  stored as Pandas DataFrames that can be exported as CSV or JSON files.
  There are two distinct types of algorithms:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>annotators</bold>: algorithms that work directly with the
      source data but are able to only work with a small subset of
      frames or still images</p>
    </list-item>
    <list-item>
      <p><bold>aggregators</bold>: algorithms that have access to
      information extracted from all previously run annotators and
      aggregators across across the entire input, but cannot directly
      access the visual data</p>
    </list-item>
  </list>
  <p>The separation of algorithms into these two parts enables the
  writing of straightforward, error-free code and closely mirrors the
  theory of <italic>Distant Viewing</italic>
  (<xref alt="Arnold &amp; Tilton, 2019" rid="ref-arnold2019distant" ref-type="bibr">Arnold
  &amp; Tilton, 2019</xref>):</p>
  <disp-quote>
    <p>Distant viewing is distinguished from other approaches by making
    explicit the interpretive nature of extracting semantic metadata
    from images. In other words, one must ‘view’ visual materials before
    studying them. Viewing, which we define as an interpretive action
    taken by either a person or a model, is necessitated by the way in
    which information is transferred in visual materials. Therefore, in
    order to view images computationally, a representation of elements
    contained within the visual material—a code system in semiotics or,
    similarly, a metadata schema in informatics—must be constructed.
    Algorithms capable of automatically converting raw images into the
    established representation are then needed to apply the approach at
    scale.</p>
  </disp-quote>
  <p>The annotator algorithms conduct the process of “viewing” the
  material whereas the aggregator algorithms perform a “distant” (e.g.,
  separated from the raw materials) analysis of the visual inputs. The
  schematic in Figure 1 shows the relationship between these algorithms
  and the respective input and output formats.</p>
  <p>There are many annotators and aggregators currently available in
  the toolkit. Pipelines—pre-bundled sequences of annotators and
  aggregators—are also included in the package. Details of these
  implementations can be found in the API documentation. Users can also
  construct custom Annotator and Aggregator objects, as described in the
  documentation’s tutorial.</p>
  <fig>
    <caption><p>Example video page from the Distant Viewing Toolkit’s
    command line video visualization applied to a short test clip. Users
    can click on the pull-down menus to select a video file and choose
    desired annotation type. Forward and backward buttons as well as a
    slider allow for scrolling through the frames within a particular
    video file. Hovering over an image displays the extracted metadata
    for each frame.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./img/dvt-view.png" xlink:title="" />
  </fig>
  <fig>
    <caption><p>Example of the overlay shown when clicking on an
    individual frame. Users can use the navigation buttons on the right
    to close the overlay or choose to scroll through adjacent
    frames.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./img/dvt-view-frame.png" xlink:title="" />
  </fig>
</sec>
<sec id="high-level-command-line-interface">
  <title>High-level Command Line Interface</title>
  <p>The command line tools provide a fast way to get started with the
  toolkit. Designed for users with no experience programming and minimal
  knowledge of machine learning, it is ideal for quickly getting
  meaningful results. Users call the command line by directly executing
  the Python module (e.g., “python -m dvt”), specifying the desired
  pipeline, and pointing to a video file or directory of images. The
  output data can be visualized using a local webserver. Figures 2 and 3
  show an example of the video visualization using a short video clip.
  While the command line interface is meant to be easy to run
  out-of-the-box, it also affords a high-level of customization through
  command line options. These are documented within the toolkit using
  the <bold>argparse</bold> package. It is also possible to modify the
  visualization using custom CSS and JavaScript code. This makes the
  command-line interface particularly well-suited for classroom use,
  following the task-driven paradigm popular in Digital Humanities
  pedagogy
  (<xref alt="Birnbaum &amp; Langmead, 2017" rid="ref-birnbaum2017task" ref-type="bibr">Birnbaum
  &amp; Langmead, 2017</xref>).</p>
</sec>
<sec id="process-and-development">
  <title>Process and Development</title>
  <p>Development of the Distant Viewing Toolkit follows best-practices
  for open-source software development
  (<xref alt="Wilson et al., 2014" rid="ref-wilson2014best" ref-type="bibr">Wilson
  et al., 2014</xref>). Development of the software is done publicly
  through our GitHub repository, which has both a stable master branch
  and experimental development branch. The project includes an
  open-source license (GPL-2), uses the Contributor Covenant Code of
  Conduct (v1.4), and provides user templates for submitting issues and
  pull requests
  (<xref alt="Tourani et al., 2017" rid="ref-tourani2017code" ref-type="bibr">Tourani
  et al., 2017</xref>). We make use of integrated unit testing through
  the <bold>pytest</bold> package and TravisCI. The code passes checks
  for conforming to the common Python coding styles (checked with
  <bold>pycodestyle</bold>) and the relatively aggressive checks
  provided by <bold>pylint</bold>
  (<xref alt="Reitz &amp; Schlusser, 2016" rid="ref-reitz2016hitchhiker" ref-type="bibr">Reitz
  &amp; Schlusser, 2016</xref>). JavaScript coding style was verified
  with the static checkers JSHint and JSLint
  (<xref alt="Santos et al., 2015" rid="ref-santos2015using" ref-type="bibr">Santos
  et al., 2015</xref>). Stable versions of the package are posted to
  PyPI as both source packages and pre-compiled wheels to make
  installation as straightforward as possible.</p>
  <p>Because our audiences have different levels of comfort with
  computational approaches, we have streamlined the installation
  process. The package can be installed using a fresh version of
  Anaconda Python and packages available through the pip command on PyPI
  (both of which can be installed through GUIs if preferred). We have
  kept dependencies to a minimum, and have avoided software that is
  known to be difficult to install. For example, all of the included
  deep-learning algorithms are built using Tensorflow to avoid requiring
  multiple deep-learning libraries
  (<xref alt="Abadi et al., 2016" rid="ref-abadi2016tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>). While Tensorflow can occasionally throw errors,
  we have found the CPU-version to be relatively error-free for
  non-technical users to install on both Windows and macOS relative to
  popular alternatives such as Caffe and Torch.</p>
  <p>As of version 0.2.0, the core architecture of the toolkit is
  stable. We plan to continue to update the available algorithms
  available in the toolkit as well as make improvements to the
  interactive, web-based interface. We also continue to utilize the
  toolkit for specific applications with research partnerships. Our
  first published example application looks at Network-Era Sitcoms in
  the United States and offers a model for the kind of analysis of
  visual culture made possible by the Distant Viewing Toolkit
  (<xref alt="Arnold et al., 2019" rid="ref-arnold2019visual" ref-type="bibr">Arnold
  et al., 2019</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The Distant Viewing Toolkit is supported through a Digital
  Humanities Advancement Grant from the National Endowment for the
  Humanities (HAA-261239-18).</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-abadi2016tensorflow">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
          <name><surname>Barham</surname><given-names>Paul</given-names></name>
          <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
          <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
          <name><surname>Davis</surname><given-names>Andy</given-names></name>
          <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
          <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
          <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
          <name><surname>Isard</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>Tensorflow: A system for large-scale machine learning</article-title>
        <source>12th USENIX symposium on operating systems design and implementation (OSDI 16)</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-arnold2019distant">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Arnold</surname><given-names>Taylor B</given-names></name>
          <name><surname>Tilton</surname><given-names>Lauren</given-names></name>
        </person-group>
        <article-title>Distant viewing: Analyzing large visual corpora</article-title>
        <source>Digital Scholarship in the Humanities</source>
        <year iso-8601-date="2019">2019</year>
        <uri>http://dx.doi.org/10.1093/digitalsh/fqz013</uri>
        <pub-id pub-id-type="doi">10.1093/digitalsh/fqz013</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-arnold2019visual">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Arnold</surname><given-names>Taylor B</given-names></name>
          <name><surname>Tilton</surname><given-names>Lauren</given-names></name>
          <name><surname>Berke</surname><given-names>Annie</given-names></name>
        </person-group>
        <article-title>Visual style in two Network Era sitcoms</article-title>
        <source>Cultural Analytics</source>
        <year iso-8601-date="2019">2019</year>
        <uri>http://dx.doi.org/10.22148/16.043</uri>
        <pub-id pub-id-type="doi">10.22148/16.043</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-birnbaum2017task">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Birnbaum</surname><given-names>David J</given-names></name>
          <name><surname>Langmead</surname><given-names>Alison</given-names></name>
        </person-group>
        <article-title>Task-driven programming pedagogy in the digital humanities</article-title>
        <source>New directions for computing education</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <pub-id pub-id-type="doi">10.1007/978-3-319-54226-3_5</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-butler2012television">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Butler</surname><given-names>Jeremy G</given-names></name>
        </person-group>
        <source>Television: Critical methods and applications</source>
        <publisher-name>Routledge</publisher-name>
        <year iso-8601-date="2012">2012</year>
      </element-citation>
    </ref>
    <ref id="ref-cao2018vggface2">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Cao</surname><given-names>Qiong</given-names></name>
          <name><surname>Shen</surname><given-names>Li</given-names></name>
          <name><surname>Xie</surname><given-names>Weidi</given-names></name>
          <name><surname>Parkhi</surname><given-names>Omkar M</given-names></name>
          <name><surname>Zisserman</surname><given-names>Andrew</given-names></name>
        </person-group>
        <article-title>VggFace2: A dataset for recognising faces across pose and age</article-title>
        <source>2018 13th IEEE international conference on automatic face &amp; gesture recognition</source>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.1109/fg.2018.00020</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-duhaime2020pixplot">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Duhaime</surname><given-names>Douglas</given-names></name>
        </person-group>
        <article-title>PixPlot</article-title>
        <publisher-name>https://github.com/YaleDHLab/pix-plot</publisher-name>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-farneback2003two">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Farnebäck</surname><given-names>Gunnar</given-names></name>
        </person-group>
        <article-title>Two-frame motion estimation based on polynomial expansion</article-title>
        <source>Scandinavian conference on image analysis</source>
        <year iso-8601-date="2003">2003</year>
        <pub-id pub-id-type="doi">10.1007/3-540-45103-x_50</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-flueckiger2018building">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Flueckiger</surname><given-names>Barbara</given-names></name>
          <name><surname>Halter</surname><given-names>Gaudenz</given-names></name>
        </person-group>
        <article-title>Building a crowdsourcing platform for the analysis of film colors</article-title>
        <source>Moving Image: The Journal of the Association of Moving Image Archivists</source>
        <year iso-8601-date="2018">2018</year>
        <volume>18</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.5749/movingimage.18.1.0080</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-jiang2017face">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Jiang</surname><given-names>Huaizu</given-names></name>
          <name><surname>Learned-Miller</surname><given-names>Erik</given-names></name>
        </person-group>
        <article-title>Face detection with the faster r-CNN</article-title>
        <source>2017 12th IEEE international conference on automatic face &amp; gesture recognition</source>
        <year iso-8601-date="2017">2017</year>
        <pub-id pub-id-type="doi">10.1109/fg.2017.82</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-karasek2015color">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Karasek</surname><given-names>Jan</given-names></name>
          <name><surname>Burget</surname><given-names>Radim</given-names></name>
          <name><surname>Uher</surname><given-names>Vaclav</given-names></name>
          <name><surname>Masek</surname><given-names>Jan</given-names></name>
          <name><surname>Dutta</surname><given-names>Malay Kishore</given-names></name>
        </person-group>
        <article-title>Color image (dis) similarity assessment and grouping based on dominant colors</article-title>
        <source>2015 38th international conference on telecommunications and signal processing (TSP)</source>
        <year iso-8601-date="2015">2015</year>
        <pub-id pub-id-type="doi">10.1109/tsp.2015.7296366</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-li2018recurrent">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Li</surname><given-names>Xiaobo</given-names></name>
          <name><surname>Zhao</surname><given-names>Haohua</given-names></name>
          <name><surname>Zhang</surname><given-names>Liqing</given-names></name>
        </person-group>
        <article-title>Recurrent RetinaNet: A video object detection model based on focal loss</article-title>
        <source>International conference on neural information processing</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.1007/978-3-030-04212-7_44</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-pal2015video">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Pal</surname><given-names>Gautam</given-names></name>
          <name><surname>Rudrapaul</surname><given-names>Dwijen</given-names></name>
          <name><surname>Acharjee</surname><given-names>Suvojit</given-names></name>
          <name><surname>Ray</surname><given-names>Ruben</given-names></name>
          <name><surname>Chakraborty</surname><given-names>Sayan</given-names></name>
          <name><surname>Dey</surname><given-names>Nilanjan</given-names></name>
        </person-group>
        <article-title>Video shot boundary detection: A review</article-title>
        <source>Emerging ICT for bridging the future-proceedings of the 49th annual convention of the computer society of India CSI volume 2</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2015">2015</year>
      </element-citation>
    </ref>
    <ref id="ref-reitz2016hitchhiker">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Reitz</surname><given-names>Kenneth</given-names></name>
          <name><surname>Schlusser</surname><given-names>Tanya</given-names></name>
        </person-group>
        <source>The hitchhiker’s guide to python: Best practices for development</source>
        <publisher-name>O’Reilly Media, Inc.</publisher-name>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-santos2015using">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Santos</surname><given-names>Adriano L</given-names></name>
          <name><surname>Valente</surname><given-names>Marco Tulio</given-names></name>
          <name><surname>Figueiredo</surname><given-names>Eduardo</given-names></name>
        </person-group>
        <article-title>Using JavaScript static checkers on GitHub systems: A first evaluation</article-title>
        <source>Proccedings of the 3rd workshop on software visualization, evolution and maintenance (VEM)</source>
        <year iso-8601-date="2015">2015</year>
      </element-citation>
    </ref>
    <ref id="ref-schroff2015facenet">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Schroff</surname><given-names>Florian</given-names></name>
          <name><surname>Kalenichenko</surname><given-names>Dmitry</given-names></name>
          <name><surname>Philbin</surname><given-names>James</given-names></name>
        </person-group>
        <article-title>Facenet: A unified embedding for face recognition and clustering</article-title>
        <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
        <year iso-8601-date="2015">2015</year>
        <pub-id pub-id-type="doi">10.1109/cvpr.2015.7298682</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-szegedy2017inception">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Szegedy</surname><given-names>Christian</given-names></name>
          <name><surname>Ioffe</surname><given-names>Sergey</given-names></name>
          <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
          <name><surname>Alemi</surname><given-names>Alexander A</given-names></name>
        </person-group>
        <article-title>Inception-v4, inception-resnet and the impact of residual connections on learning</article-title>
        <source>Thirty-first AAAI conference on artificial intelligence</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-tourani2017code">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Tourani</surname><given-names>Parastou</given-names></name>
          <name><surname>Adams</surname><given-names>Bram</given-names></name>
          <name><surname>Serebrenik</surname><given-names>Alexander</given-names></name>
        </person-group>
        <article-title>Code of conduct in open source projects</article-title>
        <source>2017 IEEE 24th international conference on software analysis, evolution and reengineering (SANER)</source>
        <year iso-8601-date="2017">2017</year>
        <pub-id pub-id-type="doi">10.1109/saner.2017.7884606</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wevers2019visual">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wevers</surname><given-names>Melvin</given-names></name>
          <name><surname>Smits</surname><given-names>Thomas</given-names></name>
        </person-group>
        <article-title>The visual digital turn: Using neural networks to study historical images</article-title>
        <source>Digital Scholarship in the Humanities</source>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.1093/llc/fqy085</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wilson2014best">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wilson</surname><given-names>Greg</given-names></name>
          <name><surname>Aruliah</surname><given-names>Dhavide A</given-names></name>
          <name><surname>Brown</surname><given-names>C Titus</given-names></name>
          <name><surname>Hong</surname><given-names>Neil P Chue</given-names></name>
          <name><surname>Davis</surname><given-names>Matt</given-names></name>
          <name><surname>Guy</surname><given-names>Richard T</given-names></name>
          <name><surname>Haddock</surname><given-names>Steven HD</given-names></name>
          <name><surname>Huff</surname><given-names>Kathryn D</given-names></name>
          <name><surname>Mitchell</surname><given-names>Ian M</given-names></name>
          <name><surname>Plumbley</surname><given-names>Mark D</given-names></name>
        </person-group>
        <article-title>Best practices for scientific computing</article-title>
        <source>PLoS biology</source>
        <publisher-name>Public Library of Science</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <volume>12</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
