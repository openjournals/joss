<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2891</article-id>
<article-id pub-id-type="doi">10.21105/joss.02891</article-id>
<title-group>
<article-title>Learning Simulator: A simulation software for animal and
human learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1242-3599</contrib-id>
<string-name>Markus Jonsson</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7270-9612</contrib-id>
<string-name>Stefano Ghirlanda</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-4159-6926</contrib-id>
<string-name>Johan Lind</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Vera Vinken</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<string-name>Magnus Enquist</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Centre for Cultural Evolution, Stockholm University,
Stockholm, Sweden</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Psychology, Brooklyn College and Graduate
Center, CUNY, New York, NY, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Biosciences Institute, Newcastle University, Newcastle upon
Tyne, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Department of Zoology, Stockholm University,
Sweden</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-11-04">
<day>4</day>
<month>11</month>
<year>2020</year>
</pub-date>
<volume>6</volume>
<issue>58</issue>
<fpage>2891</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>associative learning</kwd>
<kwd>reinforcement learning</kwd>
<kwd>behavior</kwd>
<kwd>mathematical model</kwd>
<kwd>simulation</kwd>
<kwd>gui</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Learning Simulator is a software for simulating learning in animals
  and humans, as studied for example in experimental psychology and
  animal cognition research. It is primarily intended for computational
  and behavioral biologists, ethologists, and psychologists, and it can
  also be useful to students and teachers in these fields.</p>
</sec>
<sec id="introduction">
  <title>Introduction</title>
  <p>Learning Simulator was developed to study learning in animals and
  humans. The current version implements associative learning (AL) and
  reinforcement learning (RL) algorithms, apt to study instrumental
  (operant) learning and Pavlovian (classical) learning
  (<xref alt="Bouton, 2016" rid="ref-BoutonU003A2016" ref-type="bibr">Bouton,
  2016</xref>;
  <xref alt="Pearce, 2013" rid="ref-PearceU003A2013" ref-type="bibr">Pearce,
  2013</xref>), including in complex situations such as social learning
  or maze learning. A plugin system to add more learning mechanisms is
  planned for a future version.</p>
  <p>The simulator uses a common framing of learning that comprises a
  subject interacting with an environment. The environment presents a
  stimulus to the subject, and the subject responds with a behavior. As
  a result, the environment presents the next stimulus that the subject
  responds to, and so on. See
  <xref alt="Figure 1" rid="figU003Asystem-fig">Figure 1</xref>.</p>
  <fig>
    <caption><p>The subject and the world can be seen as two interacting
    dynamical systems, where the state variables in the subject
    determine the probabilities for its behaviors (the subject’s
    output), and each behavior from the subject puts the environment in
    a state that determines (probabilistically) its output stimulus.
    <styled-content id="figU003Asystem-fig"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="system-fig.pdf" xlink:title="" />
  </fig>
  <p>The stimuli that the environment presents and the behaviors that
  the subject can exhibit are pre-defined by the user of the program.
  Each stimulus is given a reinforcement value (corresponding to
  genetically determined values in biological organisms). A stimulus
  such as food would typically have a positive value, while the
  perception of harm to the body would have a negative value.</p>
  <p>As seen in
  <xref alt="Figure 1" rid="figU003Asystem-fig">Figure 1</xref>, the
  consequence of responding with a behavior (say
  <inline-formula><alternatives>
  <tex-math><![CDATA[B]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>B</mml:mi></mml:math></alternatives></inline-formula>)
  to a stimulus (say <inline-formula><alternatives>
  <tex-math><![CDATA[S]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>S</mml:mi></mml:math></alternatives></inline-formula>)
  is that the subject meets the next stimulus (say
  <inline-formula><alternatives>
  <tex-math><![CDATA[S']]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>S</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>):
  <disp-formula><alternatives>
  <tex-math><![CDATA[S \to B \to S'. \nonumber]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>S</mml:mi><mml:mo>→</mml:mo><mml:mi>B</mml:mi><mml:mo>→</mml:mo><mml:mi>S</mml:mi><mml:mi>′</mml:mi><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  Learning algorithms can then use the reinforcement value of
  <inline-formula><alternatives>
  <tex-math><![CDATA[S']]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>S</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  as an indication of the quality of the response
  <inline-formula><alternatives>
  <tex-math><![CDATA[B]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>B</mml:mi></mml:math></alternatives></inline-formula>
  to <inline-formula><alternatives>
  <tex-math><![CDATA[S]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>S</mml:mi></mml:math></alternatives></inline-formula>.
  Specifically, this can be accomplished by updating one or more of the
  subject’s memory state variables. These state variables control the
  probabilities of future responses: if the response
  <inline-formula><alternatives>
  <tex-math><![CDATA[B]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>B</mml:mi></mml:math></alternatives></inline-formula>
  to stimulus <inline-formula><alternatives>
  <tex-math><![CDATA[S]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>S</mml:mi></mml:math></alternatives></inline-formula>
  leads to a reward (a stimulus with high reinforcement value), the
  subject will be more likely to respond with
  <inline-formula><alternatives>
  <tex-math><![CDATA[B]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>B</mml:mi></mml:math></alternatives></inline-formula>
  the next time it faces <inline-formula><alternatives>
  <tex-math><![CDATA[S]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>S</mml:mi></mml:math></alternatives></inline-formula>.</p>
  <p>The user of Learning Simulator specifies in a text script how the
  output stimulus from the environment depends on the subject’s response
  to the previous stimulus. This script also specifies the values of all
  parameters used in the learning process. The simulation script,
  written in a simple and well-documented scripting language, is the
  only input to Learning Simulator, facilitating reproducible workflows.
  The script also specifies how to visualize the simulation data, for
  example how a memory state variable changes during the simulation.
  Learning Simulator can also export results to CSV files.</p>
  <p>More information is available at
  https://www.learningsimulator.org.</p>
</sec>
<sec id="applications-of-associative-learning">
  <title>Applications of associative learning</title>
  <p>Associative learning theory has a rich tradition of computational
  modeling, which has recently converged with RL learning models from
  machine learning
  (<xref alt="Sutton &amp; Barto, 2018" rid="ref-SuttonU003A2018" ref-type="bibr">Sutton
  &amp; Barto, 2018</xref>). During the last decade or so, AL and RL
  algorithms have proven increasingly powerful. </p>
  <p>Firstly, when applied to deep neural networks, RL has been used to
  teach computers to find optimal play and achieve human-level skills in
  chess
  (<xref alt="Silver et al., 2017" rid="ref-SilverU003A2017" ref-type="bibr">Silver
  et al., 2017</xref>) and the Chinese board game Go
  (<xref alt="Silver et al., 2016" rid="ref-SilverU003A2016" ref-type="bibr">Silver
  et al., 2016</xref>).</p>
  <p>Secondly, behaving optimally (or near-optimally) is central to
  animals’ adaptation to their environment. AL and RL can provide
  explanations for a wide range of learning phenomena in both humans and
  animals
  (<xref alt="Dayan &amp; Niv, 2008" rid="ref-DayanU003A2008" ref-type="bibr">Dayan
  &amp; Niv, 2008</xref>;
  <xref alt="Enquist et al., 2016" rid="ref-EnquistU003A2016" ref-type="bibr">Enquist
  et al., 2016</xref>;
  <xref alt="Ghirlanda et al., 2020" rid="ref-GhirlandaU003A2020" ref-type="bibr">Ghirlanda
  et al., 2020</xref>;
  <xref alt="Niv, 2009" rid="ref-NivU003A2009" ref-type="bibr">Niv,
  2009</xref>). Indeed, investigating predictions about animal and human
  behavior is the main motivation behind Learning Simulator, with the
  goal to understand biological learning and its numerous applications
  to animal welfare and training
  (<xref alt="McGreevy &amp; Boakes, 2011" rid="ref-McgreevyU003A2011" ref-type="bibr">McGreevy
  &amp; Boakes, 2011</xref>), and to human health
  (<xref alt="Bernstein, 1999" rid="ref-BernsteinU003A1999" ref-type="bibr">Bernstein,
  1999</xref>;
  <xref alt="Haselgrove &amp; Hogarth, 2013" rid="ref-HaselgroveU003A2013" ref-type="bibr">Haselgrove
  &amp; Hogarth, 2013</xref>;
  <xref alt="Schachtman &amp; Reilly, 2011" rid="ref-SchachtmanU003A2011" ref-type="bibr">Schachtman
  &amp; Reilly, 2011</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>As a result of a strong theoretical tradition, and of the many
  application areas, there are now many AL and RL models with varying
  properties and varying predictive power in different environments.</p>
  <p>This situation has created a need for a general simulation software
  offering a convenient way to investigate different learning models.
  The first aim of our software is to fulfil this need.</p>
  <p>The second aim is to provide a generic, flexible way to describe
  unambiguously the design of learning experiments or, more generally,
  the features of any learning scenario.</p>
  <p>The main advantage of Learning Simulator is its simple scripting
  language that provides a way to explore and understand learning
  mechanisms in a multitude of learning scenarios, and investigate the
  effects of varying their underlying parameters.</p>
  <p>A further strength is the simplicity with which even complex
  learning environments can be specified, such as psychological
  experiments with complex dependencies between stimuli and behaviors.
  The scripting language has been developed to be useful to any
  researcher of learning phenomena – not necessarily computer
  programmers.</p>
  <p>Our software has been used in scientific publications
  (<xref alt="Ghirlanda et al., 2020" rid="ref-GhirlandaU003A2020" ref-type="bibr">Ghirlanda
  et al., 2020</xref>;
  <xref alt="Lind, 2018" rid="ref-LindU003A2018" ref-type="bibr">Lind,
  2018</xref>;
  <xref alt="Lind et al., 2019" rid="ref-LindU003A2019" ref-type="bibr">Lind
  et al., 2019</xref>) as well as in teaching at the Ethology Master’s
  Programme at Stockholm University, at the Veterinary Programme at the
  Swedish University of Agricultural Sciences, and at the Department of
  Psychology of Brooklyn College, City University of New York.</p>
  <p>An open source license and an accessible scripting language enable
  further scientific exploration of learning phenomena by students and
  experts alike within the fields of biology, ethology, psychology, and
  neuroscience.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>Other simulating software either specialize in one specific
  mechanism
  (<xref alt="Schultheis et al., 2008a" rid="ref-SchultheisU003A2008_1" ref-type="bibr">Schultheis
  et al., 2008a</xref>;
  <xref alt="Alonso et al., 2012" rid="ref-AlonsoU003A2012" ref-type="bibr">Alonso
  et al., 2012</xref>;
  <xref alt="Schultheis et al., 2008b" rid="ref-SchultheisU003A2008_2" ref-type="bibr">Schultheis
  et al., 2008b</xref>) or only include models of classical conditioning
  (<xref alt="George, 2019" rid="ref-learnSim" ref-type="bibr">George,
  2019</xref>;
  <xref alt="Harris &amp; Livesey, 2010" rid="ref-HarrisU003A2010" ref-type="bibr">Harris
  &amp; Livesey, 2010</xref>;
  <xref alt="Thorwart et al., 2009" rid="ref-ThorwartU003A2009" ref-type="bibr">Thorwart
  et al., 2009</xref>), or provide only one learning algorithm or
  environment
  (<xref alt="Q-Learning Simulator, n.d." rid="ref-QLSim" ref-type="bibr"><italic>Q-Learning
  Simulator</italic>, n.d.</xref>).</p>
  <p>Learning Simulator provides a common interface to several AL and RL
  algorithms:</p>
  <list list-type="bullet">
    <list-item>
      <p><italic>Stimulus-response learning</italic>
      (<xref alt="Bush &amp; Mosteller, 1951" rid="ref-BushU003A1951" ref-type="bibr">Bush
      &amp; Mosteller, 1951</xref>),</p>
    </list-item>
    <list-item>
      <p><italic>Rescorla-Wagner</italic>
      (<xref alt="Rescorla &amp; Wagner, 1972" rid="ref-RescorlaU003A1972" ref-type="bibr">Rescorla
      &amp; Wagner, 1972</xref>),</p>
    </list-item>
    <list-item>
      <p><italic>Actor-critic</italic>
      (<xref alt="Sutton &amp; Barto, 2018" rid="ref-SuttonU003A2018" ref-type="bibr">Sutton
      &amp; Barto, 2018</xref>;
      <xref alt="Witten, 1977" rid="ref-WittenU003A1977" ref-type="bibr">Witten,
      1977</xref>),</p>
    </list-item>
    <list-item>
      <p><italic>Q-learning</italic>
      (<xref alt="Watkins, 1989" rid="ref-WatkinsU003A1989" ref-type="bibr">Watkins,
      1989</xref>;
      <xref alt="Watkins &amp; Dayan, 1992" rid="ref-WatkinsU003A1992" ref-type="bibr">Watkins
      &amp; Dayan, 1992</xref>),</p>
    </list-item>
    <list-item>
      <p><italic>Expected SARSA</italic>
      (<xref alt="Van Seijen et al., 2009" rid="ref-vanSeijenU003A2009" ref-type="bibr">Van
      Seijen et al., 2009</xref>), and</p>
    </list-item>
    <list-item>
      <p><italic>A-learning</italic>
      (<xref alt="Ghirlanda et al., 2020" rid="ref-GhirlandaU003A2020" ref-type="bibr">Ghirlanda
      et al., 2020</xref>)</p>
    </list-item>
  </list>
  <p>facilitating direct comparison of these mechanisms. </p>
</sec>
<sec id="repository">
  <title>Repository</title>
  <p>The program is written in Python and its source code repository is
  hosted on GitHub. Its documentation is hosted on Read the Docs. The
  repository incorporates continuous integration with automatic build of
  the documentation, linting, and execution of a test suite along with
  code coverage measurement. </p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Research and development was supported by the Knut and Alice
  Wallenberg Foundation (2015.0005).</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-SilverU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Silver</surname><given-names>David</given-names></name>
          <name><surname>Hubert</surname><given-names>Thomas</given-names></name>
          <name><surname>Schrittwieser</surname><given-names>Julian</given-names></name>
          <name><surname>Antonoglou</surname><given-names>Ioannis</given-names></name>
          <name><surname>Lai</surname><given-names>Matthew</given-names></name>
          <name><surname>Guez</surname><given-names>Arthur</given-names></name>
          <name><surname>Lanctot</surname><given-names>Marc</given-names></name>
          <name><surname>Sifre</surname><given-names>Laurent</given-names></name>
          <name><surname>Kumaran</surname><given-names>Dharshan</given-names></name>
          <name><surname>Graepel</surname><given-names>Thore</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Mastering chess and shogi by self-play with a general reinforcement learning algorithm</article-title>
        <source>arXiv preprint arXiv:1712.01815</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-SilverU003A2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Silver</surname><given-names>David</given-names></name>
          <name><surname>Huang</surname><given-names>Aja</given-names></name>
          <name><surname>Maddison</surname><given-names>Chris J</given-names></name>
          <name><surname>Guez</surname><given-names>Arthur</given-names></name>
          <name><surname>Sifre</surname><given-names>Laurent</given-names></name>
          <name><surname>Van Den Driessche</surname><given-names>George</given-names></name>
          <name><surname>Schrittwieser</surname><given-names>Julian</given-names></name>
          <name><surname>Antonoglou</surname><given-names>Ioannis</given-names></name>
          <name><surname>Panneershelvam</surname><given-names>Veda</given-names></name>
          <name><surname>Lanctot</surname><given-names>Marc</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Mastering the game of go with deep neural networks and tree search</article-title>
        <source>nature</source>
        <publisher-name>Nature Publishing Group</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <volume>529</volume>
        <issue>7587</issue>
      </element-citation>
    </ref>
    <ref id="ref-EnquistU003A2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Enquist</surname><given-names>Magnus</given-names></name>
          <name><surname>Lind</surname><given-names>Johan</given-names></name>
          <name><surname>Ghirlanda</surname><given-names>Stefano</given-names></name>
        </person-group>
        <article-title>The power of associative learning and the ontogeny of optimal behaviour</article-title>
        <source>Royal Society Open Science</source>
        <publisher-name>The Royal Society</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <volume>3</volume>
        <issue>11</issue>
        <pub-id pub-id-type="doi">10.1098/rsos.160734</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-McgreevyU003A2011">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>McGreevy</surname><given-names>Paul</given-names></name>
          <name><surname>Boakes</surname><given-names>Robert</given-names></name>
        </person-group>
        <source>Carrots and sticks: Principles of animal training</source>
        <publisher-name>Darlington Press</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <pub-id pub-id-type="doi">10.1111/j.1751-0813.2010.00653.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BernsteinU003A1999">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bernstein</surname><given-names>Ilene L.</given-names></name>
        </person-group>
        <article-title>Taste aversion learning: A contemporary perspective</article-title>
        <source>Nutrition</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="1999">1999</year>
        <volume>15</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1016/s0899-9007(98)00192-0</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-HaselgroveU003A2013">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Haselgrove</surname><given-names>Mark</given-names></name>
          <name><surname>Hogarth</surname><given-names>Lee</given-names></name>
        </person-group>
        <source>Clinical applications of learning theory</source>
        <publisher-name>Psychology Press</publisher-name>
        <year iso-8601-date="2013">2013</year>
        <pub-id pub-id-type="doi">10.4324/9780203803509</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-SchachtmanU003A2011">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Schachtman</surname><given-names>Todd R.</given-names></name>
          <name><surname>Reilly</surname><given-names>Steve S.</given-names></name>
        </person-group>
        <source>Associative learning and conditioning theory: Human and non-human applications</source>
        <publisher-name>OUP USA</publisher-name>
        <year iso-8601-date="2011">2011</year>
      </element-citation>
    </ref>
    <ref id="ref-LindU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lind</surname><given-names>Johan</given-names></name>
          <name><surname>Ghirlanda</surname><given-names>Stefano</given-names></name>
          <name><surname>Enquist</surname><given-names>Magnus</given-names></name>
        </person-group>
        <article-title>Social learning through associative processes: A computational theory</article-title>
        <source>Royal Society Open Science</source>
        <publisher-name>The Royal Society</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>6</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1101/446906</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-LindU003A2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lind</surname><given-names>Johan</given-names></name>
        </person-group>
        <article-title>What can associative learning do for planning?</article-title>
        <source>Royal Society Open Science</source>
        <publisher-name>The Royal Society</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>5</volume>
        <issue>11</issue>
        <pub-id pub-id-type="doi">10.1098/rsos.180778</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-PearceU003A2013">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Pearce</surname><given-names>John M</given-names></name>
        </person-group>
        <source>Animal learning and cognition: An introduction</source>
        <publisher-name>Psychology Press</publisher-name>
        <publisher-loc>Hove, Sussex</publisher-loc>
        <year iso-8601-date="2013">2013</year>
      </element-citation>
    </ref>
    <ref id="ref-BoutonU003A2016">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Bouton</surname><given-names>ME</given-names></name>
        </person-group>
        <source>Learning and behavior: A modern synthesis</source>
        <publisher-name>Sinauer</publisher-name>
        <publisher-loc>Sunderland, MA</publisher-loc>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-SchultheisU003A2008_1">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schultheis</surname><given-names>Holger</given-names></name>
          <name><surname>Thorwart</surname><given-names>Anna</given-names></name>
          <name><surname>Lachnit</surname><given-names>Harald</given-names></name>
        </person-group>
        <article-title>HMS: A MATLAB simulator of the Harris model of associative learning</article-title>
        <source>Behavior Research Methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2008">2008</year>
        <volume>40</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.3758/brm.40.2.442</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-HarrisU003A2010">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Harris</surname><given-names>Justin A</given-names></name>
          <name><surname>Livesey</surname><given-names>Evan J</given-names></name>
        </person-group>
        <article-title>An attention-modulated associative network</article-title>
        <source>Learning &amp; Behavior</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2010">2010</year>
        <volume>38</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.3758/lb.38.1.1</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-AlonsoU003A2012">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Alonso</surname><given-names>Eduardo</given-names></name>
          <name><surname>Mondragón</surname><given-names>Esther</given-names></name>
          <name><surname>Fernández</surname><given-names>Alberto</given-names></name>
        </person-group>
        <article-title>A Java simulator of Rescorla and Wagner’s prediction error model and configural cue extensions</article-title>
        <source>Computer Methods and Programs in Biomedicine</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <volume>108</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2012.02.004</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-learnSim">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>George</surname><given-names>David N</given-names></name>
        </person-group>
        <article-title>learnSim</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://github.com/DavidNGeorge/learnSim</uri>
      </element-citation>
    </ref>
    <ref id="ref-ThorwartU003A2009">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Thorwart</surname><given-names>Anna</given-names></name>
          <name><surname>Schultheis</surname><given-names>Holger</given-names></name>
          <name><surname>König</surname><given-names>Stephan</given-names></name>
          <name><surname>Lachnit</surname><given-names>Harald</given-names></name>
        </person-group>
        <article-title>ALTSim: A MATLAB simulator for current associative learning theories</article-title>
        <source>Behavior Research Methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>41</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.3758/brm.41.1.29</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-SchultheisU003A2008_2">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schultheis</surname><given-names>Holger</given-names></name>
          <name><surname>Thorwart</surname><given-names>Anna</given-names></name>
          <name><surname>Lachnit</surname><given-names>Harald</given-names></name>
        </person-group>
        <article-title>Rapid-REM: A MATLAB simulator of the replaced-elements model</article-title>
        <source>Behavior Research Methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2008">2008</year>
        <volume>40</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.3758/brm.40.2.435</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BushU003A1951">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bush</surname><given-names>Robert R</given-names></name>
          <name><surname>Mosteller</surname><given-names>Frederick</given-names></name>
        </person-group>
        <article-title>A mathematical model for simple learning</article-title>
        <source>Psychological Review</source>
        <publisher-name>American Psychological Association</publisher-name>
        <year iso-8601-date="1951">1951</year>
        <volume>58</volume>
        <issue>5</issue>
        <pub-id pub-id-type="doi">10.1037/h0054388</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-RescorlaU003A1972">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Rescorla</surname><given-names>R. A.</given-names></name>
          <name><surname>Wagner</surname><given-names>A. R.</given-names></name>
        </person-group>
        <article-title>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</article-title>
        <source>Classical conditioning: Current research and theory</source>
        <person-group person-group-type="editor">
          <name><surname>Black</surname><given-names>A. H.</given-names></name>
          <name><surname>Prokasy</surname><given-names>W. F.</given-names></name>
        </person-group>
        <publisher-name>Appleton-Century-Crofts</publisher-name>
        <year iso-8601-date="1972">1972</year>
      </element-citation>
    </ref>
    <ref id="ref-WatkinsU003A1989">
      <element-citation publication-type="thesis">
        <person-group person-group-type="author">
          <name><surname>Watkins</surname><given-names>Christopher John Cornish Hellaby</given-names></name>
        </person-group>
        <article-title>Learning from delayed rewards</article-title>
        <publisher-name>King’s College, Cambridge</publisher-name>
        <year iso-8601-date="1989">1989</year>
      </element-citation>
    </ref>
    <ref id="ref-WatkinsU003A1992">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Watkins</surname><given-names>Christopher John Cornish Hellaby</given-names></name>
          <name><surname>Dayan</surname><given-names>Peter</given-names></name>
        </person-group>
        <article-title>Q-learning</article-title>
        <source>Machine learning</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="1992">1992</year>
        <volume>8</volume>
        <issue>3-4</issue>
      </element-citation>
    </ref>
    <ref id="ref-vanSeijenU003A2009">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Van Seijen</surname><given-names>Harm</given-names></name>
          <name><surname>Van Hasselt</surname><given-names>Hado</given-names></name>
          <name><surname>Whiteson</surname><given-names>Shimon</given-names></name>
          <name><surname>Wiering</surname><given-names>Marco</given-names></name>
        </person-group>
        <article-title>A theoretical and empirical analysis of Expected Sarsa</article-title>
        <source>2009 IEEE symposium on adaptive dynamic programming and reinforcement learning</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <pub-id pub-id-type="doi">10.1109/adprl.2009.4927542</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-WittenU003A1977">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Witten</surname><given-names>Ian H</given-names></name>
        </person-group>
        <article-title>An adaptive optimal controller for discrete-time Markov environments</article-title>
        <source>Information and Control</source>
        <year iso-8601-date="1977">1977</year>
        <volume>34</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1016/s0019-9958(77)90354-0</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-GhirlandaU003A2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Ghirlanda</surname><given-names>Stefano</given-names></name>
          <name><surname>Lind</surname><given-names>Johan</given-names></name>
          <name><surname>Enquist</surname><given-names>Magnus</given-names></name>
        </person-group>
        <article-title>A-learning: A new formulation of associative learning theory</article-title>
        <source>Psychonomic Bulletin &amp; Review</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <pub-id pub-id-type="doi">10.3758/s13423-020-01749-0</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-QLSim">
      <element-citation>
        <article-title>Q-Learning Simulator</article-title>
        <publisher-name>https://www.mladdict.com/q-learning-simulator</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-SuttonU003A2018">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Sutton</surname><given-names>Richard S</given-names></name>
          <name><surname>Barto</surname><given-names>Andrew G</given-names></name>
        </person-group>
        <source>Reinforcement learning: An introduction</source>
        <publisher-name>MIT press</publisher-name>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-DayanU003A2008">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dayan</surname><given-names>Peter</given-names></name>
          <name><surname>Niv</surname><given-names>Yael</given-names></name>
        </person-group>
        <article-title>Reinforcement learning: The good, the bad and the ugly</article-title>
        <source>Current Opinion in Neurobiology</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2008">2008</year>
        <volume>18</volume>
        <issue>2</issue>
      </element-citation>
    </ref>
    <ref id="ref-NivU003A2009">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Niv</surname><given-names>Yael</given-names></name>
        </person-group>
        <article-title>Reinforcement learning in the brain</article-title>
        <source>Journal of Mathematical Psychology</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>53</volume>
        <issue>3</issue>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
