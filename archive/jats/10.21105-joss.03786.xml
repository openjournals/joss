<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3786</article-id>
<article-id pub-id-type="doi">10.21105/joss.03786</article-id>
<title-group>
<article-title>robustHD: An R package for robust regression with
high-dimensional data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-2513-3788</contrib-id>
<string-name>Andreas Alfons</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Erasmus School of Economics, Erasmus University Rotterdam,
Netherlands</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-09-29">
<day>29</day>
<month>9</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>67</issue>
<fpage>3786</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>high-dimensional data</kwd>
<kwd>outliers</kwd>
<kwd>regression</kwd>
<kwd>variable selection</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In regression analysis with high-dimensional data, variable
  selection is an important step to (i) overcome computational problems,
  (ii) improve prediction performance by variance reduction, and (iii)
  increase interpretability of the resulting models due to the smaller
  number of variables. However, robust methods are necessary to prevent
  outlying data points from distorting the results. The add-on package
  <monospace>robustHD</monospace>
  (<xref alt="Alfons, 2021" rid="ref-robustHD" ref-type="bibr">Alfons,
  2021</xref>) for the statistical computing environment R
  (<xref alt="R Core Team, 2021" rid="ref-RCore" ref-type="bibr">R Core
  Team, 2021</xref>) provides functionality for robust linear regression
  and model selection with high-dimensional data. More specifically, the
  implemented functionality includes robust least angle regression
  (<xref alt="Khan et al., 2007" rid="ref-khan07b" ref-type="bibr">Khan
  et al., 2007</xref>), robust groupwise least angle regression
  (<xref alt="Alfons et al., 2016" rid="ref-alfons16a" ref-type="bibr">Alfons
  et al., 2016</xref>), as well as sparse least trimmed squares
  regression
  (<xref alt="Alfons et al., 2013" rid="ref-alfons13b" ref-type="bibr">Alfons
  et al., 2013</xref>). The latter can be seen as a trimmed version of
  the popular lasso regression estimator
  (<xref alt="Tibshirani, 1996" rid="ref-tibshirani96" ref-type="bibr">Tibshirani,
  1996</xref>). Selecting the optimal model can be done via
  cross-validation or an information criterion, and various plots are
  available to illustrate model selection and to evaluate the final
  model estimates. Furthermore, the package includes functionality for
  pre-processing such as robust standardization and winsorization.
  Finally, <monospace>robustHD</monospace> follows a clear
  object-oriented design and takes advantage of C++ code and parallel
  computing to reduce computing time.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>While the authors of Khan et al.
  (<xref alt="2007" rid="ref-khan07b" ref-type="bibr">2007</xref>) did
  provide an R script for robust least angle regression (although the
  link in their paper appears to be broken), the implementation in
  <monospace>robustHD</monospace> utilizes C++ code for computational
  efficiency and provides the convenience of an R package. Moreover,
  code for robust groupwise least angle regression and sparse least
  trimmed squares regression is not available elsewhere. Package
  <monospace>robustHD</monospace> therefore provides researchers with
  access to several popular methods for robust regression and variable
  selection with high-dimensional data. It has been used in many
  benchmarking studies in the statistical literature (e.g.,
  <xref alt="Chang et al., 2018" rid="ref-chang18" ref-type="bibr">Chang
  et al., 2018</xref>;
  <xref alt="Cohen Freue et al., 2019" rid="ref-cohenfreue19" ref-type="bibr">Cohen
  Freue et al., 2019</xref>;
  <xref alt="Kurnaz et al., 2018b" rid="ref-kurnaz18" ref-type="bibr">Kurnaz
  et al., 2018b</xref>;
  <xref alt="Smucler &amp; Yohai, 2017" rid="ref-smucler17" ref-type="bibr">Smucler
  &amp; Yohai, 2017</xref>), as well as in empirical research (e.g.,
  <xref alt="Antczak-Orlewska et al., 2021" rid="ref-antczakorlewska21" ref-type="bibr">Antczak-Orlewska
  et al., 2021</xref>;
  <xref alt="Stadlbauer et al., 2020" rid="ref-stadlbauer20" ref-type="bibr">Stadlbauer
  et al., 2020</xref>).</p>
</sec>
<sec id="example-robust-groupwise-least-angle-regression">
  <title>Example: Robust groupwise least angle regression</title>
  <p>Robust least angle regression
  (<xref alt="Khan et al., 2007" rid="ref-khan07b" ref-type="bibr">Khan
  et al., 2007</xref>) and robust groupwise least angle regression
  (<xref alt="Alfons et al., 2016" rid="ref-alfons16a" ref-type="bibr">Alfons
  et al., 2016</xref>) follow a hybrid model selection strategy: first
  obtain a sequence of important candidate predictors, then fit
  submodels along that sequence via robust regressions. Here, data on
  cars featured in the popular television show <italic>Top Gear</italic>
  are used to illustrate this functionality.</p>
  <p>The response variable is fuel consumption in miles per gallon
  (MPG), with all remaining variables used as candidate predictors.
  Information on the car model is first removed from the data set, and
  the car price is log-transformed. In addition, only observations with
  complete information are used in this illustrative example.</p>
  <preformat># load package and data
library(&quot;robustHD&quot;)
data(&quot;TopGear&quot;)

# keep complete observations and remove information on car model
keep &lt;- complete.cases(TopGear)
TopGear &lt;- TopGear[keep, -(1:3)]
# log-transform price
TopGear$Price &lt;- log(TopGear$Price)</preformat>
  <p>As the <italic>Top Gear</italic> data set contains several
  categorical variables, robust groupwise least angle regression is
  used. Through the formula interface, function
  <monospace>rgrplars()</monospace> by default takes each categorical
  variable (<monospace>factor</monospace>) as a group of dummy variables
  while all remaining variables are taken individually. However, the
  group assignment can be defined by the user through argument
  <monospace>assign</monospace>. The maximum number of candidate
  predictor groups to be sequenced is determined by argument
  <monospace>sMax</monospace>. Furthermore, with
  <monospace>crit = &quot;BIC&quot;</monospace>, the optimal submodel
  along the sequence is selected via the Bayesian information criterion
  (BIC). Note that each submodel along the sequence is fitted using a
  robust regression estimator with a non-deterministic algorithm, hence
  the seed of the random number generator is supplied for
  reproducibility.</p>
  <preformat># fit robust groupwise least angle regression and print results
fit &lt;- rgrplars(MPG ~ ., data = TopGear, sMax = 15, 
                crit = &quot;BIC&quot;, seed = 20210507)
fit</preformat>
  <preformat>## Call:
## rgrplars(formula = MPG ~ ., data = TopGear, sMax = 15, crit = &quot;BIC&quot;, 
##     seed = 20210507)
## 
## Sequence of moves:
##       1 2 3 4  5 6  7  8 9 10 11 12 13 14 15
## Group 6 4 8 1 10 5 12 13 9 18 27 16 28 17 26
## 
## Coefficients of optimal submodel:
##     (Intercept)      FuelPetrol    Displacement DriveWheelFront 
##   149.512783142   -12.905795146    -0.003968404     5.374692058 
##  DriveWheelRear             BHP    Acceleration        TopSpeed 
##     0.612416948     0.015265327     0.530554736    -0.191667161 
##          Weight           Width          Height 
##    -0.001817037    -0.013641306    -0.029560052 
## 
## Optimal step: 9</preformat>
  <p>The output prints information on the sequence of predictor groups,
  as well as the results of the final model fit. Here, 9 predictor
  groups consisting of 10 individual covariates are selected into the
  final model.</p>
  <p>Several plots are available for the results:
  <monospace>coefPlot()</monospace> visualizes the coefficient path
  along the sequence of submodels, <monospace>critPlot()</monospace>
  plots the values of the optimality criterion against the step along
  the sequence, and <monospace>diagnosticPlot()</monospace> allows to
  produce various diagnostic plots for the final model fit. Examples of
  these plots are shown in
  <xref alt="Figure 1" rid="figU003Argrplars">Figure 1</xref>.</p>
  <fig>
    <caption><p>Examples of the coefficient plot
    (<italic>left</italic>), the optimality criterion plot
    (<italic>center</italic>), and the regression diagnostic plot
    (<italic>right</italic>) for output of function
    <monospace>rgrplars()</monospace> in package
    <monospace>robustHD</monospace>.<styled-content id="figU003Argrplars"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="figure_rgrplars.pdf" xlink:title="" />
  </fig>
</sec>
<sec id="example-sparse-least-trimmed-squares-regression">
  <title>Example: Sparse least trimmed squares regression</title>
  <p>The well-known NCI-60 cancer cell panel
  (<xref alt="Reinhold et al., 2012" rid="ref-reinhold12" ref-type="bibr">Reinhold
  et al., 2012</xref>) is used to illustrate the functionality for
  sparse least trimmed squares regression. The protein expressions for a
  specific protein are selected as the response variable, and the gene
  expressions of the 100 genes that have the highest (robustly
  estimated) correlations with the response variable are screened as
  candidate predictors.</p>
  <preformat># load package and data
library(&quot;robustHD&quot;)
data(&quot;nci60&quot;)  # contains matrices 'protein' and 'gene'

# define response variable
y &lt;- protein[, 92]
# screen most correlated predictor variables
correlations &lt;- apply(gene, 2, corHuber, y)
keep &lt;- partialOrder(abs(correlations), 100, decreasing = TRUE)
X &lt;- gene[, keep]</preformat>
  <p>Sparse least trimmed squares is a regularized estimator of the
  linear regression model, whose results depend on a non-negative
  regularization parameter (see
  <xref alt="Alfons et al., 2013" rid="ref-alfons13b" ref-type="bibr">Alfons
  et al., 2013</xref>). In general, a larger value of this
  regularization parameter yields more regression coefficients being set
  to zero, which can be seen as a form of variable selection.</p>
  <p>For convenience, <monospace>sparseLTS()</monospace> can internally
  estimate the smallest value of the regularization parameter that sets
  all coefficients to zero. With
  <monospace>mode = &quot;fraction&quot;</monospace>, the values
  supplied via the argument <monospace>lambda</monospace> are then taken
  as fractions of this estimated value (i.e., they are multiplied with
  the internally estimated value). In this example, the optimal value of
  the the regularization parameter is selected by estimating the
  prediction error (<monospace>crit = &quot;PE&quot;</monospace>) via
  5-fold cross-validation with one replication
  (<monospace>splits = foldControl(K = 5, R = 1)</monospace>). The
  default prediction loss function is the root trimmed mean squared
  prediction error. Finally, the seed of the random number generator is
  supplied for reproducibility.</p>
  <preformat># fit sparse least trimmed squares regression and print results
lambda &lt;- seq(0.01, 0.5, length.out = 10)
fit &lt;- sparseLTS(X, y, lambda = lambda, mode = &quot;fraction&quot;, crit = &quot;PE&quot;,
                 splits = foldControl(K = 5, R = 1), seed = 20210507)
fit</preformat>
  <preformat>## Final model:
## 
## Call:
## sparseLTS(x = X, y = y, lambda = 0.0448151412422958)
## 
## Coefficients:
##  (Intercept)         8502        21786          134         4454 
## -3.709498642  0.593549132  0.033366829  0.115955965  0.015899659 
##         1106        20125         8510        14785        17400 
##  0.020447909 -0.091451556  0.111369625 -0.014556471  0.002262256 
##         8460         8120        18447        15622         7696 
## -0.003669024  0.112165149 -0.229292900 -0.008785651  0.020915212 
##         5550        16784        13547 
##  0.005880150  0.015398316  0.037262275 
##                                    
## Penalty parameter:       0.04481514
## Residual scale estimate: 0.62751742</preformat>
  <p>Among other information (which is omitted above), the output prints
  the results of the final model fit, which here consists of 17 genes
  with non-zero coefficients. Similar plots as in
  <xref alt="Figure 1" rid="figU003Argrplars">Figure 1</xref> are
  available to visualize the results.</p>
</sec>
<sec id="related-software">
  <title>Related software</title>
  <p>Package <monospace>enetLTS</monospace>
  (<xref alt="Kurnaz et al., 2018a" rid="ref-enetLTS" ref-type="bibr">Kurnaz
  et al., 2018a</xref>) provides robust elastic-net-regularized
  estimators based on trimming for the linear and logistic regression
  models. Package <monospace>pense</monospace>
  (<xref alt="Kepplinger et al., 2021" rid="ref-pense" ref-type="bibr">Kepplinger
  et al., 2021</xref>) contains implementations of robust S- and MM-type
  estimators with elastic net regularization for linear regression.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Andreas Alfons is partially supported by a grant of the Dutch
  Research Council (NWO), research program Vidi (project number
  VI.Vidi.195.141).</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-robustHD">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Alfons</surname><given-names>A.</given-names></name>
        </person-group>
        <source>robustHD: Robust methods for high-dimensional data</source>
        <year iso-8601-date="2021">2021</year>
        <uri>https://github.com/aalfons/robustHD/</uri>
      </element-citation>
    </ref>
    <ref id="ref-RCore">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <string-name>R Core Team</string-name>
        </person-group>
        <source>R: A language and environment for statistical computing</source>
        <publisher-name>R Foundation for Statistical Computing</publisher-name>
        <publisher-loc>Vienna, Austria</publisher-loc>
        <year iso-8601-date="2021">2021</year>
        <uri>https://www.R-project.org/</uri>
      </element-citation>
    </ref>
    <ref id="ref-khan07b">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Khan</surname><given-names>J. A.</given-names></name>
          <name><surname>Van Aelst</surname><given-names>S.</given-names></name>
          <name><surname>Zamar</surname><given-names>R. H.</given-names></name>
        </person-group>
        <article-title>Robust linear model selection based on least angle regression</article-title>
        <source>Journal of the American Statistical Association</source>
        <year iso-8601-date="2007">2007</year>
        <volume>102</volume>
        <issue>480</issue>
        <pub-id pub-id-type="doi">10.1198/016214507000000950</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-alfons13b">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Alfons</surname><given-names>A.</given-names></name>
          <name><surname>Croux</surname><given-names>C.</given-names></name>
          <name><surname>Gelper</surname><given-names>S.</given-names></name>
        </person-group>
        <article-title>Sparse least trimmed squares regression for analyzing high-dimensional large data sets</article-title>
        <source>The Annals of Applied Statistics</source>
        <year iso-8601-date="2013">2013</year>
        <volume>7</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1214/12-AOAS575</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-alfons16a">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Alfons</surname><given-names>A.</given-names></name>
          <name><surname>Croux</surname><given-names>C.</given-names></name>
          <name><surname>Gelper</surname><given-names>S.</given-names></name>
        </person-group>
        <article-title>Robust groupwise least angle regression</article-title>
        <source>Computational Statistics &amp; Data Analysis</source>
        <year iso-8601-date="2016">2016</year>
        <volume>93</volume>
        <pub-id pub-id-type="doi">10.1016/j.csda.2015.02.007</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-tibshirani96">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tibshirani</surname><given-names>R.</given-names></name>
        </person-group>
        <article-title>Regression shrinkage and selection via the lasso</article-title>
        <source>Journal of the Royal Statistical Society, Series B</source>
        <year iso-8601-date="1996">1996</year>
        <volume>58</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-smucler17">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Smucler</surname><given-names>E.</given-names></name>
          <name><surname>Yohai</surname><given-names>V. J.</given-names></name>
        </person-group>
        <article-title>Robust and sparse estimators for linear regression models</article-title>
        <source>Computational Statistics &amp; Data Analysis</source>
        <year iso-8601-date="2017">2017</year>
        <volume>111</volume>
        <pub-id pub-id-type="doi">10.1016/j.csda.2017.02.002</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kurnaz18">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kurnaz</surname><given-names>F. S.</given-names></name>
          <name><surname>Hoffmann</surname><given-names>I.</given-names></name>
          <name><surname>Filzmoser</surname><given-names>P.</given-names></name>
        </person-group>
        <article-title>Robust and sparse estimation methods for high-dimensional linear and logistic regression</article-title>
        <source>Chemometrics and Intelligent Laboratory Systems</source>
        <year iso-8601-date="2018">2018</year>
        <volume>172</volume>
        <pub-id pub-id-type="doi">10.1016/j.chemolab.2017.11.017</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-chang18">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Chang</surname><given-names>L.</given-names></name>
          <name><surname>Roberts</surname><given-names>S.</given-names></name>
          <name><surname>Welsh</surname><given-names>A.</given-names></name>
        </person-group>
        <article-title>Robust lasso regression using Tukey’s biweight criterion</article-title>
        <source>Technometrics</source>
        <year iso-8601-date="2018">2018</year>
        <volume>60</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1080/00401706.2017.1305299</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-cohenfreue19">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Cohen Freue</surname><given-names>G. V.</given-names></name>
          <name><surname>Kepplinger</surname><given-names>D.</given-names></name>
          <name><surname>Salibian-Barrera</surname><given-names>M.</given-names></name>
          <name><surname>Smucler</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Robust elastic net estimators for variable selection and identification of proteomic biomarkers</article-title>
        <source>The Annals of Applied Statistics</source>
        <year iso-8601-date="2019">2019</year>
        <volume>13</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1214/19-AOAS1269</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-stadlbauer20">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Stadlbauer</surname><given-names>V.</given-names></name>
          <name><surname>Engertsberger</surname><given-names>L.</given-names></name>
          <name><surname>Komarova</surname><given-names>I.</given-names></name>
          <name><surname>Feldbacher</surname><given-names>N.</given-names></name>
          <name><surname>Leber</surname><given-names>B.</given-names></name>
          <name><surname>Pichler</surname><given-names>G.</given-names></name>
          <name><surname>Fink</surname><given-names>N.</given-names></name>
          <name><surname>Scarpatetti</surname><given-names>N.</given-names></name>
          <name><surname>Schippinger</surname><given-names>W.</given-names></name>
          <name><surname>Schmidt</surname><given-names>R.</given-names></name>
          <name><surname>Horvath</surname><given-names>A.</given-names></name>
        </person-group>
        <article-title>Dysbiosis, gut barrier dysfunction and inflammation in dementia: A pilot study</article-title>
        <source>BMC Geriatrics</source>
        <year iso-8601-date="2020">2020</year>
        <volume>20</volume>
        <issue>248</issue>
        <pub-id pub-id-type="doi">10.1186/s12877-020-01644-2</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-antczakorlewska21">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Antczak-Orlewska</surname><given-names>O.</given-names></name>
          <name><surname>Płóciennik</surname><given-names>M.</given-names></name>
          <name><surname>Sobczyk</surname><given-names>R.</given-names></name>
          <name><surname>Okupny</surname><given-names>D.</given-names></name>
          <name><surname>Stachowicz-Rybka</surname><given-names>R.</given-names></name>
          <name><surname>Rzodkiewicz</surname><given-names>M.</given-names></name>
          <name><surname>Siciński</surname><given-names>J.</given-names></name>
          <name><surname>Mroczkowska</surname><given-names>A.</given-names></name>
          <name><surname>Krąpiec</surname><given-names>M.</given-names></name>
          <name><surname>Słowiński</surname><given-names>M.</given-names></name>
          <name><surname>Kittel</surname><given-names>P.</given-names></name>
        </person-group>
        <article-title>Chironomidae morphological types and functional feeding groups as a habitat complexity vestige</article-title>
        <source>Frontiers in Ecology and Evolution</source>
        <year iso-8601-date="2021">2021</year>
        <volume>8</volume>
        <issue>583831</issue>
        <pub-id pub-id-type="doi">10.3389/fevo.2020.583831</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-reinhold12">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Reinhold</surname><given-names>W. C.</given-names></name>
          <name><surname>Sunshine</surname><given-names>M.</given-names></name>
          <name><surname>Liu</surname><given-names>H.</given-names></name>
          <name><surname>Varma</surname><given-names>S.</given-names></name>
          <name><surname>Kohn</surname><given-names>K. W.</given-names></name>
          <name><surname>Morris</surname><given-names>J.</given-names></name>
          <name><surname>Doroshow</surname><given-names>J.</given-names></name>
          <name><surname>Pommier</surname><given-names>Y.</given-names></name>
        </person-group>
        <article-title>CellMiner: A web-based suite of genomic and pharmacologic tools to explore transcript and drug patterns in the NCI-60 cell line set</article-title>
        <source>Cancer Research</source>
        <year iso-8601-date="2012">2012</year>
        <volume>72</volume>
        <issue>14</issue>
        <pub-id pub-id-type="doi">10.1158/0008-5472.can-12-1370</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-enetLTS">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Kurnaz</surname><given-names>F. S.</given-names></name>
          <name><surname>Hoffmann</surname><given-names>I.</given-names></name>
          <name><surname>Filzmoser</surname><given-names>P.</given-names></name>
        </person-group>
        <source>enetLTS: Robust and sparse methods for high dimensional linear and logistic regression</source>
        <year iso-8601-date="2018">2018</year>
        <uri>https://CRAN.R-project.org/package=enetLTS</uri>
      </element-citation>
    </ref>
    <ref id="ref-pense">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Kepplinger</surname><given-names>D.</given-names></name>
          <name><surname>Salibián-Barrera</surname><given-names>M.</given-names></name>
          <name><surname>Cohen Freue</surname><given-names>G.</given-names></name>
        </person-group>
        <source>pense: Penalized elastic net S/MM-estimator of regression</source>
        <year iso-8601-date="2021">2021</year>
        <uri>https://CRAN.R-project.org/package=pense</uri>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
