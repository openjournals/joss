<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1525</article-id>
<article-id pub-id-type="doi">10.21105/joss.01525</article-id>
<title-group>
<article-title>multimatch-gaze: The MultiMatch algorithm for gaze path
comparison in Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-2917-3450</contrib-id>
<string-name>Adina S. Wagner</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-3456-2493</contrib-id>
<string-name>Yaroslav O. Halchenko</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6398-6370</contrib-id>
<string-name>Michael Hanke</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Institute of Neuroscience and Medicine, Brain &amp;
Behaviour (INM-7), Research Centre Jülich, Jülich, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Institute of Systems Neuroscience, Medical Faculty,
Heinrich Heine University Düsseldorf, Düsseldorf, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Psychological and Brain Sciences, Dartmouth
College, Dartmouth, NH, United States</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-05-17">
<day>17</day>
<month>5</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>40</issue>
<fpage>1525</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>eyetracking</kwd>
<kwd>scan path</kwd>
<kwd>fixation</kwd>
<kwd>saccade</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>multimatch-gaze</monospace> is a Python package for
  computing the similarity of eye-movement sequences, so called scan
  paths. Scan paths are the trace of eye-movements in space and time,
  usually captured with eye tracking devices. Scan path similarity is a
  measure that is used in a variety of disciplines ranging from
  cognitive psychology, medicine, and marketing to human-machine
  interfaces. In addition to quantifying position and order of a series
  of eye-movements, comparing their temporo-spatial sequence adds an
  insightful dimension to the traditional analysis of eye tracking data.
  It reveals commonalities and differences of viewing behavior within
  and between observers, and is used to study how people explore visual
  information. For example, scan path comparisons are used to study
  analogy-making
  (<xref alt="French et al., 2017" rid="ref-french" ref-type="bibr">French
  et al., 2017</xref>), visual exploration and imagery
  (<xref alt="Johansson et al., 2006" rid="ref-Johansson" ref-type="bibr">Johansson
  et al., 2006</xref>), habituation in repetitive visual search
  (<xref alt="Burmester &amp; Mast, 2010" rid="ref-burmester" ref-type="bibr">Burmester
  &amp; Mast, 2010</xref>), or spatial attention allocation in dynamic
  scenes
  (<xref alt="Mital et al., 2011" rid="ref-mital" ref-type="bibr">Mital
  et al., 2011</xref>). The method is applied within individuals as a
  measure of change
  (<xref alt="Burmester &amp; Mast, 2010" rid="ref-burmester" ref-type="bibr">Burmester
  &amp; Mast, 2010</xref>), or across samples to study group differences
  (<xref alt="French et al., 2017" rid="ref-french" ref-type="bibr">French
  et al., 2017</xref>).</p>
  <p>Therefore, in recent years, interest in the study of eye movement
  sequences has sparked the development of novel methodologies and
  algorithms to perform scan path comparisons. However, many of the
  contemporary scan path comparison algorithms are implemented in
  closed-source, non-free software such as Matlab.</p>
  <p><monospace>multimatch-gaze</monospace> is a Python-based
  reimplementation of the MultiMatch toolbox for scan path comparison,
  originally developed by Jarodzka et al.
  (<xref alt="2010" rid="ref-Jarodzka" ref-type="bibr">2010</xref>) and
  implemented by Dewhurst et al.
  (<xref alt="2012" rid="ref-Dewhurst" ref-type="bibr">2012</xref>) in
  Matlab. This algorithm represents scan paths as geometrical vectors in
  a two-dimensional space: Any scan path is built up of a coordinate
  vector sequence in which the start and end position of vectors
  represent fixations, and the vectors represent saccades. Two such
  vector sequences are, after optional simplification based on angular
  relations and amplitudes of saccades, compared on the five dimensions
  “vector shape”, “vector length (amplitude)”, “vector position”,
  “vector direction”, and “fixation duration” for a multidimensional
  similarity evaluation.</p>
  <p>This reimplementation in Python aims at providing an accessible,
  documented, and tested open source alternative to the existing
  MultiMatch toolbox. The algorithm is an established tool for scan path
  comparison
  (<xref alt="Anderson et al., 2015" rid="ref-anderson" ref-type="bibr">Anderson
  et al., 2015</xref>), and improved availability aids adoption in a
  broader research community. <monospace>multimatch-gaze</monospace> is
  available from its Github repository and as the Python package
  <monospace>multimatch-gaze</monospace> via
  <monospace>pip install multimatch-gaze</monospace>. The module
  contains the same functionality as the original Matlab toolbox, that
  is, scan path comparison with optional simplification according to
  user-defined thresholds, and it provides this functionality via a
  command line interface or a Python API.</p>
  <p>Data for scan path comparison can be supplied as nx3 fixation
  vectors with columns corresponding to x-coordinates, y-coordinates,
  and duration of the fixation in seconds (as for the original Matlab
  toolbox). Alternatively, <monospace>multimatch-gaze</monospace> can
  natively read in event detection output produced by REMoDNaV
  (<xref alt="Dar et al., 2019" rid="ref-remodnav" ref-type="bibr">Dar
  et al., 2019</xref>), a velocity-based eye movement classification
  algorithm written in Python. For REMoDNaV-based input, users can
  additionally specify whether smooth pursuit events in the data should
  be kept in the scan path or discarded.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We thank Dr. Richard Dewhurst for kindly and swiftly providing the
  original Matlab code for the MultiMatch toolbox via e-mail and being
  supportive of an open source implementation.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Jarodzka">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Jarodzka</surname><given-names>Halszka</given-names></name>
          <name><surname>Holmqvist</surname><given-names>Kenneth</given-names></name>
          <name><surname>Nyström</surname><given-names>Marcus</given-names></name>
        </person-group>
        <article-title>A vector-based, multidimensional scanpath similarity measure</article-title>
        <source>Proceedings of the 2010 symposium on eye-tracking research &amp; applications</source>
        <publisher-name>ACM</publisher-name>
        <year iso-8601-date="2010">2010</year>
        <uri>http://doi.acm.org/10.1145/1743666.1743718</uri>
        <pub-id pub-id-type="doi">10.1145/1743666.1743718</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Dewhurst">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dewhurst</surname><given-names>Richard</given-names></name>
          <name><surname>Nyström</surname><given-names>Marcus</given-names></name>
          <name><surname>Jarodzka</surname><given-names>Halszka</given-names></name>
          <name><surname>Foulsham</surname><given-names>Tom</given-names></name>
          <name><surname>Johansson</surname><given-names>Roger</given-names></name>
          <name><surname>Holmqvist</surname><given-names>Kenneth</given-names></name>
        </person-group>
        <article-title>It depends on how you look at it: Scanpath comparison in multiple dimensions with MultiMatch, a vector-based approach</article-title>
        <source>Behavior research methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <volume>44</volume>
        <issue>4</issue>
        <uri>https://doi.org/10.3758/s13428-012-0212-2</uri>
        <pub-id pub-id-type="doi">10.3758/s13428-012-0212-2</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Johansson">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Johansson</surname><given-names>Roger</given-names></name>
          <name><surname>Holsanova</surname><given-names>Jana</given-names></name>
          <name><surname>Holmqvist</surname><given-names>Kenneth</given-names></name>
        </person-group>
        <article-title>Pictures and spoken descriptions elicit similar eye movements during mental imagery, both in light and in complete darkness</article-title>
        <source>Cognitive Science</source>
        <year iso-8601-date="2006">2006</year>
        <uri>https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_86</uri>
        <pub-id pub-id-type="doi">10.1207/s15516709cog0000_86</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-mital">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mital</surname><given-names>Parag K</given-names></name>
          <name><surname>Smith</surname><given-names>Tim J</given-names></name>
          <name><surname>Hill</surname><given-names>Robin L</given-names></name>
          <name><surname>Henderson</surname><given-names>John M</given-names></name>
        </person-group>
        <article-title>Clustering of gaze during dynamic scene viewing is predicted by motion</article-title>
        <source>Cognitive Computation</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <volume>3</volume>
        <issue>1</issue>
        <uri>https://doi.org/10.1007/s12559-010-9074-z</uri>
        <pub-id pub-id-type="doi">10.1007/s12559-010-9074-z</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-french">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>French</surname><given-names>Robert M</given-names></name>
          <name><surname>Glady</surname><given-names>Yannick</given-names></name>
          <name><surname>Thibaut</surname><given-names>Jean-Pierre</given-names></name>
        </person-group>
        <article-title>An evaluation of scanpath-comparison and machine-learning classification algorithms used to study the dynamics of analogy making</article-title>
        <source>Behavior Research Methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>49</volume>
        <issue>4</issue>
        <uri>https://doi.org/10.3758/s13428-016-0788-z</uri>
        <pub-id pub-id-type="doi">10.3758/s13428-016-0788-z</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-burmester">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Burmester</surname><given-names>Michael</given-names></name>
          <name><surname>Mast</surname><given-names>Marcus</given-names></name>
        </person-group>
        <article-title>Repeated web page visits and the scanpath theory: A recurrent pattern detection approach</article-title>
        <source>Journal of Eye Movement Research</source>
        <year iso-8601-date="2010">2010</year>
        <volume>3</volume>
        <issue>4</issue>
        <uri>https://doi.org/10.16910/jemr.3.4.5</uri>
        <pub-id pub-id-type="doi">10.16910/jemr.3.4.5</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-anderson">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Anderson</surname><given-names>Nicola C</given-names></name>
          <name><surname>Anderson</surname><given-names>Fraser</given-names></name>
          <name><surname>Kingstone</surname><given-names>Alan</given-names></name>
          <name><surname>Bischof</surname><given-names>Walter F</given-names></name>
        </person-group>
        <article-title>A comparison of scanpath comparison methods</article-title>
        <source>Behavior Research Methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>47</volume>
        <issue>4</issue>
        <uri>https://doi.org/10.3758/s13428-014-0550-3</uri>
        <pub-id pub-id-type="doi">10.3758/s13428-014-0550-3</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-remodnav">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dar</surname><given-names>Asim H.</given-names></name>
          <name><surname>Wagner</surname><given-names>Adina S.</given-names></name>
          <name><surname>Hanke</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>REMoDNaV: Robust eye movement detection for natural viewing</article-title>
        <source>bioRxiv</source>
        <year iso-8601-date="2019">2019</year>
        <uri>https://www.biorxiv.org/content/early/2019/04/26/619254</uri>
        <pub-id pub-id-type="doi">10.1101/619254</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
