<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">749</article-id>
<article-id pub-id-type="doi">10.21105/joss.00749</article-id>
<title-group>
<article-title>SpeechPy - A Library for Speech Processing and
Recognition</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-2282-4361</contrib-id>
<string-name>Amirsina Torfi</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Virginia Tech, Department of Computer Science</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2018-05-15">
<day>15</day>
<month>5</month>
<year>2018</year>
</pub-date>
<volume>3</volume>
<issue>27</issue>
<fpage>749</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
</article-meta>
</front>
<body>
<sec id="abstract">
  <title>Abstract</title>
  <p>SpeechPy is an open source Python package that contains speech
  preprocessing techniques, speech features, and important
  post-processing operations. It provides most frequent used speech
  features including MFCCs and filterbank energies alongside with the
  log-energy of filter-banks. The aim of the package is to provide
  researchers with a simple tool for speech feature extraction and
  processing purposes in applications such as Automatic Speech
  Recognition and Speaker Verification.</p>
</sec>
<sec id="introduction">
  <title>Introduction</title>
  <p>Automatic Speech Recognition (ASR) requires three main components
  for further analysis: Preprocessing, feature extraction, and
  post-processing. Feature extraction, in an abstract meaning, is
  extracting descriptive features from raw signal for speech
  classification purposes. Due to the high dimensionality, the raw
  signal can be less informative compared to extracted higher level
  features. Feature extraction comes to our rescue for turning the high
  dimensional signal to a lower dimensional and yet a more informative
  version of that for sound recognition and
  classification (<xref alt="Furui, 1986" rid="ref-furui1986speaker" ref-type="bibr">Furui,
  1986</xref>;
  <xref alt="Guyon et al., 2008" rid="ref-guyon2008feature" ref-type="bibr">Guyon
  et al., 2008</xref>;
  <xref alt="Hirsch &amp; Pearce, 2000" rid="ref-hirsch2000aurora" ref-type="bibr">Hirsch
  &amp; Pearce, 2000</xref>).</p>
  <fig>
    <caption><p>Scheme of speech recognition</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="test/_imgs/Scheme_of_speech_recognition_system.png" xlink:title="" />
  </fig>
  <p>Feature extraction, in essence, should be done considering the
  specific application at hand. For example, in ASR applications, the
  linguistic characteristics of the raw signal are of great importance
  and the other characteristics must be
  ignored (<xref alt="Rabiner &amp; Juang, 1993" rid="ref-rabiner1993fundamentals" ref-type="bibr">Rabiner
  &amp; Juang, 1993</xref>;
  <xref alt="Yu &amp; Deng, 2016" rid="ref-yu2016automatic" ref-type="bibr">Yu
  &amp; Deng, 2016</xref>). On the other hand, in Speaker
  Recognition (SR) task, solely voice-associated information must be
  contained in the extracted
  feature (<xref alt="Campbell, 1997" rid="ref-campbell1997speaker" ref-type="bibr">Campbell,
  1997</xref>). So the feature extraction goal is to extract the
  relevant feature from the raw signal and map it to a lower dimensional
  feature space. The problem of feature extraction has been investigated
  in pattern classification aimed at preventing the curse of
  dimensionality. There are some feature extraction approaches based on
  information theory
  (<xref alt="Shannon, 2001" rid="ref-shannon2001mathematical" ref-type="bibr">Shannon,
  2001</xref>;
  <xref alt="Amisina Torfi et al., 2017" rid="ref-torfi2017construction" ref-type="bibr">Amisina
  Torfi et al., 2017</xref>) applied to multimodal signals and
  demonstrated promising
  results (<xref alt="Gurban &amp; Thiran, 2009" rid="ref-gurban2009information" ref-type="bibr">Gurban
  &amp; Thiran, 2009</xref>).</p>
  <p>The speech features can be categorized into two general types of
  acoustic and linguistic features. The former one is mainly related to
  non-verbal sounds and the later one is associated with ASR and SR
  systems for which verbal part has the major role. Perhaps one of the
  most famous linguistic feature which is hard to beat is the
  Mel-Frequency Cepstral Coefficients (MFCC). It uses speech raw frames
  in the range from 20ms to 40ms for having stationary
  characteristics (<xref alt="Rabiner &amp; Juang, 1993" rid="ref-rabiner1993fundamentals" ref-type="bibr">Rabiner
  &amp; Juang, 1993</xref>). MFCC is widely used for both ASR and SR
  tasks and more recently in the associated deep learning applications
  as the input to the network rather than directly feeding the
  signal (<xref alt="Deng et al., 2013" rid="ref-deng2013recent" ref-type="bibr">Deng
  et al., 2013</xref>;
  <xref alt="Lee et al., 2009" rid="ref-lee2009unsupervised" ref-type="bibr">Lee
  et al., 2009</xref>;
  <xref alt="Yu &amp; Seltzer, 2011" rid="ref-yu2011improved" ref-type="bibr">Yu
  &amp; Seltzer, 2011</xref>). With the advent of deep
  learning (<xref alt="LeCun et al., 2015" rid="ref-lecun2015deep" ref-type="bibr">LeCun
  et al., 2015</xref>;
  <xref alt="Amirsina Torfi &amp; Shirvani, 2018" rid="ref-torfi2018attention" ref-type="bibr">Amirsina
  Torfi &amp; Shirvani, 2018</xref>), major improvements have been
  achieved by using deep neural networks rather than traditional methods
  for speech recognition
  applications (<xref alt="Hinton et al., 2012" rid="ref-hinton2012deep" ref-type="bibr">Hinton
  et al., 2012</xref>;
  <xref alt="Liu et al., 2015" rid="ref-liu2015deep" ref-type="bibr">Liu
  et al., 2015</xref>;
  <xref alt="Variani et al., 2014" rid="ref-variani2014deep" ref-type="bibr">Variani
  et al., 2014</xref>).</p>
  <p>With the availability of free software for speech recognition such
  as VOICEBOX, most of these softwares are Matlab-based which limits
  their reproducibility due to commercial issues. Another great package
  is
  PyAudioAnalysis (<xref alt="Giannakopoulos, 2015" rid="ref-giannakopoulos2015pyaudioanalysis" ref-type="bibr">Giannakopoulos,
  2015</xref>), which is a the comprehensive package developed in
  Python. However, the issue with PyAudioAnalysis is that its complexity
  and being too verbose for extracting simple features and it also lacks
  some important preprocessing and post-processing operations for its
  current version.</p>
  <p>Considering the recent advent of deep learning in ASR and SR and
  the importance of the accurate speech feature extraction, here are the
  motivations behind SpeechPy package:</p>
  <list list-type="bullet">
    <list-item>
      <p>Developing a free open source package which covers important
      preprocessing techniques, speech features, and post-processing
      operations required for ASR and SR applications.</p>
    </list-item>
    <list-item>
      <p>A simple package with a minimum degree of complexity should be
      available for beginners.</p>
    </list-item>
    <list-item>
      <p>A well-tested and continuously integrated package for future
      developments should be developed.</p>
    </list-item>
  </list>
  <p>SpeechPy has been developed to satisfy the aforementioned needs. It
  contains the most important preprocessing and post-processing
  operations and a selection of frequently used speech features. The
  package is free and released as an open source software. Continuous
  integration using for instant error check and validity of changes has
  been deployed for SpeechPy. Moreover, prior to the latest official
  release of SpeechPy, the package has successfully been utilized for
  research
  purposes (<xref alt="Amirsina Torfi, Iranmanesh, et al., 2017" rid="ref-torfi20173d" ref-type="bibr">Amirsina
  Torfi, Iranmanesh, et al., 2017</xref>;
  <xref alt="Amirsina Torfi, Nasrabadi, et al., 2017" rid="ref-torfi2017text" ref-type="bibr">Amirsina
  Torfi, Nasrabadi, et al., 2017</xref>).</p>
</sec>
<sec id="package-eco-system">
  <title>Package Eco-system</title>
  <p>SpeechPy has been developed using Python language for its interface
  and backed as well. An empirical study demonstrated that Python as a
  scripting language, is more effective and productive than conventional
  languages for some programming problems and memory consumption is
  often “better than Java and not much worse than C or
  C++” (<xref alt="Prechelt, 2000" rid="ref-prechelt2000empirical" ref-type="bibr">Prechelt,
  2000</xref>). We chose Python due to its simplicity and popularity.
  Third-party libraries are avoided except <italic>Numpy</italic> and
  <italic>Scipy</italic> for handling data and numeric computations.</p>
  <sec id="complexity">
    <title>Complexity</title>
    <p>As the user should not and does not even need to manipulate the
    internal package structure, object-oriented programming is mostly
    used for package development which provides an easier interface for
    the user with a sacrifice to the simplicity of the code. However,
    the internal code complexity of the package does not affect the user
    experience since the modules can easily be called with the
    associated arguments. SpeechPy is a library with a collection of
    sub-modules.</p>
  </sec>
  <sec id="code-style-and-documentation">
    <title>Code Style and Documentation</title>
    <p>SpeechPy is constructed based on PEP 8 style guide for Python
    codes. Moreover, it is extensively documented using the formatted
    docstrings and Sphinx for further automatic modifications to the
    document in case of changing internal modules. The full
    documentation of the project will be generated in HTML and PDF
    format using Sphinx and is hosted online. The official releases of
    the project are hosted on the Zenodo as well
    (<xref alt="Amirsina Torfi, 2017" rid="ref-torfispeechpy" ref-type="bibr">Amirsina
    Torfi, 2017</xref>).</p>
    <fig>
      <caption><p>A general view of the package</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="test/_imgs/packageview.png" xlink:title="" />
    </fig>
  </sec>
  <sec id="continuous-testing-and-extensibility">
    <title>Continuous Testing and Extensibility</title>
    <p>The output of each function has been evaluated as well as using
    different tests as opposed to the other existing standard packages.
    For continuous testing, the code is hosted on GitHub and integrated
    with Travis CI. Each modification to the code must pass the unit
    tests defined for the continuous integration. This will ensure the
    package does not break with unadapted code scripts. However, the
    validity of the modifications should always be investigated with the
    owner or authorized collaborators of the project. The code will be
    tested at each time of modification for Python versions
    <italic>“2.7”</italic>, <italic>“3.4”</italic> and
    <italic>“3.5”</italic>. In the future, these versions are subject to
    change.</p>
    <fig>
      <caption><p>Travic CI web interface after testing SpeechPy against
      a new change</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="test/_imgs/travicCI.png" xlink:title="" />
    </fig>
  </sec>
</sec>
<sec id="availability">
  <title>Availability</title>
  <sec id="operating-system">
    <title>Operating system</title>
    <p>Tested on Ubuntu 14.04 and 16.04 LTS Linux, Apple Mac OS X 10.9.5
    , and Microsoft Windows 7 &amp; 10. We expect that SpeechPy works on
    any distribution as long as Python and the package dependencies are
    installed.</p>
  </sec>
  <sec id="programming-language">
    <title>Programming language</title>
    <p>The package has been tested with Python 2.7, 3.4 and 3.5.
    However, using Python 3.5 is suggested.</p>
  </sec>
  <sec id="additional-system-requirements-dependencies">
    <title>Additional system requirements &amp; dependencies</title>
    <p>SpeechPy is a light package and small computational power would
    be enough for running it. Although the speed of the execution is
    totally dependent on the system architecture. The dependencies are
    as follows:</p>
    <list list-type="bullet">
      <list-item>
        <p>Numpy</p>
      </list-item>
      <list-item>
        <p>SciPy</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="acknowledgement">
  <title>Acknowledgement</title>
  <p>This work has been completed with computational resources provided
  by the West Virginia University and Virginia Tech and is based upon a
  work supported by the Center for Identification Technology
  Research (CITeR) and the National Science Foundation (NSF) under Grant
  #1650474. I would like to thank professor Nasser Nasrabadi for
  supporting me through this project and for his valuable supervision
  regarding my research in speech technology.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-hirsch2000aurora">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Hirsch</surname><given-names>Hans-Günter</given-names></name>
          <name><surname>Pearce</surname><given-names>David</given-names></name>
        </person-group>
        <article-title>The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions</article-title>
        <source>ASR2000-automatic speech recognition: Challenges for the new millenium ISCA tutorial and research workshop (ITRW)</source>
        <year iso-8601-date="2000">2000</year>
      </element-citation>
    </ref>
    <ref id="ref-guyon2008feature">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Guyon</surname><given-names>Isabelle</given-names></name>
          <name><surname>Gunn</surname><given-names>Steve</given-names></name>
          <name><surname>Nikravesh</surname><given-names>Masoud</given-names></name>
          <name><surname>Zadeh</surname><given-names>Lofti A</given-names></name>
        </person-group>
        <source>Feature extraction: Foundations and applications</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2008">2008</year>
        <volume>207</volume>
      </element-citation>
    </ref>
    <ref id="ref-furui1986speaker">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Furui</surname><given-names>Sadaoki</given-names></name>
        </person-group>
        <article-title>Speaker-independent isolated word recognition using dynamic features of speech spectrum</article-title>
        <source>IEEE Transactions on Acoustics, Speech, and Signal Processing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="1986">1986</year>
        <volume>34</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
    <ref id="ref-yu2016automatic">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Yu</surname><given-names>Dong</given-names></name>
          <name><surname>Deng</surname><given-names>Li</given-names></name>
        </person-group>
        <source>AUTOMATIC SPEECH RECOGNITION.</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-rabiner1993fundamentals">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Rabiner</surname><given-names>Lawrence R</given-names></name>
          <name><surname>Juang</surname><given-names>Biing-Hwang</given-names></name>
        </person-group>
        <source>Fundamentals of speech recognition</source>
        <publisher-name>PTR Prentice Hall Englewood Cliffs</publisher-name>
        <year iso-8601-date="1993">1993</year>
        <volume>14</volume>
      </element-citation>
    </ref>
    <ref id="ref-campbell1997speaker">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Campbell</surname><given-names>Joseph P</given-names></name>
        </person-group>
        <article-title>Speaker recognition: A tutorial</article-title>
        <source>Proceedings of the IEEE</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="1997">1997</year>
        <volume>85</volume>
        <issue>9</issue>
      </element-citation>
    </ref>
    <ref id="ref-deng2013recent">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Deng</surname><given-names>Li</given-names></name>
          <name><surname>Li</surname><given-names>Jinyu</given-names></name>
          <name><surname>Huang</surname><given-names>Jui-Ting</given-names></name>
          <name><surname>Yao</surname><given-names>Kaisheng</given-names></name>
          <name><surname>Yu</surname><given-names>Dong</given-names></name>
          <name><surname>Seide</surname><given-names>Frank</given-names></name>
          <name><surname>Seltzer</surname><given-names>Michael</given-names></name>
          <name><surname>Zweig</surname><given-names>Geoff</given-names></name>
          <name><surname>He</surname><given-names>Xiaodong</given-names></name>
          <name><surname>Williams</surname><given-names>Jason</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Recent advances in deep learning for speech research at microsoft</article-title>
        <source>Acoustics, speech and signal processing (ICASSP), 2013 IEEE international conference on</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2013">2013</year>
      </element-citation>
    </ref>
    <ref id="ref-lee2009unsupervised">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Lee</surname><given-names>Honglak</given-names></name>
          <name><surname>Pham</surname><given-names>Peter</given-names></name>
          <name><surname>Largman</surname><given-names>Yan</given-names></name>
          <name><surname>Ng</surname><given-names>Andrew Y</given-names></name>
        </person-group>
        <article-title>Unsupervised feature learning for audio classification using convolutional deep belief networks</article-title>
        <source>Advances in neural information processing systems</source>
        <year iso-8601-date="2009">2009</year>
      </element-citation>
    </ref>
    <ref id="ref-yu2011improved">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Yu</surname><given-names>Dong</given-names></name>
          <name><surname>Seltzer</surname><given-names>Michael L</given-names></name>
        </person-group>
        <article-title>Improved bottleneck features using pretrained deep neural networks</article-title>
        <source>Twelfth annual conference of the international speech communication association</source>
        <year iso-8601-date="2011">2011</year>
      </element-citation>
    </ref>
    <ref id="ref-giannakopoulos2015pyaudioanalysis">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Giannakopoulos</surname><given-names>Theodoros</given-names></name>
        </person-group>
        <article-title>pyAudioAnalysis: An open-source python library for audio signal analysis</article-title>
        <source>PloS one</source>
        <publisher-name>Public Library of Science</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>10</volume>
        <issue>12</issue>
      </element-citation>
    </ref>
    <ref id="ref-torfi2017text">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Torfi</surname><given-names>Amirsina</given-names></name>
          <name><surname>Nasrabadi</surname><given-names>Nasser M</given-names></name>
          <name><surname>Dawson</surname><given-names>Jeremy</given-names></name>
        </person-group>
        <article-title>Text-independent speaker verification using 3d convolutional neural networks</article-title>
        <source>arXiv preprint arXiv:1705.09422</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-torfi20173d">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Torfi</surname><given-names>Amirsina</given-names></name>
          <name><surname>Iranmanesh</surname><given-names>Seyed Mehdi</given-names></name>
          <name><surname>Nasrabadi</surname><given-names>Nasser</given-names></name>
          <name><surname>Dawson</surname><given-names>Jeremy</given-names></name>
        </person-group>
        <article-title>3D convolutional neural networks for cross audio-visual matching recognition</article-title>
        <source>IEEE Access</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>5</volume>
      </element-citation>
    </ref>
    <ref id="ref-prechelt2000empirical">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Prechelt</surname><given-names>Lutz</given-names></name>
        </person-group>
        <article-title>An empirical comparison of c, c++, java, perl, python, rexx and tcl</article-title>
        <source>IEEE Computer</source>
        <year iso-8601-date="2000">2000</year>
        <volume>33</volume>
        <issue>10</issue>
      </element-citation>
    </ref>
    <ref id="ref-torfispeechpy">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Torfi</surname><given-names>Amirsina</given-names></name>
        </person-group>
        <article-title>SpeechPy: Speech recognition and feature extraction</article-title>
        <year iso-8601-date="2017-08">2017</year><month>08</month>
        <uri>https://doi.org/10.5281/zenodo.810391</uri>
        <pub-id pub-id-type="doi">10.5281/zenodo.810391</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-torfi2017construction">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Torfi</surname><given-names>Amisina</given-names></name>
          <name><surname>Soleymani</surname><given-names>Sobhan</given-names></name>
          <name><surname>Vakili</surname><given-names>Vahid Tabataba</given-names></name>
        </person-group>
        <article-title>On the construction of polar codes for achieving the capacity of marginal channels</article-title>
        <source>arXiv preprint arXiv:1707.04512</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-shannon2001mathematical">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shannon</surname><given-names>Claude Elwood</given-names></name>
        </person-group>
        <article-title>A mathematical theory of communication</article-title>
        <source>ACM SIGMOBILE Mobile Computing and Communications Review</source>
        <publisher-name>ACM</publisher-name>
        <year iso-8601-date="2001">2001</year>
        <volume>5</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
    <ref id="ref-gurban2009information">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gurban</surname><given-names>Mihai</given-names></name>
          <name><surname>Thiran</surname><given-names>Jean-Philippe</given-names></name>
        </person-group>
        <article-title>Information theoretic feature extraction for audio-visual speech recognition</article-title>
        <source>IEEE Transactions on signal processing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>57</volume>
        <issue>12</issue>
      </element-citation>
    </ref>
    <ref id="ref-variani2014deep">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Variani</surname><given-names>Ehsan</given-names></name>
          <name><surname>Lei</surname><given-names>Xin</given-names></name>
          <name><surname>McDermott</surname><given-names>Erik</given-names></name>
          <name><surname>Moreno</surname><given-names>Ignacio Lopez</given-names></name>
          <name><surname>Gonzalez-Dominguez</surname><given-names>Javier</given-names></name>
        </person-group>
        <article-title>Deep neural networks for small footprint text-dependent speaker verification</article-title>
        <source>Acoustics, speech and signal processing (ICASSP), 2014 IEEE international conference on</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2014">2014</year>
      </element-citation>
    </ref>
    <ref id="ref-hinton2012deep">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hinton</surname><given-names>Geoffrey</given-names></name>
          <name><surname>Deng</surname><given-names>Li</given-names></name>
          <name><surname>Yu</surname><given-names>Dong</given-names></name>
          <name><surname>Dahl</surname><given-names>George E</given-names></name>
          <name><surname>Mohamed</surname><given-names>Abdel-rahman</given-names></name>
          <name><surname>Jaitly</surname><given-names>Navdeep</given-names></name>
          <name><surname>Senior</surname><given-names>Andrew</given-names></name>
          <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
          <name><surname>Nguyen</surname><given-names>Patrick</given-names></name>
          <name><surname>Sainath</surname><given-names>Tara N</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</article-title>
        <source>IEEE Signal Processing Magazine</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <volume>29</volume>
        <issue>6</issue>
      </element-citation>
    </ref>
    <ref id="ref-lecun2015deep">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>LeCun</surname><given-names>Yann</given-names></name>
          <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
          <name><surname>Hinton</surname><given-names>Geoffrey</given-names></name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>nature</source>
        <publisher-name>Nature Publishing Group</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>521</volume>
        <issue>7553</issue>
      </element-citation>
    </ref>
    <ref id="ref-liu2015deep">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Liu</surname><given-names>Yuan</given-names></name>
          <name><surname>Qian</surname><given-names>Yanmin</given-names></name>
          <name><surname>Chen</surname><given-names>Nanxin</given-names></name>
          <name><surname>Fu</surname><given-names>Tianfan</given-names></name>
          <name><surname>Zhang</surname><given-names>Ya</given-names></name>
          <name><surname>Yu</surname><given-names>Kai</given-names></name>
        </person-group>
        <article-title>Deep feature for text-dependent speaker verification</article-title>
        <source>Speech Communication</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>73</volume>
      </element-citation>
    </ref>
    <ref id="ref-torfi2018attention">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Torfi</surname><given-names>Amirsina</given-names></name>
          <name><surname>Shirvani</surname><given-names>Rouzbeh A</given-names></name>
        </person-group>
        <article-title>Attention-based guided structured sparsity of deep neural networks</article-title>
        <source>arXiv preprint arXiv:1802.09902</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
