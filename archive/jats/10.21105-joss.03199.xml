<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3199</article-id>
<article-id pub-id-type="doi">10.21105/joss.03199</article-id>
<title-group>
<article-title>Sapsan: Framework for Supernovae Turbulence Modeling with
Machine Learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4311-8490</contrib-id>
<string-name>Platon I. Karpov</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6809-8943</contrib-id>
<string-name>Iskandar Sitdikov</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-3176-8042</contrib-id>
<string-name>Chengkun Huang</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-2624-0056</contrib-id>
<string-name>Chris L. Fryer</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Astronomy &amp; Astrophysics, University of
California, Santa Cruz, CA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Los Alamos National Laboratory, Los Alamos,
NM</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Provectus IT Inc., Palo Alto, CA</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>6</volume>
<issue>67</issue>
<fpage>3199</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>machine learning</kwd>
<kwd>astronomy</kwd>
<kwd>supernovae</kwd>
<kwd>turbulence</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><ext-link ext-link-type="uri" xlink:href="https://github.com/pikarpov-LANL/Sapsan"><monospace>Sapsan</monospace></ext-link>
  is a framework designed to make Machine Learning (ML) more accessible
  in the study of turbulence, with a focus on astrophysical
  applications. <monospace>Sapsan</monospace> includes modules to load,
  filter, subsample, batch, and split the data from hydrodynamic (HD)
  simulations for training and validation. Next, the framework includes
  built-in conventional and physically-motivated estimators that have
  been used for turbulence modeling. This ties into
  <monospace>Sapsan</monospace>’s custom estimator module, aimed at
  designing a custom ML model layer-by-layer, which is the core benefit
  of using the framework. To share your custom model, every new project
  created via <monospace>Sapsan</monospace> comes with pre-filled,
  ready-for-release Docker files. Furthermore, training and evaluation
  modules come with <monospace>Sapsan</monospace> as well. The latter,
  among other features, includes the construction of power spectra and
  comparison to established analytical turbulence closure models, such
  as a gradient model. Thus, <monospace>Sapsan</monospace> attempts to
  minimize the hard work required for data preparation and analysis,
  leaving one to focus on the ML model design itself.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Domain sciences have been slow to adopt Machine Learning (ML) for a
  range of projects, but particularly for physical simulations modeling
  turbulence. It is challenging to prove that an ML model has learned
  the laws of physics in a particular problem, and that it has the
  ability to extrapolate within the parameter-space of the simulation.
  The inability to directly infer the predictive capabilities of ML is
  one of the major causes behind the slow adoption rates; however, the
  community cannot ignore the effectiveness of ML.</p>
  <p>Turbulence is ubiquitous in astrophysical environments, however, it
  involves physics at a vast range of temporal and spatial scales,
  making accurate fully-resolved modeling difficult. Various analytical
  turbulence models have been developed to be used in simulations using
  temporal or spatial averaged governing equations, such as RANS
  (Reynolds-averaged Navier-Stokes) and LES (Large Eddy Simulation), but
  the accuracy of these methods is sometimes inadequate. In search of
  better methods to model turbulence in core-collapse supernovae, it
  became apparent that ML has the potential to produce more accurate
  turbulence models on an un-averaged subgrid-scale than the current
  methods. Scientists from both industry and academia
  (<xref alt="King et al., 2016" rid="ref-king2016" ref-type="bibr">King
  et al., 2016</xref>;
  <xref alt="Zhang et al., 2018" rid="ref-zhang2018" ref-type="bibr">Zhang
  et al., 2018</xref>) have already begun using ML for applied turbulent
  problems. Still, none of these efforts have yet reached the scales
  relevant for the physics and astronomy community on a practical level.
  For example, physics-based model evaluation and interpretability tools
  are not standardized, nor are they widely available. As a result, it
  is a common struggle to verify published results, with the setup not
  fully documented, the opaquely structured code lacking clear
  commenting, or even worse, not publicly available. This is a problem
  that the broader ML community can relate to as well
  (<xref alt="Hutson, 2018" rid="ref-Hutson725" ref-type="bibr">Hutson,
  2018</xref>). Thus, it is not surprising that there is considerable
  skepticism against ML in physical sciences, with astrophysics being no
  exception
  (<xref alt="Carleo et al., 2019" rid="ref-carleo2019" ref-type="bibr">Carleo
  et al., 2019</xref>).</p>
  <p>In pursuit of our supernova (SNe) study, the issues outlined above
  became painfully apparent. Thus, we have attempted to lower the
  barrier to entry for new researchers in domain science fields studying
  turbulence to employ ML, with the main focus on astrophysical
  applications. As a result, we developed an ML Python-based pipeline
  called <monospace>Sapsan</monospace>. The goals have been to make this
  library accessible and shared with the community through Jupyter
  Notebooks, a command-line-interface (CLI) and a
  graphical-user-interface (GUI)<xref ref-type="fn" rid="fn1">1</xref>
  available for end-users. <monospace>Sapsan</monospace> includes
  built-in optimized ML models for turbulence treatment, both
  conventional and physics-based. More importantly, at its core, the
  framework is meant to be flexible and modular; hence there is an
  intuitive interface for users to work on their own ML algorithms. Most
  of the mundane turbulence ML researcher needs, such as data
  preprocessing and prediction analysis, can be automated through
  <monospace>Sapsan</monospace>, with a streamlined process of custom
  estimator development. In addition, <monospace>Sapsan</monospace>
  brings best practices from the industry regarding ML development
  frameworks. For example, <monospace>Sapsan</monospace> includes docker
  containers for reproducible release, as well as
  <ext-link ext-link-type="uri" xlink:href="https://mlflow.org/">MLflow</ext-link>
  for experiment tracking. Thus, <monospace>Sapsan</monospace> is a
  single, complete interface for ML-based turbulence research.</p>
  <p><monospace>Sapsan</monospace> is distributed through
  <ext-link ext-link-type="uri" xlink:href="https://github.com/pikarpov-LANL/Sapsan">GitHub</ext-link>
  and
  <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/sapsan/">pip</ext-link>.
  For further reference,
  <ext-link ext-link-type="uri" xlink:href="https://github.com/pikarpov-LANL/Sapsan/wiki">wiki</ext-link>
  is maintained on GitHub as well.</p>
</sec>
<sec id="framework">
  <title>Framework</title>
  <p><monospace>Sapsan</monospace> organizes workflow via three
  respective stages: data preparation, machine learning, and analysis,
  as shown in Figure 1. The whole process can be further distributed
  using Docker for reproducibility. Let’s break down each stage in the
  context of turbulence subgrid modeling, e.g., a model to predict
  turbulent behavior at the under-resolved simulation scales.</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Data Preparation</bold></p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Loading Data:</bold> <monospace>Sapsan</monospace> is
          ready to process common 2D &amp; 3D hydrodynamic (HD) and
          magnetohydrodynamic (MHD) turbulence data in
          simulation-code-specific data formats, such as HDF5 (with more
          to come per community need).</p>
        </list-item>
        <list-item>
          <p><bold>Transformations:</bold> A variety of tools are
          available for the user to prepare data for training:</p>
          <list list-type="bullet">
            <list-item>
              <p><bold>Filtering:</bold> To build a subgrid model, one
              will have to filter the data to, for example, remove
              small-scale perturbations. Some possible choices include a
              box, spectral, or Gaussian filter. The data can be
              filtered on the fly within the framework.</p>
            </list-item>
            <list-item>
              <p><bold>Sampling:</bold> to run quick tests of your
              model, you might want to test on a sampled version of the
              data while retaining the full spatial domain. For this
              application, equidistant sampling is available in
              <monospace>Sapsan</monospace>.</p>
            </list-item>
            <list-item>
              <p><bold>Batching &amp; Splitting:</bold> The data are
              spatially batched and divided into testing and validation
              subsets.</p>
            </list-item>
          </list>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p><bold>Machine Learning</bold></p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Model Setup:</bold> Different ML models may be
          appropriate for different physical regimes, and
          <monospace>Sapsan</monospace> provides templates for a
          selection of both conventional and physics-based models with
          more to come. Only the most important options are left up to
          the user to edit, with most overhead kept in the backend. This
          stage also includes tools for defining ML layers, tracking
          parameters, and choosing and tuning optimization
          algorithms.</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p><bold>Analysis</bold></p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Trained Model:</bold> A turbulence subgrid model
          defines how small-scale structure affects the large scale
          quantities. In other words, it completes or ‘’closes’’ the
          governing large-scale equations of motion with small-scale
          terms. The prediction from a trained ML model is used to
          provide the needed quantities.</p>
        </list-item>
        <list-item>
          <p><bold>Analytical Tools:</bold> There are also methods
          included for comparing the trained model with conventional
          analytic turbulence models [such as the Dynamic Smagorisnky,
          Lilly
          (<xref alt="1966" rid="ref-lilly1966" ref-type="bibr">1966</xref>);
          or Gradient, Liu et al.
          (<xref alt="1994" rid="ref-liu_meneveau_katz_1994" ref-type="bibr">1994</xref>);
          models], or to conduct other tests of, for example, the power
          spectrum of the model prediction.</p>
        </list-item>
      </list>
    </list-item>
  </list>
  <p>For further information on each stage, please refer to
  <ext-link ext-link-type="uri" xlink:href="https://github.com/pikarpov-LANL/Sapsan/wiki"><monospace>Sapsan</monospace>’s
  Wiki on Gihub</ext-link>.</p>
  <fig>
    <caption><p>High-level overview of <monospace>Sapsan's</monospace>
    workflow.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="Sapsan_highlevel_overview.png" xlink:title="" />
  </fig>
  <sec id="dependencies">
    <title>Dependencies</title>
    <p>The following is a list of the core functional
    dependencies<xref ref-type="fn" rid="fn2">2</xref> and a short
    description of how they are used within
    <monospace>Sapsan</monospace>:</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>PyTorch:</bold> <monospace>Sapsan</monospace>, at
        large, relies on PyTorch to configure and train ML models. Thus,
        the parameters in the aforementioned <bold>Model Setup</bold>
        stage should be configured with PyTorch functions.
        <ext-link ext-link-type="uri" xlink:href="https://github.com/pikarpov-LANL/Sapsan/blob/master/sapsan/examples/cnn_example.ipynb">Convolutional
        Neural Network (CNN)</ext-link> and
        <ext-link ext-link-type="uri" xlink:href="https://github.com/pikarpov-LANL/Sapsan/blob/master/sapsan/examples/picae_example.ipynb">Physics-Informed
        Convolutional Auto Encoder (PICAE)</ext-link> examples included
        with <monospace>Sapsan</monospace> are based on PyTorch.
        (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
        et al., 2019</xref>)</p>
      </list-item>
      <list-item>
        <p><bold>Scikit-learn:</bold> A alternative to
        <monospace>PyTorch</monospace>, as demonstrated in the
        <ext-link ext-link-type="uri" xlink:href="https://github.com/pikarpov-LANL/Sapsan/blob/master/sapsan/examples/krr_example.ipynb">Kernel
        Ridge Regression (KRR)</ext-link> example in
        <monospace>Sapsan</monospace>. Since
        <monospace>scikit-learn</monospace> is less flexible and
        scalable than <monospace>PyTorch</monospace>,
        <monospace>PyTorch</monospace> is the recommended interface.
        (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
        et al., 2011</xref>)</p>
      </list-item>
      <list-item>
        <p><bold>Catalyst:</bold> used as part of the backend to
        configure early-stopping of the model and logging.
        (<xref alt="Kolesnikov, 2018" rid="ref-catalyst" ref-type="bibr">Kolesnikov,
        2018</xref>)</p>
      </list-item>
      <list-item>
        <p><bold>MLflow:</bold> provides an intuitive web interface for
        tracking the results of large experiments and parameter studies.
        Beyond a few default parameters, a user can include custom
        parameters to be tracked.
        (<xref alt="Databricks, 2020" rid="ref-mlflow_github" ref-type="bibr">Databricks,
        2020</xref>)</p>
      </list-item>
      <list-item>
        <p><bold>Jupyter Notebook:</bold> the most direct and versatile
        way to use <monospace>Sapsan</monospace>.</p>
      </list-item>
      <list-item>
        <p><bold>Streamlit (GUI):</bold> a graphical user interface
        (GUI) for <monospace>Sapsan</monospace>. While not as flexible
        as the other interfaces, this can be useful for developing
        public-facing demonstrations. An example of this interface can
        be found online at
        <ext-link ext-link-type="uri" xlink:href="https://sapsan.app">sapsan.app</ext-link>.
        (<xref alt="Treuille, 2019" rid="ref-streamlit2019" ref-type="bibr">Treuille,
        2019</xref>)</p>
      </list-item>
      <list-item>
        <p><bold>Click (CLI):</bold> a command-line interface (CLI) for
        <monospace>Sapsan</monospace>. It is used to get the user up and
        running with templates for a custom project.
        (<xref alt="Ronacher, 2021" rid="ref-click" ref-type="bibr">Ronacher,
        2021</xref>)</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="applications">
  <title>Applications</title>
  <p>While <monospace>Sapsan</monospace> is designed to be highly
  customizable for a wide variety of projects in the physical sciences,
  it is optimized for the study of turbulence. In this section we will
  demostrate various capabilities of <monospace>Sapsan</monospace>
  working with 2D and 3D data, various machine learning libraries, and
  built-in analytical tools. The ML methods used are included in
  <monospace>Sapsan's</monospace> distribution as example
  <monospace>Jupyter notebooks</monospace> to get started with the
  framework.</p>
  <sec id="hydro-simulations">
    <title>Hydro simulations</title>
    <p>Here is an examples of a turbulence closure model trained on the
    high-resolution Johns Hopkins Turbulence Database (JHTDB,
    <xref alt="Li et al., 2008" rid="ref-jhtdb2008" ref-type="bibr">Li
    et al., 2008</xref>). The training data is a 2D slice of a direct
    numerical simulation (DNS) of a statistically-stationary isotropic
    3D MHD turbulence dataset, <inline-formula><alternatives>
    <tex-math><![CDATA[1024^3]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>1024</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
    in spatial resolution and covering roughly one large eddy turnover
    time over 1024 checkpoints, i.e. the dynamical time of the system
    (<xref alt="Eyink et al., 2013" rid="ref-Eyink2013" ref-type="bibr">Eyink
    et al., 2013</xref>). We compare it with a commonly used Dynamic
    Smagorinsky (DS) turbulence closure model
    (<xref alt="Lilly, 1966" rid="ref-lilly1966" ref-type="bibr">Lilly,
    1966</xref>). On the <monospace>Sapsan</monospace> side, a Kernel
    Ridge Regression model
    (<xref alt="Murphy, 2012" rid="ref-murphy2004" ref-type="bibr">Murphy,
    2012</xref>) by the means of <monospace>scikit-learn</monospace> is
    used to demonstrate the effectiveness of conventional ML approaches
    in tackling turbulence problems. In this test, we used the following
    setup:</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>Train features:</bold> velocity (<italic>u</italic>),
        vector potential (<italic>A</italic>), magnetic field
        (<italic>B</italic>), and their respective derivatives a
        checkpoint = 0. All quantities have been filtered down to 15
        Fourier modes to remove small-scale perturbations, mimicking the
        lower fidelity of a non-DNS simulation. Next they were sampled
        down to <inline-formula><alternatives>
        <tex-math><![CDATA[128^3]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>128</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>,
        with the last step leaving a single slice of
        <inline-formula><alternatives>
        <tex-math><![CDATA[128^2]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>128</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
        ready for training.</p>
      </list-item>
      <list-item>
        <p><bold>Model Input:</bold> low fidelity velocity
        (<italic>u</italic>), vector potential (<italic>A</italic>),
        magnetic field (<italic>B</italic>), and their respective
        derivatives at a set checkpoint = 10.</p>
      </list-item>
      <list-item>
        <p><bold>Model Output:</bold> velocity stress tensor component
        (<inline-formula><alternatives>
        <tex-math><![CDATA[\tau_{xy}]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>)
        at the matching checkpoint in the future, which effectively
        represents the difference between large and small scale
        structures of the system.</p>
      </list-item>
    </list>
    <p>In Figure 2, it can be seen that the ML-based approach
    significantly outperforms the DS subgrid model in reproducing the
    probability density function, i.e., a statistical distribution of
    the stress tensor. The results are consistent with
    (<xref alt="King et al., 2016" rid="ref-king2016" ref-type="bibr">King
    et al., 2016</xref>).</p>
    <fig>
      <caption><p>Predicting a 2D turbulent stress-tensor component
      (<inline-formula><alternatives>
      <tex-math><![CDATA[\tau_{xy}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>)
      in statistically-stationary isotropic MHD turbulence setup. The
      left plot compares the original spatial map of the stress-tensor
      component to the predicted spatial map (middle). The plot on the
      right presents probability density functions (PDF), i.e.,
      distributions, of the original stress-tensor component values, the
      ML predicted values, and the conventional Dynamic Smagorinsky (DS)
      subgrid model prediction.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="jhtdb_t10_tn1_slices_pdf.png" xlink:title="" />
    </fig>
  </sec>
  <sec id="supernovae">
    <title>Supernovae</title>
    <p>Even though the conventional regression-based ML approach worked
    well in the 2D setup from the previous example, the complexity of
    our physical problem forced us to seek out a more sophisticated ML
    method. Supernovae host a different physical regime that is far from
    the idealistic MHD turbulence case from before. Here we are dealing
    with dynamically evolving turbulence that is not necessarily
    isotropic. Turbulence can behave drastically differently depending
    on the evolutionary stage. With <monospace>Sapsan</monospace>, we
    have tested a 3D CNN (Convolutional Neural Network) model built with
    <monospace>PyTorch</monospace> to predict a turbulent velocity
    stress tensor in a realistic Core-Collapse Supernova (CCSN) case.
    Figure 3 presents results of the following:</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>Train features:</bold> velocity (<italic>u</italic>),
        magnetic field (<italic>B</italic>), and their respective
        derivatives at time steps before 5 ms (halfway of the total
        simulation). All quantities have been filtered down with a
        <inline-formula><alternatives>
        <tex-math><![CDATA[\sigma=9]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
        Gaussian filter to remove small-scale perturbations, mimicking
        the lower fidelity of a non-DNS simulation. Lastly they were
        sampled from the original <inline-formula><alternatives>
        <tex-math><![CDATA[348^3]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>348</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
        down to <inline-formula><alternatives>
        <tex-math><![CDATA[116^3]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>116</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
        in resolution.</p>
      </list-item>
      <list-item>
        <p><bold>Model Input:</bold> low fidelity velocity
        (<italic>u</italic>), magnetic field (<italic>B</italic>), and
        their respective derivatives at a set time step in the future
        beyond 5 ms.</p>
      </list-item>
      <list-item>
        <p><bold>Model Output:</bold> velocity stress tensor components
        (<inline-formula><alternatives>
        <tex-math><![CDATA[\tau_{ij}]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>)
        at the matching time step in the future, which effectively
        represents the difference between large and small scale
        structures of the system.</p>
      </list-item>
    </list>
    <p>In this case, the probability density functions are overall
    consistent, with minor disagreement at the positive outliers, even
    though the prediction is performed far into the future (time = 9.48
    ms, end of the simulation time). Predictive advantage is highlighted
    when compared with the analytical Gradient model that misses a large
    portion of positive data.</p>
    <fig>
      <caption><p>Predicting turbulent stress-tensor component in a
      core-collapse supernovae (CCSN). The model has been trained on a
      selection of dynamically evolving turbulence timesteps during the
      first 5 ms (out of the total <inline-formula><alternatives>
      <tex-math><![CDATA[\sim 10]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      ms) of a 3D MHD direct numerical simulation (DNS) after the
      shockwave bounced off the core in a CCSN scenario. On the left,
      the two figures are the 2D slices of a 3D
      <inline-formula><alternatives>
      <tex-math><![CDATA[\tau_{xy}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
      prediction, with the right plot comparing PDFs of the original 3D
      data, 3D ML prediction, and a conventional Gradient subgrid
      model.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="ccsn_t160_tn1_slices_pdf.png" xlink:title="" />
    </fig>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The development of <monospace>Sapsan</monospace> was supported by
  the Laboratory Directed Research and Development program and the
  Center for Space and Earth Science at Los Alamos National Laboratory
  through the student fellow grant. We would like to thank DOE SciDAC
  for additional funding support.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-lilly1966">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lilly</surname><given-names>D. K.</given-names></name>
        </person-group>
        <article-title>On the application of the eddy viscosity concept in the Inertial sub-range of turbulence</article-title>
        <source>NCAR Manuscript 123</source>
        <year iso-8601-date="1966-01">1966</year><month>01</month>
      </element-citation>
    </ref>
    <ref id="ref-king2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>King</surname><given-names>Ryan N.</given-names></name>
          <name><surname>Hamlington</surname><given-names>Peter E.</given-names></name>
          <name><surname>Dahm</surname><given-names>Werner J. A.</given-names></name>
        </person-group>
        <article-title>Autonomic closure for turbulence simulations</article-title>
        <source>Phys. Rev. E</source>
        <publisher-name>American Physical Society</publisher-name>
        <year iso-8601-date="2016-03">2016</year><month>03</month>
        <volume>93</volume>
        <uri>https://link.aps.org/doi/10.1103/PhysRevE.93.031301</uri>
        <pub-id pub-id-type="doi">10.1103/PhysRevE.93.031301</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-streamlit2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Treuille</surname><given-names>Adrien</given-names></name>
        </person-group>
        <article-title>Turn python scripts into beautiful ML tools</article-title>
        <source>Towards Data Science</source>
        <year iso-8601-date="2019-10">2019</year><month>10</month>
        <volume>8</volume>
      </element-citation>
    </ref>
    <ref id="ref-click">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Ronacher</surname><given-names>Armin</given-names></name>
        </person-group>
        <source>Click</source>
        <year iso-8601-date="2021">2021</year>
        <uri>https://click.palletsprojects.com/</uri>
      </element-citation>
    </ref>
    <ref id="ref-mlflow_github">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Databricks</surname><given-names>Inc</given-names></name>
        </person-group>
        <article-title>MLflow</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/mlflow/mlflow; GitHub</publisher-name>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-pytorch">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Paszke</surname><given-names>Adam</given-names></name>
          <name><surname>Gross</surname><given-names>Sam</given-names></name>
          <name><surname>Massa</surname><given-names>Francisco</given-names></name>
          <name><surname>Lerer</surname><given-names>Adam</given-names></name>
          <name><surname>Bradbury</surname><given-names>James</given-names></name>
          <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
          <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
          <name><surname>Lin</surname><given-names>Zeming</given-names></name>
          <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
          <name><surname>Antiga</surname><given-names>Luca</given-names></name>
          <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
          <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
          <name><surname>Yang</surname><given-names>Edward</given-names></name>
          <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
          <name><surname>Raison</surname><given-names>Martin</given-names></name>
          <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
          <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
          <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
          <name><surname>Fang</surname><given-names>Lu</given-names></name>
          <name><surname>Bai</surname><given-names>Junjie</given-names></name>
          <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
        </person-group>
        <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
        <source>Advances in neural information processing systems 32</source>
        <person-group person-group-type="editor">
          <name><surname>Wallach</surname><given-names>H.</given-names></name>
          <name><surname>Larochelle</surname><given-names>H.</given-names></name>
          <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
          <name><surname>dAlché-Buc</surname><given-names>F.</given-names></name>
          <name><surname>Fox</surname><given-names>E.</given-names></name>
          <name><surname>Garnett</surname><given-names>R.</given-names></name>
        </person-group>
        <publisher-name>Curran Associates, Inc.</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-scikit-learn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
          <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
          <name><surname>Gramfort</surname><given-names>A.</given-names></name>
          <name><surname>Michel</surname><given-names>V.</given-names></name>
          <name><surname>Thirion</surname><given-names>B.</given-names></name>
          <name><surname>Grisel</surname><given-names>O.</given-names></name>
          <name><surname>Blondel</surname><given-names>M.</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
          <name><surname>Weiss</surname><given-names>R.</given-names></name>
          <name><surname>Dubourg</surname><given-names>V.</given-names></name>
          <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
          <name><surname>Passos</surname><given-names>A.</given-names></name>
          <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
          <name><surname>Brucher</surname><given-names>M.</given-names></name>
          <name><surname>Perrot</surname><given-names>M.</given-names></name>
          <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
      </element-citation>
    </ref>
    <ref id="ref-zhang2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Zhang</surname><given-names>Weiwei</given-names></name>
          <name><surname>Zhu</surname><given-names>Linyang</given-names></name>
          <name><surname>Liu</surname><given-names>Yilang</given-names></name>
          <name><surname>Kou</surname><given-names>Jiaqing</given-names></name>
        </person-group>
        <article-title>Machine learning methods for turbulence modeling in subsonic flows over airfoils</article-title>
        <source>arXiv e-prints</source>
        <year iso-8601-date="2018-06">2018</year><month>06</month>
        <uri>https://arxiv.org/abs/1806.05904</uri>
      </element-citation>
    </ref>
    <ref id="ref-carleo2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Carleo</surname><given-names>Giuseppe</given-names></name>
          <name><surname>Cirac</surname><given-names>Ignacio</given-names></name>
          <name><surname>Cranmer</surname><given-names>Kyle</given-names></name>
          <name><surname>Daudet</surname><given-names>Laurent</given-names></name>
          <name><surname>Schuld</surname><given-names>Maria</given-names></name>
          <name><surname>Tishby</surname><given-names>Naftali</given-names></name>
          <name><surname>Vogt-Maranto</surname><given-names>Leslie</given-names></name>
          <name><surname>Zdeborová</surname><given-names>Lenka</given-names></name>
        </person-group>
        <article-title>Machine learning and the physical sciences</article-title>
        <source>Reviews of Modern Physics</source>
        <year iso-8601-date="2019-10">2019</year><month>10</month>
        <volume>91</volume>
        <issue>4</issue>
        <uri>https://arxiv.org/abs/1903.10563</uri>
        <pub-id pub-id-type="doi">10.1103/RevModPhys.91.045002</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-jhtdb2008">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Li</surname><given-names>Yi</given-names></name>
          <name><surname>Perlman</surname><given-names>Eric</given-names></name>
          <name><surname>Wan</surname><given-names>Minping</given-names></name>
          <name><surname>Yang</surname><given-names>Yunke</given-names></name>
          <name><surname>Meneveau</surname><given-names>Charles</given-names></name>
          <name><surname>Burns</surname><given-names>Randal</given-names></name>
          <name><surname>Chen</surname><given-names>Shiyi</given-names></name>
          <name><surname>Szalay</surname><given-names>Alexander</given-names></name>
          <name><surname>Eyink</surname><given-names>Gregory</given-names></name>
        </person-group>
        <article-title>A public turbulence database cluster and applications to study Lagrangian evolution of velocity increments in turbulence</article-title>
        <source>Journal of Turbulence</source>
        <year iso-8601-date="2008-01">2008</year><month>01</month>
        <volume>9</volume>
        <uri>https://arxiv.org/abs/0804.1703</uri>
        <pub-id pub-id-type="doi">10.1080/14685240802376389</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Eyink2013">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Eyink</surname><given-names>Gregory</given-names></name>
          <name><surname>Vishniac</surname><given-names>Ethan</given-names></name>
          <name><surname>Lalescu</surname><given-names>Cristian</given-names></name>
          <name><surname>Aluie</surname><given-names>Hussein</given-names></name>
          <name><surname>Kanov</surname><given-names>Kalin</given-names></name>
          <name><surname>Bürger</surname><given-names>Kai</given-names></name>
          <name><surname>Burns</surname><given-names>Randal</given-names></name>
          <name><surname>Meneveau</surname><given-names>Charles</given-names></name>
          <name><surname>Szalay</surname><given-names>Alexander</given-names></name>
        </person-group>
        <article-title>Flux-freezing breakdown in high-conductivity magnetohydrodynamic turbulence</article-title>
        <source>Nature</source>
        <year iso-8601-date="2013-05-01">2013</year><month>05</month><day>01</day>
        <volume>497</volume>
        <issue>7450</issue>
        <issn>1476-4687</issn>
        <uri>https://doi.org/10.1038/nature12128</uri>
        <pub-id pub-id-type="doi">10.1038/nature12128</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-murphy2004">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Murphy</surname><given-names>Kevin P.</given-names></name>
        </person-group>
        <article-title>Machine learning: A probabilistic perspective</article-title>
        <publisher-name>The MIT Press</publisher-name>
        <year iso-8601-date="2012">2012</year>
      </element-citation>
    </ref>
    <ref id="ref-catalyst">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Kolesnikov</surname><given-names>Sergey</given-names></name>
        </person-group>
        <article-title>Accelerated DL r&amp;d</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/catalyst-team/catalyst; GitHub</publisher-name>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-Hutson725">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hutson</surname><given-names>Matthew</given-names></name>
        </person-group>
        <article-title>Artificial intelligence faces reproducibility crisis</article-title>
        <source>Science</source>
        <publisher-name>American Association for the Advancement of Science</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>359</volume>
        <issue>6377</issue>
        <issn>0036-8075</issn>
        <uri>https://science.sciencemag.org/content/359/6377/725</uri>
        <pub-id pub-id-type="doi">10.1126/science.359.6377.725</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-liu_meneveau_katz_1994">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Liu</surname><given-names>Shewen</given-names></name>
          <name><surname>Meneveau</surname><given-names>Charles</given-names></name>
          <name><surname>Katz</surname><given-names>Joseph</given-names></name>
        </person-group>
        <article-title>On the properties of similarity subgrid-scale models as deduced from measurements in a turbulent jet</article-title>
        <source>Journal of Fluid Mechanics</source>
        <publisher-name>Cambridge University Press</publisher-name>
        <year iso-8601-date="1994">1994</year>
        <volume>275</volume>
        <pub-id pub-id-type="doi">10.1017/S0022112094002296</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>A demo is available at
    <ext-link ext-link-type="uri" xlink:href="https://sapsan.app/">sapsan.app</ext-link>.</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>Please refer to
    <ext-link ext-link-type="uri" xlink:href="https://github.com/pikarpov-LANL/Sapsan">GitHub</ext-link>
    for the complete list of dependencies.</p>
  </fn>
</fn-group>
</back>
</article>
