<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2704</article-id>
<article-id pub-id-type="doi">10.21105/joss.02704</article-id>
<title-group>
<article-title>MLJ: A Julia package for composable machine
learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6689-886X</contrib-id>
<string-name>Anthony D. Blaom</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9254-793X</contrib-id>
<string-name>Franz Kiraly</string-name>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0872-7098</contrib-id>
<string-name>Thibaut Lienart</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-0287-8699</contrib-id>
<string-name>Yiannis Simillides</string-name>
<xref ref-type="aff" rid="aff-7"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7829-6102</contrib-id>
<string-name>Diego Arenas</string-name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9025-0753</contrib-id>
<string-name>Sebastian J. Vollmer</string-name>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Auckland, New Zealand</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>New Zealand eScience Infrastructure, New
Zealand</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Alan Turing Institute, London, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>University College London, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>University of Warwick, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>University of St Andrews, St Andrews, United
Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>Imperial College London, United Kingdom</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>5</volume>
<issue>55</issue>
<fpage>2704</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Machine Learning</kwd>
<kwd>model composition</kwd>
<kwd>stacking</kwd>
<kwd>ensembling</kwd>
<kwd>hyper-parameter tuning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="introduction">
  <title>Introduction</title>
  <p>Statistical modeling, and the building of complex modeling
  pipelines, is a cornerstone of modern data science. Most experienced
  data scientists rely on high-level open source modeling toolboxes -
  such as sckit-learn
  (<xref alt="Buitinck et al., 2013" rid="ref-Buitinck2013" ref-type="bibr">Buitinck
  et al., 2013</xref>;
  <xref alt="Pedregosa et al., 2011" rid="ref-Pedregosa2001" ref-type="bibr">Pedregosa
  et al., 2011</xref>) (Python); Weka
  (<xref alt="Holmes et al., 1994" rid="ref-Holmes1994" ref-type="bibr">Holmes
  et al., 1994</xref>) (Java); mlr
  (<xref alt="Bischl et al., 2016" rid="ref-BischlEtal2016" ref-type="bibr">Bischl
  et al., 2016</xref>) and caret
  (<xref alt="Kuhn, 2008" rid="ref-Kuhn2008" ref-type="bibr">Kuhn,
  2008</xref>) (R) - for quick blueprinting, testing, and creation of
  deployment-ready models. They do this by providing a common interface
  to atomic components, from an ever-growing model zoo, and by providing
  the means to incorporate these into complex work-flows. Practitioners
  are wanting to build increasingly sophisticated composite models, as
  exemplified in the strategies of top contestants in machine learning
  competitions such as Kaggle.</p>
  <p>MLJ (Machine Learning in Julia)
  (<xref alt="A. Blaom, 2020b" rid="ref-MLJdocs" ref-type="bibr">A.
  Blaom, 2020b</xref>) is a toolbox written in Julia that provides a
  common interface and meta-algorithms for selecting, tuning,
  evaluating, composing and comparing machine model implementations
  written in Julia and other languages. More broadly, the MLJ project
  hopes to bring cohesion and focus to a number of emerging and
  existing, but previously disconnected, machine learning algorithms and
  tools of high quality, written in Julia. A welcome corollary of this
  activity will be increased cohesion and synergy within the talent-rich
  communities developing these tools.</p>
  <p>In addition to other novelties outlined below, MLJ aims to provide
  first-in-class model composition capabilities. Guiding goals of the
  MLJ project have been usability, interoperability, extensibility, code
  transparency, and reproducibility.</p>
  <sec id="why-julia">
    <title>Why Julia?</title>
    <p>Nowadays, even technically competent users of scientific software
    will prototype solutions using a high-level language such as python,
    R, or MATLAB. However, to achieve satisfactory performance, such
    code typically wraps performance critical algorithms written in a
    second low-level language, such as C or FORTRAN. Through its use of
    an extensible, hierarchical system of abstract types, just-in-time
    compilation, and by replacing object-orientation with multiple
    dispatch, Julia solves the ubiquitous “two language problem”
    (<xref alt="Bezanson et al., 2017" rid="ref-BezansonEtal2017" ref-type="bibr">Bezanson
    et al., 2017</xref>). With less technical programming knowledge,
    experts in a domain of application can get under the hood of machine
    learning software to broaden its applicability, and innovation can
    be accelerated through a dramatically reduced software development
    cycle.</p>
    <p>As an example of the productivity boost provided by the
    single-language paradigm, we cite the DifferentialEquations.jl
    package
    (<xref alt="C. Rackauckas &amp; Nie, 2017" rid="ref-RackauckasNie2017" ref-type="bibr">C.
    Rackauckas &amp; Nie, 2017</xref>), which, in a few short years of
    development by a small team of domain experts, became the best
    package in its class
    (<xref alt="Christopher Rackauckas, 2018" rid="ref-Rackauckas2017" ref-type="bibr">Christopher
    Rackauckas, 2018</xref>).</p>
    <p>Another major advantage of a single-language solution is the
    ability to automatically differentiate (AD) functions from their
    code representations. The Flux.jl package
    (<xref alt="Innes, 2018" rid="ref-Innes2018" ref-type="bibr">Innes,
    2018</xref>), for example, already makes use of AD to allow
    unparalleled flexibility in neural network design.</p>
    <p>As a new language, Julia is high-performance computing-ready, and
    its superlative meta-programming features allow developers to create
    domain-specific syntax for user interaction.</p>
  </sec>
  <sec id="novelties">
    <title>Novelties</title>
    <p><bold>Composability.</bold> In line with current trends in
    “auto-ML”, MLJ’s design is largely predicated on the importance of
    model composability. Composite models share all the behavior of
    regular models, constructed using a new flexible “learning networks”
    syntax. Unlike the toolboxes cited above, MLJ’s composition syntax
    is flexible enough to define stacked models, with out-of-sample
    predictions for the base learners, as well as more routine linear
    pipelines, which can include target transformations that are
    learned. As in mlr, hyper-parameter tuning is implemented as a model
    wrapper.</p>
    <p><bold>A unified approach to probabilistic predictions.</bold> In
    MLJ, probabilistic prediction is treated as a first class feature,
    leveraging Julia’s type system. In particular, unnecessary
    case-distinctions, and ambiguous conventions regarding the
    representation of probabilities, are avoided.</p>
    <p><bold>Scientific types</bold> To help users focus less on data
    representation (e.g., <monospace>Float32</monospace>,
    <monospace>DataFrame</monospace>) and more on the intended
    <italic>purpose</italic> or <italic>interpretation</italic> of data,
    MLJ articulates model data requirements using <italic>scientific
    types</italic>
    (<xref alt="Anthony Blaom and collaborators, 2019" rid="ref-ScientificTypes" ref-type="bibr">Anthony
    Blaom and collaborators, 2019</xref>), such as “continuous”,
    “ordered factor” or “table”.</p>
    <p><bold>Connecting models directly to arbitrary data
    containers</bold>. A user can connect models directly to tabular
    data in a manifold of in-memory and out-of-memory formats by using a
    universal table interface provided by the Tables.jl package
    (<xref alt="Quinn, 2020" rid="ref-Quinn" ref-type="bibr">Quinn,
    2020</xref>).</p>
    <p><bold>Finding the right model.</bold> A model registry gives the
    user access to model metadata without the need to actually load code
    defining the model implementation. This metadata includes the
    model’s data requirements, for example, as well as a load path to
    enable MLJ to locate the model interface code. Users can readily
    match models to machine learning tasks, facilitating searches for an
    optimal model, a search that can be readily automated.</p>
    <p><bold>Tracking classes of categorical variables.</bold> Finally,
    with the help of scientific types and the CategoricalArrays.jl
    package
    (<xref alt="Bouchet-Valat, 2014" rid="ref-CategoricalArrays" ref-type="bibr">Bouchet-Valat,
    2014</xref>), users are guided to create safe representations of
    categorical data, in which the complete pool of possible classes is
    embedded in the data representation, and classifiers preserve this
    information when making predictions. This avoids a pain-point
    familiar in frameworks that simply recast categorical data using
    integers: evaluating a classifier on the test target, only to find
    the test data includes classes not seen in the training data.
    Preservation of the original labels for these classes also
    facilitates exploratory data analysis and interpretability.</p>
  </sec>
</sec>
<sec id="scientific-types">
  <title>Scientific types</title>
  <p>A scientific type is an ordinary Julia type (generally without
  instances) reserved for indicating how some data should be
  interpreted. Some of these types are shown in
  <xref alt="Figure 1" rid="fig1">Figure 1</xref>.</p>
  <fig>
    <caption><p>Part of the scientific type
    hierarchy.<styled-content id="fig1"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="scitypesII.png" xlink:title="" />
  </fig>
  <p>To the scientific types, MLJ adds a specific
  <italic>convention</italic> specifying a scientific type for every
  Julia object. The convention is expressed through a single method
  <monospace>scitype</monospace>. So, for example,
  <monospace>scitype(x)</monospace> returns
  <monospace>Continuous</monospace> whenever the type of
  <monospace>x</monospace> is a subtype of Julia’s
  <monospace>AbstractFloat</monospace> type, as in
  <monospace>scitype(3.14) == Continuous</monospace>. A tabular data
  structure satisfying the Tables.jl interface, will always have type
  <monospace>Table{K}</monospace>, where the type parameter
  <monospace>K</monospace> is the union of all column scientific types.
  A <monospace>coerce</monospace> method recasts machine types to have
  the desired scientific type (interpretation), and a
  <monospace>schema</monospace> method summarizes the machine and
  scientific types of tabular data.</p>
  <p>Since scientific types are also Julia types, Julia’s advanced type
  system means scientific types can be organized in a type hierarchy. It
  is straightforward to check the compatibility of data with a model’s
  scientific requirements and methods can be dispatched on scientific
  type just as they would on ordinary types.</p>
</sec>
<sec id="flexible-and-compact-work-flows-for-performance-evaluation-and-tuning">
  <title>Flexible and compact work-flows for performance evaluation and
  tuning</title>
  <p>To evaluate the performance of some <monospace>model</monospace>
  object (specifying the hyper-parameters of some supervised learning
  algorithm) using some specified <monospace>resampling</monospace>
  strategy, and measured against some battery of performance
  <monospace>measures</monospace>, one runs:</p>
  <code language="julia">evaluate(model, X, y, 
         resampling=CV(nfolds=6), 
         measures=[L2HingeLoss(), BrierScore()])</code>
  <p>which has (truncated) output</p>
  <table-wrap>
    <table>
      <colgroup>
        <col width="30%" />
        <col width="40%" />
        <col width="30%" />
      </colgroup>
      <thead>
        <tr>
          <th><monospace>measure</monospace></th>
          <th><monospace>measurement</monospace></th>
          <th><monospace>per_fold</monospace></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>L2HingeLoss</td>
          <td>1.4</td>
          <td>[0.485, 1.58, 2.06, 1.09, 2.18, 1.03]</td>
        </tr>
        <tr>
          <td>BrierScore{UnivariateFinite}</td>
          <td>-0.702</td>
          <td>[-0.242, -0.788, -1.03, -0.545, -1.09, -0.514]</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>As in mlr, hyper-parameter optimization is realized as a model
  wrapper, which transforms a base model into a “self-tuning” version of
  that model. That is, tuning is is abstractly specified before being
  executed. This allows tuning to be integrated into work-flows
  (learning networks) in multiple ways. A well-documented tuning
  interface
  (<xref alt="A. Blaom &amp; collaborators, 2020" rid="ref-MLJTuning" ref-type="bibr">A.
  Blaom &amp; collaborators, 2020</xref>) allows developers to easily
  extend available hyper-parameter tuning strategies.</p>
  <p>We now give an example of syntax for wrapping a model called
  <monospace>forest_model</monospace> in a random search tuning
  strategy, using cross-validation, and optimizing the mean square loss.
  The <monospace>model</monospace> in this case is a composite model
  with an ordinary hyper-parameter called
  <monospace>bagging_fraction</monospace> and a <italic>nested</italic>
  hyper-parameter <monospace>atom.n_subfeatures</monospace> (where
  <monospace>atom</monospace> is another model). The first two lines of
  code define ranges for these parameters.</p>
  <code language="julia">r1 = range(forest_model, :(atom.n_subfeatures), lower=1, upper=9)
r2 = range(forest_model, :bagging_fraction, lower=0.4, upper=1.0)
self_tuning_forest_model = TunedModel(model=forest_model,
                                      tuning=RandomSearch(),
                                      resampling=CV(nfolds=6),
                                      range=[r1, r2],
                                      measure=LPDistLoss(2),
                                      n=25)</code>
  <p>In this random search example, default priors are assigned to each
  hyper-parameter, but options exist to customize these. Both resampling
  and tuning have options for parallelization; Julia has first class
  support for both distributed and multi-threaded parallelism.</p>
</sec>
<sec id="a-unified-approach-to-probabilistic-predictions-and-their-evaluation">
  <title>A unified approach to probabilistic predictions and their
  evaluation</title>
  <p>MLJ puts probabilistic models and deterministic models on equal
  footing. Unlike most most frameworks, a supervised model is either
  <italic>probabilistic</italic> - meaning it’s
  <monospace>predict</monospace> method returns a distribution object -
  <italic>or</italic> it is <italic>deterministic</italic> - meaning it
  returns objects of the same scientific type as the training
  observations. To use a probabilistic model to make deterministic
  predictions one can wrap the model in a pipeline with an appropriate
  post-processing function, or use additional
  <monospace>predict_mean</monospace>,
  <monospace>predict_median</monospace>,
  <monospace>predict_mode</monospace> methods to deal with the common
  use-cases.</p>
  <p>A “distribution” object returned by a probabilistic predictor is
  one that can be sampled (using Julia’s <monospace>rand</monospace>
  method) and queried for properties. Where possible the object is in
  fact a <monospace>Distribution</monospace> object from the
  Distributions.jl package
  (<xref alt="Lin et al., 2020" rid="ref-LinEtal2020" ref-type="bibr">Lin
  et al., 2020</xref>), for which an additional
  <monospace>pdf</monospace> method for evaluating the distribution’s
  probability density or mass function will be implemented, in addition
  to <monospace>mode</monospace>, <monospace>mean</monospace> and
  <monospace>median</monospace> methods (allowing MLJ’s fallbacks for
  <monospace>predict_mean</monospace>, etc, to work).</p>
  <p>One important distribution <italic>not</italic> provided by
  Distributions.jl is a distribution for finite sample spaces with
  <italic>labeled</italic> elements (called
  <monospace>UnivariateFinite</monospace>) which additionally tracks all
  possible classes of the categorical variable it is modeling, and not
  just those observed in training data.</p>
  <p>By predicting distributions, instead of raw probabilities or
  parameters, MLJ avoids a common pain point, namely deciding and
  agreeing upon a convention about how these should be represented:
  Should a binary classifier predict one probability or two? Are we
  using the standard deviation or the variance here? What’s the protocol
  for deciding the order of (unordered) classes? How should multi-target
  predictions be combined?, etc.</p>
  <p>A case-in-point concerns performance measures (metrics) for
  probabilistic models, such as cross-entropy and Brier loss. All
  built-in probabilistic measures provided by MLJ are passed a
  distribution in their prediction slot.</p>
  <p>For an overview on probabilistic supervised learning we refer to
  (<xref alt="Gressmann et al., 2018" rid="ref-Gressmann2018" ref-type="bibr">Gressmann
  et al., 2018</xref>).</p>
</sec>
<sec id="model-interfaces">
  <title>Model interfaces</title>
  <p>In MLJ a <italic>model</italic> is just a struct storing the
  hyper-parameters associated with some learning algorithm suggested by
  the struct name (e.g., <monospace>DecisionTreeClassifier</monospace>)
  and that is all. MLJ provides a basic <italic>model
  interface</italic>, to be implemented by new machine learning models,
  which is functional in style, for simplicity and maximal flexibility.
  In addition to a <monospace>fit</monospace> and optional
  <monospace>update</monospace> method, one implements one or more
  operations, such as <monospace>predict</monospace>,
  <monospace>transform</monospace> and
  <monospace>inverse_transform</monospace>, acting on the learned
  parameters returned by <monospace>fit</monospace>.</p>
  <p>The optional <monospace>update</monospace> method allows one to
  avoid unnecessary repetition of code execution (warm restart). The
  three main use-cases are:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Iterative models.</bold> If the only change to a random
      forest model is an increase in the number of trees by ten, for
      example, then not all trees need to be retrained; only ten new
      trees need to be trained.</p>
    </list-item>
    <list-item>
      <p><bold>Data preprocessing.</bold> Avoid overheads associated
      with data preprocessing, such as coercion of data into an
      algorithm-specific type.</p>
    </list-item>
    <list-item>
      <p><bold>Smart training of composite models.</bold> When tuning a
      simple transformer-predictor pipeline model using a holdout set,
      for example, it is unnecessary to retrain the transformer if only
      the predictor hyper-parameters change. MLJ implements “smart”
      retraining of composite models like this by defining appropriate
      <monospace>update</monospace> methods.</p>
    </list-item>
  </list>
  <p>In the future MLJ will add an <monospace>update_data</monospace>
  method to support models that can carry out on-line learning.</p>
  <p>Presently, the general MLJ user is encouraged to interact through a
  <italic>machine interface</italic> which sits on top of the model
  interface. This makes some work-flows more convenient but, more
  significantly, introduces a syntax which is more natural in the
  context of model composition (see below). A <italic>machine</italic>
  is a mutable struct that binds a model to data at construction, as in
  <monospace>mach = machine(model, data)</monospace>, and which stores
  learned parameters after the user calls
  <monospace>fit!(mach, rows=...)</monospace>. To retrain with new
  hyper-parameters, the user can mutate <monospace>model</monospace> and
  repeat the <monospace>fit!</monospace> call.</p>
  <p>The operations <monospace>predict</monospace>,
  <monospace>transform</monospace>, etc are overloaded for machines,
  which is how the user typically uses them, as in the call
  <monospace>predict(mach, Xnew)</monospace>.</p>
</sec>
<sec id="flexible-model-composition">
  <title>Flexible model composition</title>
  <p>Several limitations surrounding model composition are increasingly
  evident to users of the dominant machine learning software platforms.
  The basic model composition interfaces provided by the toolboxes
  mentioned in the Introduction all share one or more of the following
  shortcomings, which do not exist in MLJ:</p>
  <list list-type="bullet">
    <list-item>
      <p>Composite models do not inherit all the behavior of ordinary
      models.</p>
    </list-item>
    <list-item>
      <p>Composition is limited to linear (non-branching) pipelines.</p>
    </list-item>
    <list-item>
      <p>Supervised components in a linear pipeline can only occur at
      the end of the pipeline.</p>
    </list-item>
    <list-item>
      <p>Only static (unlearned) target transformations/inverse
      transformations are supported.</p>
    </list-item>
    <list-item>
      <p>Hyper-parameters in homogeneous model ensembles cannot be
      coupled.</p>
    </list-item>
    <list-item>
      <p>Model stacking, with out-of-sample predictions for base
      learners, cannot be implemented.</p>
    </list-item>
    <list-item>
      <p>Hyper-parameters and/or learned parameters of component models
      are not easily inspected or manipulated (in tuning algorithms, for
      example)</p>
    </list-item>
    <list-item>
      <p>Composite models cannot implement multiple operations, for
      example, both a <monospace>predict</monospace> and
      <monospace>transform</monospace> method (as in clustering models)
      or both a <monospace>transform</monospace> and
      <monospace>inverse_transform</monospace> method.</p>
    </list-item>
  </list>
  <p>We now sketch MLJ’s composition API, referring the reader to
  (<xref alt="A. Blaom, 2020a" rid="ref-Blaom_I" ref-type="bibr">A.
  Blaom, 2020a</xref>) for technical details, and to the MLJ
  documentation
  (<xref alt="A. Blaom, 2020b" rid="ref-MLJdocs" ref-type="bibr">A.
  Blaom, 2020b</xref>;
  <xref alt="Lienart et al., 2020" rid="ref-MLJtutorials" ref-type="bibr">Lienart
  et al., 2020</xref>) for examples that will clarify how the
  composition syntax works in practice.</p>
  <p>Note that MLJ also provides “canned” model composition for common
  use cases, such as non-branching pipelines and homogeneous ensembles,
  which are not discussed further here.</p>
  <p>Specifying a new composite model type is in two steps,
  <italic>prototyping</italic> and <italic>export</italic>.</p>
  <sec id="prototyping">
    <title>Prototyping</title>
    <p>In prototyping the user defines a so-called <italic>learning
    network</italic>, by effectively writing down the same code she
    would use if composing the models “by hand”. She does this using the
    machine syntax, with which she will already be familiar, from the
    basic <monospace>fit!</monospace>/<monospace>predict</monospace>
    work-flow for single models. There is no need for the user to
    provide production training data in this process. A dummy data set
    suffices, for the purposes of testing the learning network as it is
    built.</p>
    <fig>
      <caption><p>Specifying prediction and training flows in a simple
      learning network. The network shown combines a ridge regressor
      with a learned target transformation (Box
      Cox).<styled-content id="fig2"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="target_transformerVERTICAL.png" xlink:title="" />
    </fig>
    <p>The upper panel of Figure
    <xref alt="Figure 2" rid="fig2">Figure 2</xref> illustrates a simple
    learning network in which a continuous target
    <monospace>y</monospace> is “normalized” using a learned Box Cox
    transformation, producing <monospace>z</monospace>, while PCA
    dimension reduction is applied to some features
    <monospace>X</monospace>, to obtain <monospace>Xr</monospace>. A
    Ridge regressor, trained using data from <monospace>Xr</monospace>
    and <monospace>z</monospace>, is then applied to
    <monospace>Xr</monospace> to make a target prediction
    <monospace>ẑ</monospace>. To obtain a final prediction
    <monospace>ŷ</monospace>, we apply the <italic>inverse</italic> of
    the Box Cox transform, learned previously, to
    <monospace>ẑ</monospace>.</p>
    <p>The lower “training” panel of the figure shows the three machines
    which will store the parameters learned in training - the Box Cox
    exponent and shift (<monospace>machine1</monospace>), the PCA
    projection (<monospace>machine2</monospace>) and the ridge model
    coefficients and intercept (<monospace>machine3</monospace>). The
    diagram additionally indicates where machines should look for
    training data, and where to access model hyper-parameters (stored in
    <monospace>box_cox</monospace>, <monospace>PCA</monospace> and
    <monospace>ridge_regressor</monospace>).</p>
    <p>The only syntactic difference between composing “by hand” and
    building a learning network is that the training data must be
    wrapped in “source nodes” (which can be empty if testing is not
    required) and the <monospace>fit!</monospace> calls can be omitted,
    as training is now lazy. Each data “variable” in the manual
    work-flow is now a node of a directed acyclic graph encoding the
    composite model architecture. Nodes are callable, with a node call
    triggering lazy evaluation of the <monospace>predict</monospace>,
    <monospace>transform</monospace> and other operations in the
    network. Instead of calling <monospace>fit!</monospace> on every
    machine, a single call to <monospace>fit!</monospace> on a
    <italic>node</italic> triggers training of all machines needed to
    call that node, in appropriate order. As mentioned earlier, training
    such a node is “smart” in the sense that hyper-parameter changes to
    a model only trigger retraining of necessary machines. So, for
    example, there is no need to retrain the Box Cox transformer in the
    preceding example if only the ridge regressor hyper-parameters have
    changed.</p>
    <p>The syntax, then, for specifying the learning network shown
    <xref alt="Figure 2" rid="fig2">Figure 2</xref> looks like this:</p>
    <code language="julia">X = source(X_dummy)        # or just source()
y = source(y_dummy)        # or just source()

machine1 = machine(box_cox, y)
z = transform(machine1, y)

machine2 = machine(PCA, X)
Xr = transform(machine2, X)

machine3 = machine(ridge_regressor, Xr, z)
ẑ = predict(machine3, Xr)

ŷ = inverse_transform(machine1, ẑ)

fit!(ŷ)  # to test training on the dummy data
ŷ()      # to test prediction on the dummy data</code>
    <p>Note that the machine syntax is a mechanism allowing for multiple
    nodes to point to the same learned parameters of a model, as in the
    learned target transformation/inverse transformation above. They
    also allow multiple nodes to share the same model (hyper-parameters)
    as in homogeneous ensembles. And different nodes can be accessed
    during training and “prediction” modes of operation, as in
    stacking.</p>
  </sec>
  <sec id="export">
    <title>Export</title>
    <p>In the second step of model composition, the learning network is
    “exported” as a new stand-alone composite model type, with the
    component models appearing in the learning network becoming default
    values for corresponding hyper-parameters of the composite. This new
    type (which is unattached to any particular data) can be
    instantiated and used just like any other MLJ model (tuned,
    evaluated, etc). Under the hood, training such a model builds a
    learning network, so that training is “smart”. Defining a new
    composite model type requires generating and evaluating code, but
    this is readily implemented using Julia’s meta-programming tools,
    i.e., executed by the user with a simple macro call.</p>
  </sec>
</sec>
<sec id="future-directions">
  <title>Future directions</title>
  <p>There are plans to: (i) grow the number of models; (ii) enhance
  core functionality, particularly around hyper-parameter optimization
  (<xref alt="A. Blaom &amp; collaborators, 2020" rid="ref-MLJTuning" ref-type="bibr">A.
  Blaom &amp; collaborators, 2020</xref>); and (iii) broaden scope,
  particularly around probabilistic programming models, time series,
  sparse data and natural language processing. A more comprehensive road
  map is linked from the MLJ repository
  (<xref alt="A. et al. Blaom, 2019" rid="ref-MLJ" ref-type="bibr">A. et
  al. Blaom, 2019</xref>).</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We acknowledge valuable conversations with Avik Sengupta, Mike
  Innes, mlr author Bernd Bischl, and IQVIA’s Yaqub Alwan and Gwyn
  Jones. Seed funding for the MLJ project has been provided by the Alan
  Turing Institute’s Tools, Practices and Systems programme, with
  special thanks to Dr James Hethering, its former Programme Director,
  and Katrina Payne. Mathematics for Real-World Systems Centre for
  Doctoral Training at the University of Warwick provided funding for
  students exploring the Julia ML ecosystem, who created an initial
  proof-of-concept.</p>
  <p><bold>Code contributors.</bold> D. Aluthge, D. Arenas, E. Barp, C.
  Bieganek, A. Blaom, G. Bohner, M. K. Borregaard, D. Buchaca, V.
  Churavy, H. Devereux, M. Giordano, J. Hoffimann, T. Lienart, M. Nook,
  Z. Nugent, S. Okon, P. Oleśkiewicz, J. Samaroo, A. Shridar, Y.
  Simillides, A. Stechemesser, S. Vollmer</p>
  <disp-quote>
  </disp-quote>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Holmes1994">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Holmes</surname><given-names>G.</given-names></name>
          <name><surname>Donkin</surname><given-names>A.</given-names></name>
          <name><surname>Witten</surname><given-names>I. H.</given-names></name>
        </person-group>
        <article-title>WEKA: A machine learning workbench</article-title>
        <source>Proceedings of ANZIIS ’94 - australian new zealnd intelligent information systems conference</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="1994">1994</year>
        <isbn>0-7803-2404-8</isbn>
        <uri>http://ieeexplore.ieee.org/document/396988/</uri>
        <pub-id pub-id-type="doi">10.1109/ANZIIS.1994.396988</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Gressmann2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gressmann</surname><given-names>Frithjof</given-names></name>
          <name><surname>Király</surname><given-names>Franz J.</given-names></name>
          <name><surname>Mateen</surname><given-names>Bilal</given-names></name>
          <name><surname>Oberhauser</surname><given-names>Harald</given-names></name>
        </person-group>
        <article-title>Probabilistic supervised learning</article-title>
        <source>ArXiv</source>
        <year iso-8601-date="2018">2018</year>
        <volume>1801.00753</volume>
      </element-citation>
    </ref>
    <ref id="ref-Pedregosa2001">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
          <name><surname>Varoquaux</surname><given-names>Gaël</given-names></name>
          <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
          <name><surname>Michel</surname><given-names>Vincent</given-names></name>
          <name><surname>Thirion</surname><given-names>Bertrand</given-names></name>
          <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
          <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
          <name><surname>Weiss</surname><given-names>Ron</given-names></name>
          <name><surname>Dubourg</surname><given-names>Vincent</given-names></name>
          <name><surname>Vanderplas</surname><given-names>Jake</given-names></name>
          <name><surname>Passos</surname><given-names>Alexandre</given-names></name>
          <name><surname>Cournapeau</surname><given-names>David</given-names></name>
          <name><surname>Brucher</surname><given-names>Matthieu</given-names></name>
          <name><surname>Perrot</surname><given-names>Matthieu</given-names></name>
          <name><surname>Duchesnay</surname><given-names>Édouard</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine Learning in Python</article-title>
        <source>The Journal of Machine Learning Research</source>
        <publisher-name>MIT Press</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
        <uri>https://dl.acm.org/doi/10.5555/1953048.2078195</uri>
      </element-citation>
    </ref>
    <ref id="ref-Buitinck2013">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Buitinck</surname><given-names>Lars</given-names></name>
          <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
          <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
          <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
          <name><surname>Mueller</surname><given-names>Andreas</given-names></name>
          <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
          <name><surname>Niculae</surname><given-names>Vlad</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
          <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
          <name><surname>Grobler</surname><given-names>Jaques</given-names></name>
          <name><surname>Layton</surname><given-names>Robert</given-names></name>
          <name><surname>VanderPlas</surname><given-names>Jacob</given-names></name>
          <name><surname>Joly</surname><given-names>Arnaud</given-names></name>
          <name><surname>Holt</surname><given-names>Brian</given-names></name>
          <name><surname>Varoquaux</surname><given-names>Gaël</given-names></name>
        </person-group>
        <article-title>API design for machine learning software: experiences from the scikit-learn project</article-title>
        <source>ArXiv</source>
        <year iso-8601-date="2013">2013</year>
        <volume>abs/1309.0238</volume>
      </element-citation>
    </ref>
    <ref id="ref-BischlEtal2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bischl</surname><given-names>Bernd</given-names></name>
          <name><surname>Lang</surname><given-names>Michel</given-names></name>
          <name><surname>Kotthoff</surname><given-names>Lars</given-names></name>
          <name><surname>Schiffner</surname><given-names>Julia</given-names></name>
          <name><surname>Richter</surname><given-names>Jakob</given-names></name>
          <name><surname>Studerus</surname><given-names>Erich</given-names></name>
          <name><surname>Casalicchio</surname><given-names>Giuseppe</given-names></name>
          <name><surname>Jones</surname><given-names>Zachary M.</given-names></name>
        </person-group>
        <article-title>mlr: Machine Learning in R</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2016">2016</year>
        <volume>17</volume>
        <issue>170</issue>
        <uri>http://jmlr.org/papers/v17/15-066.html</uri>
      </element-citation>
    </ref>
    <ref id="ref-BezansonEtal2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
          <name><surname>Edelman</surname><given-names>Alan</given-names></name>
          <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
          <name><surname>Shah</surname><given-names>Viral B.</given-names></name>
        </person-group>
        <article-title>Julia: A fresh approach to numerical computing</article-title>
        <source>SIAM Rev.</source>
        <year iso-8601-date="2017">2017</year>
        <volume>59</volume>
        <issue>1</issue>
        <issn>0036-1445</issn>
        <uri>https://doi.org/10.1137/141000671</uri>
        <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Blaom_I">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Blaom</surname><given-names>Anthony</given-names></name>
        </person-group>
        <article-title>Flexible model composition in machine learning and its implementation in MLJ</article-title>
        <source></source>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-Innes2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Innes</surname><given-names>Mike</given-names></name>
        </person-group>
        <article-title>Flux: Elegant machine learning with Julia</article-title>
        <source>Journal of Open Source Software</source>
        <publisher-name>The Open Journal</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>3</volume>
        <issue>25</issue>
        <uri>https://doi.org/10.21105/joss.00602</uri>
        <pub-id pub-id-type="doi">10.21105/joss.00602</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Kuhn2008">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kuhn</surname><given-names>Max</given-names></name>
        </person-group>
        <article-title>Building predictive models in R using the caret package</article-title>
        <source>Journal of Statistical Software, Articles</source>
        <year iso-8601-date="2008">2008</year>
        <volume>28</volume>
        <issue>5</issue>
        <issn>1548-7660</issn>
        <uri>https://www.jstatsoft.org/v028/i05</uri>
        <pub-id pub-id-type="doi">10.18637/jss.v028.i05</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-LinEtal2020">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Lin</surname><given-names>Dahua</given-names></name>
          <name><surname>White</surname><given-names>John Myles</given-names></name>
          <name><surname>Byrne</surname><given-names>Simon</given-names></name>
          <name><surname>Noack</surname><given-names>Andreas</given-names></name>
          <name><surname>Besançon</surname><given-names>Mathieu</given-names></name>
          <name><surname>Bates</surname><given-names>Douglas</given-names></name>
          <name><surname>Pearson</surname><given-names>John</given-names></name>
          <name><surname>Arslan</surname><given-names>Alex</given-names></name>
          <name><surname>Squire</surname><given-names>Kevin</given-names></name>
          <name><surname>Anthoff</surname><given-names>David</given-names></name>
          <name><surname>Zito</surname><given-names>John</given-names></name>
          <name><surname>Papamarkou</surname><given-names>Theodore</given-names></name>
          <name><surname>Schauer</surname><given-names>Moritz</given-names></name>
          <name><surname>Drugowitsch</surname><given-names>Jan</given-names></name>
          <name><surname>Sengupta</surname><given-names>Avik</given-names></name>
          <name><surname>Smith</surname><given-names>Brian J</given-names></name>
          <name><surname>Moynihan</surname><given-names>Glenn</given-names></name>
          <name><surname>Ragusa</surname><given-names>Giuseppe</given-names></name>
          <name><surname>Stephen</surname><given-names>Gord</given-names></name>
          <name><surname>Dann</surname><given-names>Christoph</given-names></name>
          <name><surname>Innes</surname><given-names>Mike J</given-names></name>
          <name><surname>Michael</surname></name>
          <name><surname>O’Leary</surname><given-names>Martin</given-names></name>
          <name><surname>Papp</surname><given-names>Tamas K.</given-names></name>
          <name><surname>Chen</surname><given-names>Jiahao</given-names></name>
          <name><surname>Dunning</surname><given-names>Iain</given-names></name>
          <name><surname>Lacerda</surname><given-names>Gustavo</given-names></name>
          <name><surname>Reeve</surname><given-names>Richard</given-names></name>
          <name><surname>Xu</surname><given-names>Kai</given-names></name>
          <name><surname>Widmann</surname><given-names>David</given-names></name>
        </person-group>
        <source>JuliaStats/distributions.jl: A Julia package for probability distributions and associated functions</source>
        <publisher-name>Zenodo</publisher-name>
        <year iso-8601-date="2020-03">2020</year><month>03</month>
        <uri>http://doi.org/10.5281/zenodo.3730565</uri>
        <pub-id pub-id-type="doi">10.5281/zenodo.3730565</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-CategoricalArrays">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Bouchet-Valat</surname><given-names>Milan et al.</given-names></name>
        </person-group>
        <article-title>CategoricalArrays.jl: Arrays for working with categorical data</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <uri>https://github.com/JuliaData/CategoricalArrays.jl</uri>
      </element-citation>
    </ref>
    <ref id="ref-MLJ">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Blaom</surname><given-names>Anthony et al.</given-names></name>
        </person-group>
        <article-title>MLJ: A machine learning framework for Julia</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://github.com/alan-turing-institute/MLJ.jl</uri>
      </element-citation>
    </ref>
    <ref id="ref-MLJdocs">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Blaom</surname><given-names>Anthony</given-names></name>
        </person-group>
        <article-title>MLJ documentation</article-title>
        <source>GitHub pages</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <uri>https://alan-turing-institute.github.io/MLJ.jl/dev/</uri>
      </element-citation>
    </ref>
    <ref id="ref-MLJTuning">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Blaom</surname><given-names>Anthony</given-names></name>
          <name><surname>collaborators</surname></name>
        </person-group>
        <article-title>Hyperparameter optimization algorithms for use in the MLJ machine learning framework</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <uri>https://github.com/alan-turing-institute/MLJTuning.jl</uri>
      </element-citation>
    </ref>
    <ref id="ref-MLJtutorials">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Lienart</surname><given-names>Thibaut</given-names></name>
          <name><surname>Blaom</surname><given-names>Anthony</given-names></name>
          <name><surname>collaborators</surname></name>
        </person-group>
        <article-title>Data science tutorials in Julia</article-title>
        <source>GitHub pages</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <uri>https://alan-turing-institute.github.io/DataScienceTutorials.jl/</uri>
      </element-citation>
    </ref>
    <ref id="ref-Rackauckas2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Rackauckas</surname><given-names>Christopher</given-names></name>
        </person-group>
        <article-title>A comparison between differential equation solver suites in MATLAB, r, julia, python, c, mathematica, maple, and fortran</article-title>
        <source>The Winnower</source>
        <year iso-8601-date="2018-08">2018</year><month>08</month>
        <uri>https://dx.doi.org/10.15200/winn.153459.98975</uri>
        <pub-id pub-id-type="doi">10.15200/winn.153459.98975</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-RackauckasNie2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Rackauckas</surname><given-names>C.</given-names></name>
          <name><surname>Nie</surname><given-names>Q.</given-names></name>
        </person-group>
        <article-title>DifferentialEquations.jl – A performant and feature-rich ecosystem for solving differential equations in Julia</article-title>
        <source>Journal of Open Research Software</source>
        <year iso-8601-date="2017">2017</year>
        <volume>5</volume>
        <issue>1</issue>
        <uri>https://openresearchsoftware.metajnl.com/articles/10.5334/jors.151/</uri>
        <pub-id pub-id-type="doi">10.5334/jors.151</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-ScientificTypes">
      <element-citation>
        <person-group person-group-type="author">
          <string-name>Anthony Blaom and collaborators</string-name>
        </person-group>
        <article-title>ScientificTypes.jl: An API for dispatching on the &quot;scientific&quot; type of data instead of the machine type</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://github.com/alan-turing-institute/ScientificTypes.jl</uri>
      </element-citation>
    </ref>
    <ref id="ref-Quinn">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Quinn</surname><given-names>J.</given-names></name>
        </person-group>
        <article-title>Tables.jl: An interface for tables in Julia</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <uri>https://github.com/JuliaData/Tables.jl</uri>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
