<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">431</article-id>
<article-id pub-id-type="doi">10.21105/joss.00431</article-id>
<title-group>
<article-title>pyGPGO: Bayesian Optimization for Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5335-7834</contrib-id>
<string-name>José Jiménez</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-9521-9635</contrib-id>
<string-name>Josep Ginebra</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Computational Biophysics Laboratory, Universitat Pompeu
Fabra, Parc de Recerca Biomèdica de Barcelona, Carrer del Dr. Aiguader
88. Barcelona 08003, Spain.</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Statistics and Operations Research.
Universitat Politècnica de Catalunya (UPC). Av. Diagonal 647, Barcelona
08028, Spain.</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2017-09-04">
<day>4</day>
<month>9</month>
<year>2017</year>
</pub-date>
<volume>2</volume>
<issue>19</issue>
<fpage>431</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>machine-learning</kwd>
<kwd>bayesian</kwd>
<kwd>optimization</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Bayesian optimization has risen over the last few years as a very
  attractive method to optimize expensive to evaluate, black box,
  derivative-free and possibly noisy functions
  (<xref alt="Shahriari et al., 2016" rid="ref-Shahriari2016" ref-type="bibr">Shahriari
  et al., 2016</xref>). This framework uses <italic>surrogate
  models</italic>, such as the likes of a Gaussian Process
  (<xref alt="Rasmussen &amp; Williams, 2004" rid="ref-Rasmussen2004" ref-type="bibr">Rasmussen
  &amp; Williams, 2004</xref>) which describe a prior belief over the
  possible objective functions in order to approximate them. The
  procedure itself is inherently sequential: our function is first
  evaluated a few times, a surrogate model is then fit with this
  information, which will later suggest the next point to be evaluated
  according to a predefined <italic>acquisition function</italic>. These
  strategies typically aim to balance exploitation and exploration, that
  is, areas where the posterior mean or variance of our surrogate model
  are high respectively.</p>
  <p>These strategies have recently grabbed the attention of machine
  learning researchers over simpler black-box optimization strategies,
  such as grid search or random search
  (<xref alt="Bergstra James &amp; Bengio Yoshua, 2012" rid="ref-Bergstra2012" ref-type="bibr">Bergstra
  James &amp; Bengio Yoshua, 2012</xref>). It is specially interesting
  in areas such as automatic machine-learning hyperparameter
  optimization
  (<xref alt="Snoek et al., 2012" rid="ref-Snoek2012" ref-type="bibr">Snoek
  et al., 2012</xref>), A/B testing
  (<xref alt="Chapelle &amp; Li, 2011" rid="ref-Chapelle2011" ref-type="bibr">Chapelle
  &amp; Li, 2011</xref>) or recommender systems
  (<xref alt="Vanchinathan et al., 2014" rid="ref-Vanchinathan2014" ref-type="bibr">Vanchinathan
  et al., 2014</xref>), among others. Furthermore, the framework is
  entirely modular; there are many choices a user could take regarding
  the design of the optimization procedure: choice of surrogate model,
  covariance function, acquisition function behaviour or hyperparameter
  treatment, to name a few.</p>
  <p>Here we present <italic>pyGPGO</italic> , an open-source Python
  package for Bayesian Optimization, which embraces this modularity in
  its design. While additional Python packages exist for the same
  purpose, either they are restricted for non-commercial applications
  (<xref alt="Snoek, 2012" rid="ref-SpearmintSnoek2012" ref-type="bibr">Snoek,
  2012</xref>), implement a small subset of the features
  (<xref alt="Yelp, 2014" rid="ref-yelpmoe" ref-type="bibr">Yelp,
  2014</xref>), or do not provide a modular interface
  (<xref alt="team., 2016" rid="ref-scikitoptimize" ref-type="bibr">team.,
  2016</xref>). <italic>pyGPGO</italic> on the other hand aims to
  provide the highest degree of freedom in the design and inference of a
  Bayesian optimization pipeline, while being feature-wise competitive
  with other existing software. <italic>pyGPGO</italic> currently
  supports:</p>
  <list list-type="bullet">
    <list-item>
      <p>Different surrogate models: Gaussian Processes, Student-t
      Processes, Random Forests (&amp; variants) and Gradient Boosting
      Machines.</p>
    </list-item>
    <list-item>
      <p>Most usual covariance function structures, as well as their
      derivatives: squared exponential, Matèrn, gamma-exponential,
      rational-quadratic, exponential-sine and dot-product kernel.</p>
    </list-item>
    <list-item>
      <p>Several acquisition function behaviours: probability of
      improvement, expected improvement, upper confidence bound and
      entropy-based, as well as their integrated versions.</p>
    </list-item>
    <list-item>
      <p>Type II maximum-likelihood estimation of covariance
      hyperparameters.</p>
    </list-item>
    <list-item>
      <p>MCMC sampling for the full-bayesian treatment of
      hyperparameters (via <monospace>pyMC3</monospace>
      (<xref alt="Salvatier et al., 2016" rid="ref-Salvatier2016" ref-type="bibr">Salvatier
      et al., 2016</xref>))</p>
    </list-item>
  </list>
  <p><italic>pyGPGO</italic> is MIT-licensed and can be retrieved from
  both
  <ext-link ext-link-type="uri" xlink:href="https://github.com/hawk31/pyGPGO">GitHub</ext-link>
  and
  <ext-link ext-link-type="uri" xlink:href="https://pypi.python.org/pypi/pyGPGO/">PyPI</ext-link>,
  with extensive documentation available at
  <ext-link ext-link-type="uri" xlink:href="http://pygpgo.readthedocs.io/en/latest/">ReadTheDocs</ext-link>.
  <italic>pyGPGO</italic> is built on top of other well known packages
  of the Python scientific ecosystem as dependencies, such as numpy,
  scikit-learn, pyMC3 and theano.</p>
  <fig>
    <caption><p>pyGPGO in action.</p></caption>
    <graphic mimetype="image" mime-subtype="gif" xlink:href="franke.gif" xlink:title="" />
  </fig>
</sec>
<sec id="future-work">
  <title>Future work</title>
  <p><italic>pyGPGO</italic> is an ongoing project, and as such there
  are several improvements that will be tackled in the near future:</p>
  <list list-type="bullet">
    <list-item>
      <p>Support for linear combinations of covariance functions, with
      automatic gradient computation.</p>
    </list-item>
    <list-item>
      <p>Support for more diverse acquisition functions, such as
      Predictive Entropy Search
      (<xref alt="Hernández-Lobato et al., 2014" rid="ref-Hernandez-Lobato2014" ref-type="bibr">Hernández-Lobato
      et al., 2014</xref>).</p>
    </list-item>
    <list-item>
      <p>A class for constrained Bayesian Optimization is planned for
      the near future.
      (<xref alt="Gardner et al., 2014" rid="ref-Gardner2014" ref-type="bibr">Gardner
      et al., 2014</xref>)</p>
    </list-item>
  </list>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Shahriari2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shahriari</surname><given-names>Bobak</given-names></name>
          <name><surname>Swersky</surname><given-names>Kevin</given-names></name>
          <name><surname>Wang</surname><given-names>Ziyu</given-names></name>
          <name><surname>Adams</surname><given-names>Ryan P.</given-names></name>
          <name><surname>De Freitas</surname><given-names>Nando</given-names></name>
        </person-group>
        <article-title>Taking the human out of the loop: A review of Bayesian optimization</article-title>
        <source>Proceedings of the IEEE</source>
        <year iso-8601-date="2016">2016</year>
        <volume>104</volume>
        <issue>1</issue>
        <isbn>0018-9219</isbn>
        <issn>00189219</issn>
        <uri>https://arxiv.org/abs/arXiv:1011.1669v3</uri>
        <pub-id pub-id-type="doi">10.1109/JPROC.2015.2494218</pub-id>
        <pub-id pub-id-type="pmid">25246403</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Snoek2012">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Snoek</surname><given-names>Jasper</given-names></name>
          <name><surname>Larochelle</surname><given-names>Hugo</given-names></name>
          <name><surname>Adams</surname><given-names>Rp</given-names></name>
        </person-group>
        <article-title>Practical Bayesian optimization of machine learning algorithms</article-title>
        <source>Advances in Neural Information …</source>
        <year iso-8601-date="2012">2012</year>
        <isbn>9781627480031</isbn>
        <issn>10495258</issn>
        <uri>http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms</uri>
        <pub-id pub-id-type="doi">2012arXiv1206.2944S</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Bergstra2012">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bergstra James</surname><given-names>James</given-names></name>
          <name><surname>Bengio Yoshua</surname><given-names>Umontrealca</given-names></name>
        </person-group>
        <article-title>Random Search for Hyper-Parameter Optimization</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2012">2012</year>
        <volume>13</volume>
        <isbn>1532-4435</isbn>
        <issn>1532-4435</issn>
        <uri>https://arxiv.org/abs/1504.05070</uri>
        <pub-id pub-id-type="doi">10.1162/153244303322533223</pub-id>
        <pub-id pub-id-type="pmid">18244602</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-SpearmintSnoek2012">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Snoek</surname><given-names>Jasper</given-names></name>
        </person-group>
        <article-title>Spearmint</article-title>
        <year iso-8601-date="2012">2012</year>
        <uri>https://github.com/HIPS/Spearmint</uri>
      </element-citation>
    </ref>
    <ref id="ref-scikitoptimize">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>team.</surname><given-names>The scikit-optimize</given-names></name>
        </person-group>
        <article-title>Scikit-optimize</article-title>
        <year iso-8601-date="2016">2016</year>
        <uri>https://github.com/scikit-optimize/scikit-optimize</uri>
      </element-citation>
    </ref>
    <ref id="ref-yelpmoe">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Yelp</surname></name>
        </person-group>
        <article-title>MOE</article-title>
        <year iso-8601-date="2014">2014</year>
        <uri>https://github.com/Yelp/MOE</uri>
      </element-citation>
    </ref>
    <ref id="ref-Hernandez-Lobato2014">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hernández-Lobato</surname><given-names>José Miguel</given-names></name>
          <name><surname>Hoffman</surname><given-names>Matthew W</given-names></name>
          <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
        </person-group>
        <article-title>Predictive Entropy Search for Efficient Global Optimization of Black-box Functions</article-title>
        <source>Advances in Neural Information Processing Systems 28</source>
        <year iso-8601-date="2014">2014</year>
        <issn>10495258</issn>
        <uri>https://jmhldotorg.files.wordpress.com/2014/10/pes-final.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-Gardner2014">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gardner</surname><given-names>Jacob R.</given-names></name>
          <name><surname>Kusner</surname><given-names>Matt J.</given-names></name>
          <name><surname>Xu</surname><given-names>Zhixiang Eddie</given-names></name>
          <name><surname>Weinberger</surname><given-names>Kilian Q.</given-names></name>
          <name><surname>Cunningham</surname><given-names>John P.</given-names></name>
        </person-group>
        <article-title>Bayesian Optimization with Inequality Constraints</article-title>
        <source>Proceedings of the 31st International Conference on Machine Learning</source>
        <year iso-8601-date="2014">2014</year>
        <volume>32</volume>
        <isbn>9781634393973</isbn>
      </element-citation>
    </ref>
    <ref id="ref-Chapelle2011">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Chapelle</surname><given-names>Olivier</given-names></name>
          <name><surname>Li</surname><given-names>Lihong</given-names></name>
        </person-group>
        <article-title>An Empirical Evaluation of Thompson Sampling</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <year iso-8601-date="2011">2011</year>
        <isbn>9781618395993</isbn>
        <uri>http://explo.cs.ucl.ac.uk/wp-content/uploads/2011/05/An-Empirical-Evaluation-of-Thompson-Sampling-Chapelle-Li-2011.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-Vanchinathan2014">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Vanchinathan</surname><given-names>Hastagiri P.</given-names></name>
          <name><surname>Nikolic</surname><given-names>Isidor</given-names></name>
          <name><surname>De Bona</surname><given-names>Fabio</given-names></name>
          <name><surname>Krause</surname><given-names>Andreas</given-names></name>
        </person-group>
        <article-title>Explore-exploit in top-N recommender systems via Gaussian processes</article-title>
        <source>Proceedings of the 8th ACM conference on recommender systems - RecSys ’14</source>
        <year iso-8601-date="2014">2014</year>
        <isbn>9781450326681</isbn>
        <uri>http://dl.acm.org/citation.cfm?doid=2645710.2645733</uri>
        <pub-id pub-id-type="doi">10.1145/2645710.2645733</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Salvatier2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Salvatier</surname><given-names>J</given-names></name>
          <name><surname>Wiecki</surname><given-names>TV</given-names></name>
          <name><surname>C.</surname><given-names>Fonnesbeck</given-names></name>
        </person-group>
        <article-title>Probabilistic programming in Python using PyMC3</article-title>
        <source>PeerJ Computer Science</source>
        <year iso-8601-date="2016">2016</year>
        <uri>https://doi.org/10.7717/peerj-cs.55</uri>
      </element-citation>
    </ref>
    <ref id="ref-Rasmussen2004">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Rasmussen</surname><given-names>Carl E.</given-names></name>
          <name><surname>Williams</surname><given-names>Christopher K. I.</given-names></name>
        </person-group>
        <source>Gaussian processes for machine learning.</source>
        <year iso-8601-date="2004">2004</year>
        <volume>14</volume>
        <isbn>026218253X</isbn>
        <issn>0129-0657</issn>
        <uri>http://www.gaussianprocess.org/gpml/chapters/RW.pdf</uri>
        <pub-id pub-id-type="doi">10.1142/S0129065704001899</pub-id>
        <pub-id pub-id-type="pmid">15112367</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
