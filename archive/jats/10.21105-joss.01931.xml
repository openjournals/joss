<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1931</article-id>
<article-id pub-id-type="doi">10.21105/joss.01931</article-id>
<title-group>
<article-title>NeuroDiffEq: A Python package for solving differential
equations with neural networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-0476-9998</contrib-id>
<string-name>Feiyu Chen</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>David Sondak</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Pavlos Protopapas</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Marios Mattheakis</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Shuheng Liu</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Devansh Agarwal</string-name>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<string-name>Marco Di Giovanni</string-name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Institute for Applied Computational Science, Harvard
University, Cambridge, MA, United States</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Chongqing University, Chongqing, China</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Physics and Astronomy, Virginia University,
Morgantown, WV, United States</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Center for Gravitational Waves and Cosmology, West Virginia
University, Morgantown, WV, United States</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Politecnico di Milano, Milano, Italy</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-10-31">
<day>31</day>
<month>10</month>
<year>2019</year>
</pub-date>
<volume>5</volume>
<issue>46</issue>
<fpage>1931</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>differential equations</kwd>
<kwd>neural networks</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Differential equations emerge in various scientific and engineering
  domains for modeling physical phenomena. Most differential equations
  of practical interest are analytically intractable. Traditionally,
  differential equations are solved by numerical methods. Sophisticated
  algorithms exist to integrate differential equations in time and
  space. Time integration techniques continue to be an active area of
  research and include backward difference formulas and Runge-Kutta
  methods
  (<xref alt="Conde et al., 2017" rid="ref-conde2017implicit" ref-type="bibr">Conde
  et al., 2017</xref>). Common spatial discretization approaches include
  the finite difference method (FDM), finite volume method (FVM), and
  finite element method (FEM) as well as spectral methods such as the
  Fourier-spectral method. These classical methods have been studied in
  detail and much is known about their convergence properties. Moreover,
  highly optimized codes exist for solving differential equations of
  practical interest with these techniques
  (<xref alt="Seefeldt et al., 2017" rid="ref-seefeldt2017drekar" ref-type="bibr">Seefeldt
  et al., 2017</xref>;
  <xref alt="Smith &amp; Abeysinghe, 2017" rid="ref-smith2017phasta" ref-type="bibr">Smith
  &amp; Abeysinghe, 2017</xref>). While these methods are efficient and
  well-studied, their expressibility is limited by their function
  representation.</p>
  <p>Artificial neural networks (ANN) are a framework of machine
  learning algorithms that use a collection of connected units to learn
  function mappings. The most basic form of ANNs, multilayer
  perceptrons, have been proven to be universal function approximators
  (<xref alt="Hornik et al., 1989" rid="ref-hornik1989multilayer" ref-type="bibr">Hornik
  et al., 1989</xref>). This suggests the possibility of using ANNs to
  solve differential equations. Previous studies have demonstrated that
  ANNs have the potential to solve ordinary differential equations
  (ODEs) and partial differential equations (PDEs) with certain
  initial/boundary conditions
  (<xref alt="Lagaris et al., 1998" rid="ref-lagaris1998artificial" ref-type="bibr">Lagaris
  et al., 1998</xref>). These methods show nice properties including (1)
  continuous and differentiable solutions, (2) good interpolation
  properties, and (3) less memory-intensive. By less memory-intensive we
  mean that only the weights of the neural network have to be stored.
  The solution can then be recovered anywhere in the solution domain
  because a trained neural network is a closed form solution. Given the
  interest in developing neural networks for solving differential
  equations, it would be extremely beneficial to have an easy-to-use
  software package that allows researchers to quickly set up and solve
  problems.</p>
  <p><monospace>NeuroDiffEq</monospace> is a Python package built with
  <monospace>PyTorch</monospace>
  (<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>) that uses ANNs to solve ordinary and partial
  differential equations (ODEs and PDEs). During the release of
  <monospace>NeuroDiffEq</monospace> we discovered that two other groups
  had almost simultaneously released their own software packages for
  solving differential equations with neural networks:
  <monospace>DeepXDE</monospace>
  (<xref alt="Lu et al., 2019" rid="ref-lu2019deepxde" ref-type="bibr">Lu
  et al., 2019</xref>) and <monospace>PyDEns</monospace>
  (<xref alt="Koryagin et al., 2019" rid="ref-koryagin2019pydens" ref-type="bibr">Koryagin
  et al., 2019</xref>). Both <monospace>DeepXDE</monospace> and
  <monospace>PyDEns</monospace> are built on top of
  <monospace>TensorFlow</monospace>
  (<xref alt="Abadi et al., 2015" rid="ref-tensorflow2015-whitepaper" ref-type="bibr">Abadi
  et al., 2015</xref>). <monospace>DeepXDE</monospace> has an emphasis
  on the wide variety of problems it can solve. It supports mixing
  different boundary conditions and solving on domains with complex
  geometries. <monospace>PyDEns</monospace> is less flexible in the
  range of solvable problems but provides a more user-friendly API. This
  trade-off is partially determined by the way these two packages
  implement the solver, which will be discussed later.</p>
  <p><monospace>NeuroDiffEq</monospace> is designed to encourage the
  user to focus more on the problem domain (What is the differential
  equation we need to solve? What are the initial/boundary conditions?)
  and at the same time allow them to dig into solution domain (What ANN
  architecture and loss function should be used? What are the training
  hyperparameters?) when they want to.
  <monospace>NeuroDiffEq</monospace> can solve a variety of canonical
  PDEs including the heat equation and Poisson equation in a Cartesian
  domain with up to two spatial dimensions. We are actively working on
  extending <monospace>NeuroDiffEq</monospace> to support three spatial
  dimensions. <monospace>NeuroDiffEq</monospace> can also solve
  arbitrary systems of nonlinear ordinary differential equations.
  Currently, <monospace>NeuroDiffEq</monospace> is being used in a
  variety of research projects including to study the convergence
  properties of ANNs for solving differential equations as well as
  solving the equations in the field of general relativity (Schwarzchild
  and Kerr black holes).</p>
</sec>
<sec id="methods">
  <title>Methods</title>
  <p>The key idea of solving differential equations with ANNs is to
  reformulate the problem as an optimization problem in which we
  minimize the residual of the differential equations. In a very general
  sense, a differential equation can be expressed as
  <disp-formula><alternatives>
  <tex-math><![CDATA[\mathcal{L}u - f = 0]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="script"><mml:mi>ℒ</mml:mi></mml:mstyle><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{L}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℒ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  is the differential operator, <inline-formula><alternatives>
  <tex-math><![CDATA[u\left(x,t\right)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the solution that we wish to find, and
  <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  is a known forcing function. We denote the output of the neural
  network as <inline-formula><alternatives>
  <tex-math><![CDATA[u_{N}\left(x, t; p\right)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  where the parameter vector <inline-formula><alternatives>
  <tex-math><![CDATA[p]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>p</mml:mi></mml:math></alternatives></inline-formula>
  is a vector containing the weights and biases of the neural network.
  We will drop the arguments of the neural network solution in order to
  be concise. If <inline-formula><alternatives>
  <tex-math><![CDATA[u_{N}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  is a solution to the differential equation, then the residual
  <disp-formula><alternatives>
  <tex-math><![CDATA[\mathcal{R}\left(u_{N}\right) = \mathcal{L}u_{N} - f ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="script"><mml:mi>ℛ</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant="script"><mml:mi>ℒ</mml:mi></mml:mstyle><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  will be identically zero. One way to incorporate this into the
  training process of a neural network is to use the residual as the
  loss function. In general, the <inline-formula><alternatives>
  <tex-math><![CDATA[L^{2}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  loss of the residual is used. This is the convention that
  <monospace>NeuroDiffEq</monospace> follows, although we note that
  other loss functions could be conceived. Solving the differential
  equation is re-cast as the following optimization problem:
  <disp-formula><alternatives>
  <tex-math><![CDATA[
  \min_{p}\left(\mathcal{L}u_{N} - f\right)^2.
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="script"><mml:mi>ℒ</mml:mi></mml:mstyle><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>It is necessary to inform the neural network about any boundary and
  initial conditions since it has no way of enforcing these <italic>a
  priori</italic>. There are two primary ways to satisfy the boundary
  and initial conditions. First, one can impose the initial/boundary
  conditions in the loss function. For example, given an initial
  condition <inline-formula><alternatives>
  <tex-math><![CDATA[u\left(x,t_{0}\right) = u_{0}\left(x\right)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  the loss function can be modified to: <disp-formula><alternatives>
  <tex-math><![CDATA[
  \min_{p}\left[\left(\mathcal{L}u_{N} - f\right)^2 + \lambda\left(u_{N}\left(x,t_{0}\right) - u_0\left(x\right)\right)^2\right]
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="script"><mml:mi>ℒ</mml:mi></mml:mstyle><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
  where the second term penalizes solutions that don’t satisfy the
  initial condition. Larger values of the regularization parameter
  <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
  result in stricter satisfaction of the initial condition while
  sacrificing solution accuracy. However, this approach does not lead to
  <italic>exact</italic> satisfaction of the initial and boundary
  conditions.</p>
  <p>Another option is to transform the <inline-formula><alternatives>
  <tex-math><![CDATA[u_{N}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  in a way such that the initial/boundary conditions are satisfied by
  construction. Given an initial condition
  <inline-formula><alternatives>
  <tex-math><![CDATA[u_{0}\left(x\right)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  the neural network can be transformed according to:
  <disp-formula><alternatives>
  <tex-math><![CDATA[
  \widetilde{u}\left(x,t\right) = u_{0}\left(x\right) + \left(1-e^{-\left(t-t_{0}\right)}\right)u_{N}\left(x,t\right)
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
  so that when <inline-formula><alternatives>
  <tex-math><![CDATA[t = t_0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\widetilde{u}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>u</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
  will always be <inline-formula><alternatives>
  <tex-math><![CDATA[u_0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>.
  Accordingly, the objective function becomes
  <disp-formula><alternatives>
  <tex-math><![CDATA[
  \min_{p}\left(\mathcal{L}\widetilde{u} - f\right)^2.
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="script"><mml:mi>ℒ</mml:mi></mml:mstyle><mml:mover><mml:mi>u</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  This approach is similar to the trial function approach
  (<xref alt="Lagaris et al., 1998" rid="ref-lagaris1998artificial" ref-type="bibr">Lagaris
  et al., 1998</xref>), but with a different form of the trial function.
  Modifying the neural network to account for boundary conditions can
  also be done. In general, the transformed solution will have the form:
  <disp-formula><alternatives>
  <tex-math><![CDATA[
  \widetilde{u}\left(x,t\right) = A\left(x, t; x_{\text{boundary}}, t_{0}\right)u_{N}\left(x,t\right)
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext mathvariant="normal">boundary</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[A\left(x, t; x_{\text{boundary}}, t_{0}\right)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext mathvariant="normal">boundary</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  must be designed so that <inline-formula><alternatives>
  <tex-math><![CDATA[\widetilde{u}\left(x,t\right)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo accent="true">̃</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  has the correct boundary conditions. This can be very challenging for
  complicated domains.</p>
  <p>Both of these two methods have their advantages. The first way is
  simpler to implement and can be more easily extended to
  high-dimensional PDEs and PDEs formulated on complicated domains. The
  second way assures that the initial/boundary conditions are exactly
  satisfied. Considering that differential equations can be sensitive to
  initial/boundary conditions, this is expected to play an important
  role. Another advantage of the second method is that fixing these
  conditions can reduce the effort required during the training of the
  ANN
  (<xref alt="McFall &amp; Mahan, 2009" rid="ref-mcfall2009artificial" ref-type="bibr">McFall
  &amp; Mahan, 2009</xref>). <monospace>DeepXDE</monospace> uses the
  first way to impose initial/boundary conditions.
  <monospace>PyDEns</monospace> uses a variation of the second approach
  to impose initial/boundary conditions.
  <monospace>NeuroDiffEq</monospace>, the software described herein,
  employs the second approach.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-conde2017implicit">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Conde</surname><given-names>Sidafa</given-names></name>
          <name><surname>Gottlieb</surname><given-names>Sigal</given-names></name>
          <name><surname>Grant</surname><given-names>Zachary J</given-names></name>
          <name><surname>Shadid</surname><given-names>John N</given-names></name>
        </person-group>
        <article-title>Implicit and implicit–explicit strong stability preserving runge–kutta methods with high linear order</article-title>
        <source>Journal of Scientific Computing</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>73</volume>
        <issue>2-3</issue>
        <pub-id pub-id-type="doi">10.1063/1.4992752</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hornik1989multilayer">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hornik</surname><given-names>Kurt</given-names></name>
          <name><surname>Stinchcombe</surname><given-names>Maxwell</given-names></name>
          <name><surname>White</surname><given-names>Halbert</given-names></name>
        </person-group>
        <article-title>Multilayer feedforward networks are universal approximators</article-title>
        <source>Neural networks</source>
        <publisher-name>Pergamon Press</publisher-name>
        <year iso-8601-date="1989">1989</year>
        <volume>2</volume>
        <pub-id pub-id-type="doi">10.1016/0893-6080(89)90020-8</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-lagaris1998artificial">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lagaris</surname><given-names>Isaac E</given-names></name>
          <name><surname>Likas</surname><given-names>Aristidis</given-names></name>
          <name><surname>Fotiadis</surname><given-names>Dimitrios I</given-names></name>
        </person-group>
        <article-title>Artificial neural networks for solving ordinary and partial differential equations</article-title>
        <source>IEEE transactions on neural networks</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="1998">1998</year>
        <volume>9</volume>
        <issue>5</issue>
        <pub-id pub-id-type="doi">10.1109/72.712178</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-mcfall2009artificial">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>McFall</surname><given-names>Kevin Stanley</given-names></name>
          <name><surname>Mahan</surname><given-names>James Robert</given-names></name>
        </person-group>
        <article-title>Artificial neural network method for solution of boundary value problems with exact satisfaction of arbitrary boundary conditions</article-title>
        <source>IEEE Transactions on Neural Networks</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>20</volume>
        <issue>8</issue>
        <pub-id pub-id-type="doi">10.1109/TNN.2009.2020735</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-lu2019deepxde">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lu</surname><given-names>Lu</given-names></name>
          <name><surname>Meng</surname><given-names>Xuhui</given-names></name>
          <name><surname>Mao</surname><given-names>Zhiping</given-names></name>
          <name><surname>Karniadakis</surname><given-names>George E</given-names></name>
        </person-group>
        <article-title>DeepXDE: A deep learning library for solving differential equations</article-title>
        <source>arXiv preprint arXiv:1907.04502</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-koryagin2019pydens">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Koryagin</surname><given-names>Alexander</given-names></name>
          <name><surname>Khudorozkov</surname><given-names>Roman</given-names></name>
          <name><surname>Tsimfer</surname><given-names>Sergey</given-names></name>
        </person-group>
        <article-title>PyDEns: A python framework for solving differential equations with neural networks</article-title>
        <source>arXiv preprint arXiv:1909.11544</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-paszke2017automatic">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Paszke</surname><given-names>Adam</given-names></name>
          <name><surname>Gross</surname><given-names>Sam</given-names></name>
          <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
          <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
          <name><surname>Yang</surname><given-names>Edward</given-names></name>
          <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
          <name><surname>Lin</surname><given-names>Zeming</given-names></name>
          <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
          <name><surname>Antiga</surname><given-names>Luca</given-names></name>
          <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        </person-group>
        <article-title>Automatic differentiation in PyTorch</article-title>
        <source>NIPS autodiff workshop</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-seefeldt2017drekar">
      <element-citation publication-type="report">
        <person-group person-group-type="author">
          <name><surname>Seefeldt</surname><given-names>Ben</given-names></name>
          <name><surname>Sondak</surname><given-names>David</given-names></name>
          <name><surname>Hensinger</surname><given-names>David M</given-names></name>
          <name><surname>Phipps</surname><given-names>Eric T</given-names></name>
          <name><surname>Foucar</surname><given-names>James G</given-names></name>
          <name><surname>Pawlowski</surname><given-names>Roger P</given-names></name>
          <name><surname>Cyr</surname><given-names>Eric Christopher</given-names></name>
          <name><surname>Shadid</surname><given-names>John N</given-names></name>
          <name><surname>Smith</surname><given-names>Thomas M</given-names></name>
          <name><surname>Weber</surname><given-names>Paula D</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Drekar v. 2.0</article-title>
        <publisher-name>Sandia National Lab.(SNL-NM), Albuquerque, NM (United States)</publisher-name>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-smith2017phasta">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Smith</surname><given-names>Cameron W</given-names></name>
          <name><surname>Abeysinghe</surname><given-names>Eroma</given-names></name>
        </person-group>
        <article-title>The PHASTA science gateway: Web-based execution of adaptive computational fluid dynamics simulations</article-title>
        <source>Proceedings of the practice and experience in advanced research computing 2017 on sustainability, success and impact</source>
        <publisher-name>ACM</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <pub-id pub-id-type="doi">10.1145/3093338.3104151</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-tensorflow2015-whitepaper">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
          <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
          <name><surname>Barham</surname><given-names>Paul</given-names></name>
          <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
          <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
          <name><surname>Citro</surname><given-names>Craig</given-names></name>
          <name><surname>Corrado</surname><given-names>Greg S.</given-names></name>
          <name><surname>Davis</surname><given-names>Andy</given-names></name>
          <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
          <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
          <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
          <name><surname>Harp</surname><given-names>Andrew</given-names></name>
          <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
          <name><surname>Isard</surname><given-names>Michael</given-names></name>
          <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
          <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
          <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
          <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
          <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
          <name><surname>Mané</surname><given-names>Dan</given-names></name>
          <name><surname>Monga</surname><given-names>Rajat</given-names></name>
          <name><surname>Moore</surname><given-names>Sherry</given-names></name>
          <name><surname>Murray</surname><given-names>Derek</given-names></name>
          <name><surname>Olah</surname><given-names>Chris</given-names></name>
          <name><surname>Schuster</surname><given-names>Mike</given-names></name>
          <name><surname>Shlens</surname><given-names>Jonathon</given-names></name>
          <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
          <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
          <name><surname>Talwar</surname><given-names>Kunal</given-names></name>
          <name><surname>Tucker</surname><given-names>Paul</given-names></name>
          <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
          <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
          <name><surname>Viégas</surname><given-names>Fernanda</given-names></name>
          <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
          <name><surname>Warden</surname><given-names>Pete</given-names></name>
          <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
          <name><surname>Wicke</surname><given-names>Martin</given-names></name>
          <name><surname>Yu</surname><given-names>Yuan</given-names></name>
          <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
        </person-group>
        <article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title>
        <year iso-8601-date="2015">2015</year>
        <uri>http://tensorflow.org/</uri>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
