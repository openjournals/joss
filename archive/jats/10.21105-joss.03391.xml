<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3391</article-id>
<article-id pub-id-type="doi">10.21105/joss.03391</article-id>
<title-group>
<article-title>Omnizart: A General Toolbox for Automatic Music
Transcription</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<string-name>Yu-Te Wu</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Yin-Jyun Luo</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Tsung-Ping Chen</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>I-Chieh Wei</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Jui-Yang Hsu</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Yi-Chin Chuang</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Li Su</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Music and Culture Technology Lab, Institute of Information
Science, Academia Sinica, Taipei, Taiwan</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-04-18">
<day>18</day>
<month>4</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>68</issue>
<fpage>3391</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>automatic music transcription</kwd>
<kwd>music information retrieval</kwd>
<kwd>audio signal processing</kwd>
<kwd>artificial intelligence</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>We present and release Omnizart, a new Python library that provides
  a streamlined solution to automatic music transcription (AMT).
  Omnizart encompasses modules that construct the life-cycle of deep
  learning-based AMT, and is designed for ease of use with a compact
  command-line interface. To the best of our knowledge, Omnizart is the
  first toolkit that offers transcription models for various music
  content including piano solo, instrument ensembles, percussion and
  vocal. Omnizart also supports models for chord recognition and
  beat/downbeat tracking, which are highly related to AMT.</p>
  <p>In summary, Omnizart incorporates:</p>
  <list list-type="bullet">
    <list-item>
      <p>Pre-trained models for frame-level and note-level transcription
      of multiple pitched instruments, vocal melody, and drum
      events;</p>
    </list-item>
    <list-item>
      <p>Pre-trained models of chord recognition and beat/downbeat
      tracking;</p>
    </list-item>
    <list-item>
      <p>The main functionalities in the life-cycle of AMT research,
      covering dataset downloading, feature pre-processing, model
      training, to the sonification of the transcription result.</p>
    </list-item>
  </list>
  <p>Omnizart is based on Tensorflow
  (<xref alt="Abadi et al., 2016" rid="ref-abadi2016tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>). The complete code base, command-line interface,
  documentation, as well as demo examples can all be accessed from the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Music-and-Culture-Technology-Lab/omnizart">project
  website</ext-link>.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>AMT of polyphonic music is a complicated MIR task because the
  note-, melody-, timbre-, and rhythm-level attributes of music are
  overlapped with each other in music signals. A unified solution of AMT
  is therefore in eager demand. AMT is also strongly related to other
  MIR tasks such as source separation and music generation with
  transcribed data needed as supervisory resources. Omnizart considers
  multi-instrument transcription and collects several state-of-the-art
  models for transcribing pitched and percussive instruments, as well as
  singing voice, within polyphonic music signals. Omnizart is an AMT
  tool that unifies multiple transcription utilities and enables further
  productivity. Omnizart can save one’s time and labor in generating a
  massive number of multi-track MIDI files, which could have a large
  impact on music production, music generation, education, and
  musicology research.</p>
</sec>
<sec id="implementation-details">
  <title>Implementation Details</title>
  <sec id="piano-solo-transcription">
    <title>Piano solo transcription</title>
    <p>The piano solo transcription model in Omnizart reproduces the
    implementation of Wu et al.
    (<xref alt="2020" rid="ref-wu2020multi" ref-type="bibr">2020</xref>).
    The model features a U-net that takes as inputs the audio
    spectrogram, generalized cepstrum (GC)
    (<xref alt="Su &amp; Yang, 2015" rid="ref-su2015combining" ref-type="bibr">Su
    &amp; Yang, 2015</xref>), and GC of spectrogram (GCoS)
    (<xref alt="Wu et al., 2018" rid="ref-wu2018automatic" ref-type="bibr">Wu
    et al., 2018</xref>), and outputs a multi-channel time-pitch
    representation with time- and pitch-resolution of 20 ms and 25
    cents, respectively. For the U-net, implementation of the encoder
    and the decoder follows DeepLabV3+
    (<xref alt="L.-C. Chen et al., 2018" rid="ref-Chen2018DeepLabV3" ref-type="bibr">L.-C.
    Chen et al., 2018</xref>), and the bottleneck layer is adapted from
    the Image Transformer
    (<xref alt="Parmar et al., 2018" rid="ref-parmar2018image" ref-type="bibr">Parmar
    et al., 2018</xref>).</p>
    <p>The model is trained on the MAESTRO dataset
    (<xref alt="Hawthorne et al., 2019" rid="ref-hawthorne2018enabling" ref-type="bibr">Hawthorne
    et al., 2019</xref>), an external dataset containing 1,184 real
    piano performance recordings with a total length of 172.3 hours. The
    model achieves 72.50% and 79.57% for frame- and note-level
    F1-scores, respectively, on the Configuration-II test set of the
    MAPS dataset
    (<xref alt="Kelz et al., 2016" rid="ref-kelz2016potential" ref-type="bibr">Kelz
    et al., 2016</xref>).</p>
  </sec>
  <sec id="multi-instrument-polyphonic-transcription">
    <title>Multi-instrument polyphonic transcription</title>
    <p>The multi-instrument transcription model extends the piano solo
    model to support 11 output classes, namely piano, violin, viola,
    cello, flute, horn, bassoon, clarinet, harpsichord, contrabass, and
    oboe, accessed from MusicNet
    (<xref alt="Thickstun et al., 2017" rid="ref-thickstun2017learning" ref-type="bibr">Thickstun
    et al., 2017</xref>). Detailed characteristics of the model can be
    seen in Wu et al.
    (<xref alt="2020" rid="ref-wu2020multi" ref-type="bibr">2020</xref>).
    The evaluation on the test set from MusicNet
    (<xref alt="Thickstun et al., 2018" rid="ref-thickstun2018invariances" ref-type="bibr">Thickstun
    et al., 2018</xref>) yields 66.59% for the note streaming task.</p>
  </sec>
  <sec id="drum-transcription">
    <title>Drum transcription</title>
    <p>The model for drum transcription is a re-implementation of Wei et
    al.
    (<xref alt="2021" rid="ref-wei2021improving" ref-type="bibr">2021</xref>).
    Building blocks of the network include convolutional layers and the
    attention mechanism.</p>
    <p>The model is trained on a dataset with 1,454 audio clips of
    polyphonic music with synchronized drum events
    (<xref alt="Wei et al., 2021" rid="ref-wei2021improving" ref-type="bibr">Wei
    et al., 2021</xref>). The model demonstrates SoTA performance on two
    commonly used benchmark datasets, i.e., 74% for ENST
    (<xref alt="Gillet &amp; Richard, 2006" rid="ref-gillet2006enst" ref-type="bibr">Gillet
    &amp; Richard, 2006</xref>) and 71% for MDB-Drums
    (<xref alt="Southall et al., 2017" rid="ref-southall2017mdb" ref-type="bibr">Southall
    et al., 2017</xref>) in terms of the note-level F1-score.</p>
  </sec>
  <sec id="vocal-transcription-in-polyphonic-music">
    <title>Vocal transcription in polyphonic music</title>
    <p>The system for vocal transcription features a pitch extractor and
    a module for note segmentation. The inputs to the model are composed
    of spectrogram, GS, and GCoS derived from polyphonic music
    recordings
    (<xref alt="Wu et al., 2018" rid="ref-wu2018automatic" ref-type="bibr">Wu
    et al., 2018</xref>).</p>
    <p>A pre-trained Patch-CNN
    (<xref alt="Su, 2018" rid="ref-su2018vocal" ref-type="bibr">Su,
    2018</xref>) is leveraged as the pitch extractor. The module for
    note segmentation is implemented with PyramidNet-110 and ShakeDrop
    regularization
    (<xref alt="Yamada et al., 2019" rid="ref-yamada2019shakedrop" ref-type="bibr">Yamada
    et al., 2019</xref>), which is trained using Virtual Adversarial
    Training
    (<xref alt="Miyato et al., 2019" rid="ref-miyato2018virtual" ref-type="bibr">Miyato
    et al., 2019</xref>) enabling semi-supervised learning.</p>
    <p>The training data includes labeled data from TONAS
    (<xref alt="Mora et al., 2010" rid="ref-mora2010characterization" ref-type="bibr">Mora
    et al., 2010</xref>) and unlabeled data from MIR-1K
    (<xref alt="Hsu &amp; Jang, 2009" rid="ref-hsu2009improvement" ref-type="bibr">Hsu
    &amp; Jang, 2009</xref>). The model yields the SoTA F1-score of
    68.4% evaluated with the ISMIR2014 dataset
    (<xref alt="Molina et al., 2014" rid="ref-molina2014evaluation" ref-type="bibr">Molina
    et al., 2014</xref>).</p>
  </sec>
  <sec id="chord-recognition">
    <title>Chord recognition</title>
    <p>The harmony recognition model of Omnizart is implemented using
    the Harmony Transformer (HT)
    (<xref alt="T.-P. Chen &amp; Su, 2019" rid="ref-chen2019harmony" ref-type="bibr">T.-P.
    Chen &amp; Su, 2019</xref>). The HT model is based on an
    encoder-decoder architecture, where the encoder performs chord
    segmentation on the input, and the decoder recognizes the chord
    progression based on the segmentation result.</p>
    <p>The original HT supports both audio and symbolic inputs.
    Currently, Omnizart supports only audio inputs. A given audio input
    is pre-processed using Chordino VAMP plugin
    (<xref alt="Mauch &amp; Dixon, 2010" rid="ref-mauch2010approximate" ref-type="bibr">Mauch
    &amp; Dixon, 2010</xref>) as the non-negative-least-squares
    chromagram. The outputs of the model include 25 chord types,
    covering 12 major and minor chords together with a class referred to
    the absence of chord, with a time resolution of 230 ms.</p>
    <p>In an experiment with evaluations on the McGill Billboard dataset
    (<xref alt="Burgoyne et al., 2011" rid="ref-burgoyne2011anexpert" ref-type="bibr">Burgoyne
    et al., 2011</xref>), the HT outperforms the previous state of the
    art
    (<xref alt="T.-P. Chen &amp; Su, 2019" rid="ref-chen2019harmony" ref-type="bibr">T.-P.
    Chen &amp; Su, 2019</xref>).</p>
  </sec>
  <sec id="beatdownbeat-tracking">
    <title>Beat/downbeat tracking</title>
    <p>The model for beat and downbeat tracking provided in Omnizart is
    a reproduction of Chuang &amp; Su
    (<xref alt="2020" rid="ref-chuang2020beat" ref-type="bibr">2020</xref>).
    Unlike most of the available open-source projects such as
    (<xref alt="Böck et al., 2016" rid="ref-bock2016madmom" ref-type="bibr">Böck
    et al., 2016</xref>) and
    (<xref alt="McFee et al., 2015" rid="ref-mcfee2015librosa" ref-type="bibr">McFee
    et al., 2015</xref>) which focus on audio, the provided model
    targets symbolic data.</p>
    <p>The input and output of the model are respectively MIDI and
    beat/downbeat positions with the time resolution of 10 ms. The input
    representation combines piano-roll, spectral flux, and inter-onset
    interval extracted from MIDI. The model composes a two-layer BLSTM
    network with the attention mechanism, and predicts probabilities of
    the presence of beat and downbeat per time step.</p>
    <p>Experiments on the MusicNet dataset
    (<xref alt="Thickstun et al., 2018" rid="ref-thickstun2018invariances" ref-type="bibr">Thickstun
    et al., 2018</xref>) with the synchronized beat annotation show that
    the proposed model outperforms the state-of-the-art beat trackers
    which operate on synthesized audio
    (<xref alt="Chuang &amp; Su, 2020" rid="ref-chuang2020beat" ref-type="bibr">Chuang
    &amp; Su, 2020</xref>).</p>
  </sec>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>Omnizart represents the first systematic solution for the
  polyphonic AMT of general music contents ranging from pitched
  instruments, percussion instruments, to voices. In addition to note
  transcription, Omnizart also includes high-level MIR tasks such as
  chord recognition and beat/downbeat tracking. As an ongoing project,
  the research group will keep refining the package and extending the
  scope of transcription in the future.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-abadi2016tensorflow">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
          <name><surname>Barham</surname><given-names>Paul</given-names></name>
          <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
          <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
          <name><surname>Davis</surname><given-names>Andy</given-names></name>
          <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
          <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
          <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
          <name><surname>Isard</surname><given-names>Michael</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Tensorflow: A system for large-scale machine learning</article-title>
        <source>USENIX symposium on operating systems design and implementation (OSDI)</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-wu2020multi">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wu</surname><given-names>Yu-Te</given-names></name>
          <name><surname>Chen</surname><given-names>Berlin</given-names></name>
          <name><surname>Su</surname><given-names>Li</given-names></name>
        </person-group>
        <article-title>Multi-instrument automatic music transcription with self-attention-based instance segmentation</article-title>
        <source>IEEE/ACM Transactions on Audio, Speech, and Language Processing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>28</volume>
        <pub-id pub-id-type="doi">10.1109/taslp.2020.3030482</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hawthorne2018enabling">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Hawthorne</surname><given-names>Curtis</given-names></name>
          <name><surname>Stasyuk</surname><given-names>Andriy</given-names></name>
          <name><surname>Roberts</surname><given-names>Adam</given-names></name>
          <name><surname>Simon</surname><given-names>Ian</given-names></name>
          <name><surname>Huang</surname><given-names>Cheng-Zhi Anna</given-names></name>
          <name><surname>Dieleman</surname><given-names>Sander</given-names></name>
          <name><surname>Elsen</surname><given-names>Erich</given-names></name>
          <name><surname>Engel</surname><given-names>Jesse H.</given-names></name>
          <name><surname>Eck</surname><given-names>Douglas</given-names></name>
        </person-group>
        <article-title>Enabling factorized piano music modeling and generation with the MAESTRO dataset</article-title>
        <source>International conference on learning representations (ICLR)</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-parmar2018image">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Parmar</surname><given-names>Niki</given-names></name>
          <name><surname>Vaswani</surname><given-names>Ashish</given-names></name>
          <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
          <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
          <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
          <name><surname>Ku</surname><given-names>Alexander</given-names></name>
          <name><surname>Tran</surname><given-names>Dustin</given-names></name>
        </person-group>
        <article-title>Image transformer</article-title>
        <source>Proceedings of the 35th international conference on machine learning (ICML)</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-Chen2018DeepLabV3">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Chen</surname><given-names>Liang-Chieh</given-names></name>
          <name><surname>Zhu</surname><given-names>Yukun</given-names></name>
          <name><surname>Papandreou</surname><given-names>George</given-names></name>
          <name><surname>Schroff</surname><given-names>Florian</given-names></name>
          <name><surname>Adam</surname><given-names>Hartwig</given-names></name>
        </person-group>
        <article-title>Encoder-decoder with atrous separable convolution for semantic image segmentation</article-title>
        <source>Computer vision  ECCV</source>
        <publisher-name>Springer International Publishing</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <uri>https://doi.org/10.1007%2F978-3-030-01234-2_49</uri>
        <pub-id pub-id-type="doi">10.1007/978-3-030-01234-2_49</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-su2015combining">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Su</surname><given-names>Li</given-names></name>
          <name><surname>Yang</surname><given-names>Yi-Hsuan</given-names></name>
        </person-group>
        <article-title>Combining spectral and temporal representations for multipitch estimation of polyphonic music</article-title>
        <source>IEEE/ACM Transactions on Audio, Speech, and Language Processing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>23</volume>
        <issue>10</issue>
        <pub-id pub-id-type="doi">10.1109/taslp.2015.2442411</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kelz2016potential">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kelz</surname><given-names>Rainer</given-names></name>
          <name><surname>Dorfer</surname><given-names>Matthias</given-names></name>
          <name><surname>Korzeniowski</surname><given-names>Filip</given-names></name>
          <name><surname>Böck</surname><given-names>Sebastian</given-names></name>
          <name><surname>Arzt</surname><given-names>Andreas</given-names></name>
          <name><surname>Widmer</surname><given-names>Gerhard</given-names></name>
        </person-group>
        <article-title>On the potential of simple framewise approaches to piano transcription</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-wei2021improving">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Wei</surname><given-names>I-Chieh</given-names></name>
          <name><surname>Wu</surname><given-names>Chih-Wei</given-names></name>
          <name><surname>Su</surname><given-names>Li</given-names></name>
        </person-group>
        <article-title>Improving automatic drum transcription using large-scale audio-to-MIDI aligned data</article-title>
        <source>IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>
        <year iso-8601-date="2021">2021</year>
        <pub-id pub-id-type="doi">10.1109/icassp39728.2021.9414409</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wu2018automatic">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Wu</surname><given-names>Yu-Te</given-names></name>
          <name><surname>Chen</surname><given-names>Berlin</given-names></name>
          <name><surname>Su</surname><given-names>Li</given-names></name>
        </person-group>
        <article-title>Automatic music transcription leveraging generalized cepstral features and deep learning</article-title>
        <source>IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.1109/icassp.2018.8462079</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-su2018vocal">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Su</surname><given-names>Li</given-names></name>
        </person-group>
        <article-title>Vocal melody extraction using patch-based CNN</article-title>
        <source>IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.1109/icassp.2018.8462420</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-yamada2019shakedrop">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Yamada</surname><given-names>Y.</given-names></name>
          <name><surname>Iwamura</surname><given-names>M.</given-names></name>
          <name><surname>Akiba</surname><given-names>T.</given-names></name>
          <name><surname>Kise</surname><given-names>K.</given-names></name>
        </person-group>
        <article-title>Shakedrop regularization for deep residual learning</article-title>
        <source>IEEE Access</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>7</volume>
        <pub-id pub-id-type="doi">10.1109/access.2019.2960566</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-miyato2018virtual">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Miyato</surname><given-names>Takeru</given-names></name>
          <name><surname>Maeda</surname><given-names>Shin-ichi</given-names></name>
          <name><surname>Koyama</surname><given-names>Masanori</given-names></name>
          <name><surname>Ishii</surname><given-names>Shin</given-names></name>
        </person-group>
        <article-title>Virtual adversarial training: A regularization method for supervised and semi-supervised learning</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year iso-8601-date="2019">2019</year>
        <volume>41</volume>
        <issue>8</issue>
        <uri>https://doi.org/10.1109/TPAMI.2018.2858821</uri>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2858821</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-chen2019harmony">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Chen</surname><given-names>Tsung-Ping</given-names></name>
          <name><surname>Su</surname><given-names>Li</given-names></name>
        </person-group>
        <article-title>Harmony transformer: Incorporating chord segmentation into harmony recognition</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-mora2010characterization">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Mora</surname><given-names>Joaquı́n</given-names></name>
          <name><surname>Gómez</surname><given-names>Francisco</given-names></name>
          <name><surname>Gómez</surname><given-names>Emilia</given-names></name>
          <name><surname>Escobar-Borrego</surname><given-names>Francisco</given-names></name>
          <name><surname>Dı́az-Báñez</surname><given-names>José Miguel</given-names></name>
        </person-group>
        <article-title>Characterization and melodic similarity of a cappella flamenco cantes</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <year iso-8601-date="2010">2010</year>
      </element-citation>
    </ref>
    <ref id="ref-hsu2009improvement">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hsu</surname><given-names>Chao-Ling</given-names></name>
          <name><surname>Jang</surname><given-names>Jyh-Shing Roger</given-names></name>
        </person-group>
        <article-title>On the improvement of singing voice separation for monaural recordings using the MIR-1K dataset</article-title>
        <source>IEEE/ACM Transactions on Audio, Speech, and Language Processing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <volume>18</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1109/tasl.2009.2026503</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-molina2014evaluation">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Molina</surname><given-names>E.</given-names></name>
          <name><surname>Barbancho-Perez</surname><given-names>A. M.</given-names></name>
          <name><surname>Tardón</surname><given-names>L. J.</given-names></name>
          <name><surname>Barbancho-Perez</surname><given-names>I.</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Evaluation framework for automatic singing transcription</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <year iso-8601-date="2014">2014</year>
      </element-citation>
    </ref>
    <ref id="ref-gillet2006enst">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Gillet</surname><given-names>Olivier</given-names></name>
          <name><surname>Richard</surname><given-names>Gaël</given-names></name>
        </person-group>
        <article-title>ENST-drums: An extensive audio-visual database for drum signals processing.</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <year iso-8601-date="2006">2006</year>
      </element-citation>
    </ref>
    <ref id="ref-southall2017mdb">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Southall</surname><given-names>Carl</given-names></name>
          <name><surname>Wu</surname><given-names>Chih-Wei</given-names></name>
          <name><surname>Lerch</surname><given-names>Alexander</given-names></name>
          <name><surname>Hockman</surname><given-names>Jason</given-names></name>
        </person-group>
        <article-title>MDB drums: An annotated subset of MedleyDB for automatic drum transcription</article-title>
        <source>Late breaking/demos of the 18th international society for music information retrieval conference (ISMIR)</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-chuang2020beat">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Chuang</surname><given-names>Yi-Chin</given-names></name>
          <name><surname>Su</surname><given-names>Li</given-names></name>
        </person-group>
        <article-title>Beat and downbeat tracking of symbolic music data using deep recurrent neural networks</article-title>
        <source>Asia-pacific signal and information processing association annual summit and conference (APSIPA ASC)</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-thickstun2018invariances">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Thickstun</surname><given-names>John</given-names></name>
          <name><surname>Harchaoui</surname><given-names>Zaı̈d</given-names></name>
          <name><surname>Foster</surname><given-names>Dean P.</given-names></name>
          <name><surname>Kakade</surname><given-names>Sham M.</given-names></name>
        </person-group>
        <article-title>Invariances and data augmentation for supervised music transcription</article-title>
        <source>IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.1109/icassp.2018.8461686</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-bock2016madmom">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Böck</surname><given-names>Sebastian</given-names></name>
          <name><surname>Korzeniowski</surname><given-names>Filip</given-names></name>
          <name><surname>Schlüter</surname><given-names>Jan</given-names></name>
          <name><surname>Krebs</surname><given-names>Florian</given-names></name>
          <name><surname>Widmer</surname><given-names>Gerhard</given-names></name>
        </person-group>
        <article-title>Madmom: A new Python audio and music signal processing library</article-title>
        <source>Proceedings of the ACM conference on multimedia conference</source>
        <year iso-8601-date="2016">2016</year>
        <pub-id pub-id-type="doi">10.1145/2964284.2973795</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-mcfee2015librosa">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>McFee</surname><given-names>Brian</given-names></name>
          <name><surname>Raffel</surname><given-names>Colin</given-names></name>
          <name><surname>Liang</surname><given-names>Dawen</given-names></name>
          <name><surname>Ellis</surname><given-names>Daniel PW</given-names></name>
          <name><surname>McVicar</surname><given-names>Matt</given-names></name>
          <name><surname>Battenberg</surname><given-names>Eric</given-names></name>
          <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
        </person-group>
        <article-title>Librosa: Audio and music signal analysis in Python</article-title>
        <source>Proceedings of the 14th python in science conference</source>
        <publisher-name>Citeseer</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>8</volume>
        <pub-id pub-id-type="doi">10.25080/majora-7b98e3ed-003</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-burgoyne2011anexpert">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Burgoyne</surname><given-names>John Ashley</given-names></name>
          <name><surname>Wild</surname><given-names>Jonathan</given-names></name>
          <name><surname>Fujinaga</surname><given-names>Ichiro</given-names></name>
        </person-group>
        <article-title>An expert ground truth set for audio chord recognition and music analysis</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <year iso-8601-date="2011">2011</year>
      </element-citation>
    </ref>
    <ref id="ref-mauch2010approximate">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Mauch</surname><given-names>Matthias</given-names></name>
          <name><surname>Dixon</surname><given-names>Simon</given-names></name>
        </person-group>
        <article-title>Approximate note transcription for the improved identification of difficult chords</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <year iso-8601-date="2010">2010</year>
      </element-citation>
    </ref>
    <ref id="ref-thickstun2017learning">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Thickstun</surname><given-names>John</given-names></name>
          <name><surname>Harchaoui</surname><given-names>Zaid</given-names></name>
          <name><surname>Kakade</surname><given-names>Sham M.</given-names></name>
        </person-group>
        <article-title>Learning features of music from scratch</article-title>
        <source>International conference on learning representations (ICLR)</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
