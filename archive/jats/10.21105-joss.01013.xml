<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1013</article-id>
<article-id pub-id-type="doi">10.21105/joss.01013</article-id>
<title-group>
<article-title>Embeddings.jl: easy access to pretrained word embeddings
from Julia</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1386-1646</contrib-id>
<string-name>Lyndon White</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-2431-3963</contrib-id>
<string-name>David Ellison</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>The University of Western Australia</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>None</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2018-08-30">
<day>30</day>
<month>8</month>
<year>2018</year>
</pub-date>
<volume>4</volume>
<issue>36</issue>
<fpage>1013</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>julialang</kwd>
<kwd>opendata</kwd>
<kwd>NLP</kwd>
<kwd>word embeddings</kwd>
<kwd>machine learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Embeddings.jl is a tool to help users of the Julia programming
  language
  (<xref alt="Bezanson et al., 2017" rid="ref-Julia" ref-type="bibr">Bezanson
  et al., 2017</xref>) make use of pretrained word embeddings for NLP.
  Word embeddings are a very important feature representation in natural
  language processing. The use of embeddings pretrained on very large
  corpora can be seen as a form of transfer learning. It allows
  knowledge of lexical semantics derived from the distributional
  hypothesis– that words occurring in similar contexts have similar
  meaning– to be injected into models which may have only limited
  amounts of supervised, task oriented training data.</p>
  <p>Many creators of word embedding methods have generously made sets
  of pretrained word representations publicly available. Embeddings.jl
  exposes these as a standard matrix of numbers and a corresponding
  array of strings. This lets Julia programs use word embeddings easily,
  either on their own or alongside machine learning packages such as
  Flux (<xref alt="Innes, 2018" rid="ref-flux" ref-type="bibr">Innes,
  2018</xref>). In such deep learning packages, it is common to use word
  embeddings as an input layer of an LSTM or other neural network, where
  they may be kept invariant or used as initialization for fine-tuning
  on the supervised task. They can be summed to represent a bag of
  words, concatenated to form a matrix representation of a sentence or
  document, or used otherwise in a wide variety of natural language
  processing tasks.</p>
  <p>Embeddings.jl makes use of DataDeps.jl
  (<xref alt="White et al., 2018" rid="ref-DataDeps" ref-type="bibr">White
  et al., 2018</xref>), to allow for convenient automatic downloading of
  the data when and if required. It also uses the DataDeps.jl prompt to
  ensure the user of the embeddings has full knowledge of the original
  source of the data, and which papers to cite etc.</p>
  <p>It currently provides access to</p>
  <list list-type="bullet">
    <list-item>
      <p>multiple sets of word2vec embeddings
      (<xref alt="Mikolov et al., 2013" rid="ref-word2vec" ref-type="bibr">Mikolov
      et al., 2013</xref>) for English</p>
    </list-item>
    <list-item>
      <p>multiple sets of GLoVE
      (<xref alt="Pennington et al., 2014" rid="ref-glove" ref-type="bibr">Pennington
      et al., 2014</xref>) embeddings for English</p>
    </list-item>
    <list-item>
      <p>multiple sets of FastText embeddings
      (<xref alt="Grave et al., 2018" rid="ref-fasttext157lang" ref-type="bibr">Grave
      et al., 2018</xref>),
      (<xref alt="Bojanowski et al., 2017" rid="ref-fasttext" ref-type="bibr">Bojanowski
      et al., 2017</xref>) for several hundred languages</p>
    </list-item>
  </list>
  <p>It is anticipated that as more pretrained embeddings are made
  available for more languages and using newer methods, the
  Embeddings.jl package will be updated to support them.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Julia">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bezanson</surname><given-names>J.</given-names></name>
          <name><surname>Edelman</surname><given-names>A.</given-names></name>
          <name><surname>Karpinski</surname><given-names>S.</given-names></name>
          <name><surname>Shah</surname><given-names>V.</given-names></name>
        </person-group>
        <article-title>Julia: A fresh approach to numerical computing</article-title>
        <source>SIAM Review</source>
        <year iso-8601-date="2017">2017</year>
        <volume>59</volume>
        <issue>1</issue>
        <uri>https://doi.org/10.1137/141000671</uri>
        <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-DataDeps">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>White</surname><given-names>L.</given-names></name>
          <name><surname>Togneri</surname><given-names>R.</given-names></name>
          <name><surname>Liu</surname><given-names>W.</given-names></name>
          <name><surname>Bennamoun</surname><given-names>M.</given-names></name>
        </person-group>
        <article-title>DataDeps.jl: Repeatable Data Setup for Replicable Data Science</article-title>
        <source>ArXiv e-prints</source>
        <year iso-8601-date="2018-08">2018</year><month>08</month>
        <uri>https://arxiv.org/abs/1808.01091</uri>
      </element-citation>
    </ref>
    <ref id="ref-fasttext">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bojanowski</surname><given-names>Piotr</given-names></name>
          <name><surname>Grave</surname><given-names>Edouard</given-names></name>
          <name><surname>Joulin</surname><given-names>Armand</given-names></name>
          <name><surname>Mikolov</surname><given-names>Tomas</given-names></name>
        </person-group>
        <article-title>Enriching word vectors with subword information</article-title>
        <source>Transactions of the Association for Computational Linguistics</source>
        <year iso-8601-date="2017">2017</year>
        <volume>5</volume>
        <uri>https://arxiv.org/abs/1607.04606</uri>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00051</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-fasttext157lang">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Grave</surname><given-names>Edouard</given-names></name>
          <name><surname>Bojanowski</surname><given-names>Piotr</given-names></name>
          <name><surname>Gupta</surname><given-names>Prakhar</given-names></name>
          <name><surname>Joulin</surname><given-names>Armand</given-names></name>
          <name><surname>Mikolov</surname><given-names>Tomas</given-names></name>
        </person-group>
        <article-title>Learning word vectors for 157 languages</article-title>
        <source>Proceedings of the international conference on language resources and evaluation (LREC 2018)</source>
        <year iso-8601-date="2018">2018</year>
        <uri>https://arxiv.org/abs/1802.06893</uri>
      </element-citation>
    </ref>
    <ref id="ref-word2vec">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mikolov</surname><given-names>Tomas</given-names></name>
          <name><surname>Chen</surname><given-names>Kai</given-names></name>
          <name><surname>Corrado</surname><given-names>Greg</given-names></name>
          <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        </person-group>
        <article-title>Efficient estimation of word representations in vector space</article-title>
        <source>arXiv:1301.3781</source>
        <year iso-8601-date="2013">2013</year>
        <uri>http://arxiv.org/pdf/1301.3781v3</uri>
      </element-citation>
    </ref>
    <ref id="ref-glove">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Pennington</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Socher</surname><given-names>Richard</given-names></name>
          <name><surname>Manning</surname><given-names>Christopher D.</given-names></name>
        </person-group>
        <article-title>GloVe: Global vectors for word representation</article-title>
        <source>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP 2014)</source>
        <year iso-8601-date="2014">2014</year>
        <uri>http://www.aclweb.org/anthology/D14-1162</uri>
        <pub-id pub-id-type="doi">10.3115/v1/d14-1162</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-flux">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Innes</surname><given-names>Mike</given-names></name>
        </person-group>
        <article-title>Flux: Elegant machine learning with julia</article-title>
        <source>Journal of Open Source Software</source>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.21105/joss.00602</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
