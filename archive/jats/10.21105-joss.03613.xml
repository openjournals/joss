<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3613</article-id>
<article-id pub-id-type="doi">10.21105/joss.03613</article-id>
<title-group>
<article-title>AuditoryStimuli.jl: A Julia package for generating
real-time auditory stimuli</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-4930-8351</contrib-id>
<string-name>Robert Luke</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Macquarie University, Macquarie University Hearing &amp;
Department of Linguistics, Australian Hearing Hub, Sydney, New South
Wales, Australia</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-07-19">
<day>19</day>
<month>7</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>65</issue>
<fpage>3613</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>auditory</kwd>
<kwd>real-time</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The <monospace>AuditoryStimuli.jl</monospace> software package
  provides researchers with a framework to generate real-time audio
  signals in the Julia programming language. The package is designed for
  use in auditory research programs, neurofeedback applications, and
  audio signal processing development. The package is developed on top
  of the SampledSignals library
  (<xref alt="Russell, 2016" rid="ref-sampled-signals" ref-type="bibr">Russell,
  2016</xref>) to provide auditory specific functionality and encourage
  best practices in real-time audio generation. As such, modules are
  provided to generate auditory signals, modify these signals, and
  output the resulting waveforms. The package can be used to generate
  offline audio signals, but when used as a real-time audio system, it
  provides safe guards for common mistakes which can cause signal
  distortions and undesired auditory percepts.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>There has been great improvement in standardising the analysis and
  post processing of research data in the scientific fields of auditory
  perception, auditory neuroscience, and brain computer interface
  research
  (<xref alt="Gramfort et al., 2014" rid="ref-gramfort2014mne" ref-type="bibr">Gramfort
  et al., 2014</xref>;
  <xref alt="Oostenveld et al., 2011" rid="ref-oostenveld2011fieldtrip" ref-type="bibr">Oostenveld
  et al., 2011</xref>). However, despite being critical for conducting
  reproducible experiments, standardised tools for the generation of
  auditory stimuli have typically not been developed with the same
  software engineering rigour. A large portion of stimulus signals for
  auditory research are generated with privately shared code fragments,
  without version control or efficient means of reporting errors. As
  such, this package provides a documented and version-controlled
  open-source framework for generating auditory stimuli.</p>
  <p><monospace>AuditoryStimuli.jl</monospace> is specifically developed
  for real-time audio research applications. A variety of software
  packages already exist for controlling the presentation of traditional
  block-design psychoacoustic experiments
  (<xref alt="Carcagno, 2012" rid="ref-pychoacoustics" ref-type="bibr">Carcagno,
  2012</xref>;
  <xref alt="Peirce et al., 2019" rid="ref-psychopy2" ref-type="bibr">Peirce
  et al., 2019</xref>;
  <xref alt="Schönwiesner &amp; Bialas, 2021" rid="ref-Schönwiesner2021" ref-type="bibr">Schönwiesner
  &amp; Bialas, 2021</xref>), and the post-processing the analysis of
  acoustic signals
  (<xref alt="Kim et al., 2017" rid="ref-kim2017fast" ref-type="bibr">Kim
  et al., 2017</xref>;
  <xref alt="Lossner, 2019" rid="ref-python-sofa" ref-type="bibr">Lossner,
  2019</xref>;
  <xref alt="McFee et al., 2015" rid="ref-mcfee2015librosa" ref-type="bibr">McFee
  et al., 2015</xref>). As such, this package does not focus on
  providing the scaffolding for traditional experimentation such as
  block design experiments or alternative forced choice procedures;
  users are directed to existing tools such as Carcagno
  (<xref alt="2012" rid="ref-pychoacoustics" ref-type="bibr">2012</xref>)
  for this purpose. Instead this package fills the need for a
  frame-based real-time signal processing framework.
  <monospace>AuditoryStimuli.jl</monospace> provides tools to generate
  real-time audio signals that can be dynamically adapted to the
  participant’s responses or state. That is, the stimulus properties may
  change continuously and dynamically on a scale of milliseconds, rather
  than in pre-generated signals on the scale of seconds.</p>
  <p>Real-time audio stimulus generation is also required for brain
  computer interface applications. For example, methods have been
  developed to measure the brains response to sounds in real time
  (<xref alt="Cheveigné et al., 2018" rid="ref-DECHEVEIGNE2018206" ref-type="bibr">Cheveigné
  et al., 2018</xref>;
  <xref alt="Luke &amp; Wouters, 2016" rid="ref-luke2016kalman" ref-type="bibr">Luke
  &amp; Wouters, 2016</xref>). This package provides a real-time
  framework which can be used to adapt the resulting audio according to
  an external signal, such as the brain state.</p>
  <p>Real-time audio frameworks are required for developing audio
  signal-processing algorithms. For example, noise reduction, wind noise
  detection
  (<xref alt="Sapozhnykov, Harvey, &amp; Luke, 2019" rid="ref-sapozhnykov2019wind" ref-type="bibr">Sapozhnykov,
  Harvey, &amp; Luke, 2019</xref>), acoustic state detection
  (<xref alt="Luke et al., 2020" rid="ref-robert2019blocked" ref-type="bibr">Luke
  et al., 2020</xref>;
  <xref alt="Sapozhnykov, Harvey, Erfaniansaeedi, et al., 2019" rid="ref-sapozhnykov2020headset" ref-type="bibr">Sapozhnykov,
  Harvey, Erfaniansaeedi, et al., 2019</xref>), and speech enhancement
  algorithms typically run using frame based processing, with frames of
  2-4 ms. <monospace>AuditoryStimuli.jl</monospace> provides the user
  with the ability to dynamically set processing parameters for each
  audio frame, enabling real-time signal processing development.
  Algorithm parameters can be varied and processing steps can be
  adaptively enabled.</p>
  <p>Due to the real-time nature of the software package, fast
  mathematical computation is required. Audio signal processing is
  usually deployed using assembly or C programming languages, as these
  provide excellent processing speed. However, these low level languages
  require complex tooling and management of complex data structures,
  which increases development time and decreases the rate at which
  researchers can iterate on signal-processing designs. Instead
  <monospace>AuditoryStimuli.jl</monospace> is written in the Julia
  programming language
  (<xref alt="Bezanson et al., 2017" rid="ref-bezanson2017julia" ref-type="bibr">Bezanson
  et al., 2017</xref>), which provides the convenience of a high level
  language while providing excellent computational speed. The package is
  developed on top of the the SampledSignals library
  (<xref alt="Russell, 2016" rid="ref-sampled-signals" ref-type="bibr">Russell,
  2016</xref>), which provides the sample- and frame-based
  infrastructure. <monospace>AuditoryStimuli.jl</monospace> builds on
  top of this framework to provide auditory specific features.
  <monospace>AuditoryStimuli.jl</monospace> also utilises the Unitful
  (<xref alt="Keller, 2016" rid="ref-unitful" ref-type="bibr">Keller,
  2016</xref>) framework for automatic handling of units, all quantities
  must be passed with units (e.g., sample rate and modulation rate must
  be passed as Hz, kHz, etc) and will be automatically validated. This
  automatic conversion and checking of units further reduces the chance
  of user error when using
  <monospace>AuditoryStimuli.jl</monospace>.</p>
  <p><monospace>AuditoryStimuli.jl</monospace> provides functions to
  generate real-time audio signals and avoid abrupt changes in the
  stimulus that cause unwanted perceptual distortions. For example, when
  modifier functions are instantiated, the user can specify the maximum
  rate of change of the parameters. Then during processing, the modifier
  functions will ensure that parameters can not change too quickly,
  which causes distortions such as clicks. The package provides signal
  generators for common auditory stimuli such as noise, tones, and multi
  tone complexes. Similarly, the package provides for both single
  channel and multi channel audio processing, e.g., binaural stimuli
  with user specified interaural coherence. Modifier functions are
  provided for modifying the amplitude, phase, frequency content, and
  amplitude modulation of the signals. And tutorials are provided in the
  documentation which demonstrate how these generators and modifiers can
  be used together to produce common research stimuli.</p>
  <p>Taken together, the <monospace>AuditoryStimuli.jl</monospace>
  package fills a need within the auditory research community for a
  open-source real-time audio framework. The package addresses the
  communities need for a frame-based audio framework, that is
  computationally efficient and can be used in neuro feedback
  applications as well as audio processing development. An issue tracker
  and means for code contribution are provided, and the package provides
  installation instructions, documentation, and tutorials.</p>
</sec>
<sec id="audience">
  <title>Audience</title>
  <p><monospace>AuditoryStimuli.jl</monospace> is designed for students
  and researchers of all levels in the fields of auditory perception
  science, auditory neuroscience, neurofeedback research, and audio
  signal processing engineering.</p>
</sec>
<sec id="notes">
  <title>Notes</title>
  <p>This software was used to generate stimuli for Luke et al.
  (<xref alt="2021" rid="ref-luke2021analysis" ref-type="bibr">2021</xref>).</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-luke2021analysis">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Luke</surname><given-names>Robert</given-names></name>
          <name><surname>Larson</surname><given-names>Eric</given-names></name>
          <name><surname>Shader</surname><given-names>Maureen J</given-names></name>
          <name><surname>Innes-Brown</surname><given-names>Hamish</given-names></name>
          <name><surname>Van Yper</surname><given-names>Lindsey</given-names></name>
          <name><surname>Lee</surname><given-names>Adrian KC</given-names></name>
          <name><surname>Sowman</surname><given-names>Paul F</given-names></name>
          <name><surname>McAlpine</surname><given-names>David</given-names></name>
        </person-group>
        <article-title>Analysis methods for measuring passive auditory fNIRS responses generated by a block-design paradigm</article-title>
        <source>Neurophotonics</source>
        <publisher-name>International Society for Optics; Photonics</publisher-name>
        <year iso-8601-date="2021">2021</year>
        <volume>8</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1117/1.NPh.8.2.025008</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Schönwiesner2021">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schönwiesner</surname><given-names>Marc</given-names></name>
          <name><surname>Bialas</surname><given-names>Ole</given-names></name>
        </person-group>
        <article-title>S(ound)lab: An easy to learn Python package for designing and running psychoacoustic experiments.</article-title>
        <source>Journal of Open Source Software</source>
        <publisher-name>The Open Journal</publisher-name>
        <year iso-8601-date="2021">2021</year>
        <volume>6</volume>
        <issue>62</issue>
        <uri>https://doi.org/10.21105/joss.03284</uri>
        <pub-id pub-id-type="doi">10.21105/joss.03284</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-pychoacoustics">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Carcagno</surname><given-names>Samuele</given-names></name>
        </person-group>
        <article-title>Pychoacoustics</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <uri>https://github.com/sam81/pychoacoustics</uri>
      </element-citation>
    </ref>
    <ref id="ref-psychopy2">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Peirce</surname><given-names>Jonathan</given-names></name>
          <name><surname>Gray</surname><given-names>Jeremy R.</given-names></name>
          <name><surname>Simpson</surname><given-names>Sol</given-names></name>
          <name><surname>MacAskill</surname><given-names>Michael</given-names></name>
          <name><surname>Höchenberger</surname><given-names>Richard</given-names></name>
          <name><surname>Sogo</surname><given-names>Hiroyuki</given-names></name>
          <name><surname>Kastman</surname><given-names>Erik</given-names></name>
          <name><surname>Lindeløv</surname><given-names>Jonas Kristoffer</given-names></name>
        </person-group>
        <article-title>PsychoPy2: Experiments in behavior made easy</article-title>
        <source>Behavior Research Methods</source>
        <year iso-8601-date="2019">2019</year>
        <volume>51</volume>
        <issue>1</issue>
        <issn>1554-3528</issn>
        <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-mcfee2015librosa">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>McFee</surname><given-names>Brian</given-names></name>
          <name><surname>Raffel</surname><given-names>Colin</given-names></name>
          <name><surname>Liang</surname><given-names>Dawen</given-names></name>
          <name><surname>Ellis</surname><given-names>Daniel PW</given-names></name>
          <name><surname>McVicar</surname><given-names>Matt</given-names></name>
          <name><surname>Battenberg</surname><given-names>Eric</given-names></name>
          <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
        </person-group>
        <article-title>Librosa: Audio and music signal analysis in Python</article-title>
        <source>Proceedings of the 14th python in science conference</source>
        <year iso-8601-date="2015">2015</year>
        <volume>8</volume>
        <pub-id pub-id-type="doi">10.25080/majora-7b98e3ed-003</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-python-sofa">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Lossner</surname><given-names>Jannika</given-names></name>
        </person-group>
        <article-title>Spatially Oriented Format for Acoustics (SOFA) API for Python</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://github.com/spatialaudio/python-sofa</uri>
      </element-citation>
    </ref>
    <ref id="ref-unitful">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Keller</surname><given-names>Andrew</given-names></name>
        </person-group>
        <article-title>Unitful.jl</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <uri>https://github.com/PainterQubits/Unitful.jl</uri>
      </element-citation>
    </ref>
    <ref id="ref-sampled-signals">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Russell</surname><given-names>Spencer</given-names></name>
        </person-group>
        <article-title>SampledSignals.jl</article-title>
        <source>GitHub repository</source>
        <publisher-name>GitHub</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <uri>https://github.com/JuliaAudio/SampledSignals.jl</uri>
      </element-citation>
    </ref>
    <ref id="ref-gramfort2014mne">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
          <name><surname>Luessi</surname><given-names>Martin</given-names></name>
          <name><surname>Larson</surname><given-names>Eric</given-names></name>
          <name><surname>Engemann</surname><given-names>Denis A</given-names></name>
          <name><surname>Strohmeier</surname><given-names>Daniel</given-names></name>
          <name><surname>Brodbeck</surname><given-names>Christian</given-names></name>
          <name><surname>Parkkonen</surname><given-names>Lauri</given-names></name>
          <name><surname>Hämäläinen</surname><given-names>Matti S</given-names></name>
        </person-group>
        <article-title>MNE software for processing MEG and EEG data</article-title>
        <source>Neuroimage</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <volume>86</volume>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.027</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-oostenveld2011fieldtrip">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Oostenveld</surname><given-names>Robert</given-names></name>
          <name><surname>Fries</surname><given-names>Pascal</given-names></name>
          <name><surname>Maris</surname><given-names>Eric</given-names></name>
          <name><surname>Schoffelen</surname><given-names>Jan-Mathijs</given-names></name>
        </person-group>
        <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>
        <source>Computational intelligence and neuroscience</source>
        <publisher-name>Hindawi</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <volume>2011</volume>
        <pub-id pub-id-type="doi">10.1155/2011/156869</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-bezanson2017julia">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
          <name><surname>Edelman</surname><given-names>Alan</given-names></name>
          <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
          <name><surname>Shah</surname><given-names>Viral B</given-names></name>
        </person-group>
        <article-title>Julia: A fresh approach to numerical computing</article-title>
        <source>SIAM review</source>
        <publisher-name>SIAM</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>59</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-sapozhnykov2019wind">
      <element-citation publication-type="patent">
        <person-group person-group-type="author">
          <name><surname>Sapozhnykov</surname><given-names>Vitaliy</given-names></name>
          <name><surname>Harvey</surname><given-names>Thomas Ivan</given-names></name>
          <name><surname>Luke</surname><given-names>Robert</given-names></name>
        </person-group>
        <article-title>Wind noise measurement</article-title>
        <year iso-8601-date="2019">2019</year>
        <uri>https://patents.google.com/patent/US10504537B2/en</uri>
      </element-citation>
    </ref>
    <ref id="ref-robert2019blocked">
      <element-citation publication-type="patent">
        <person-group person-group-type="author">
          <name><surname>Luke</surname><given-names>Robert</given-names></name>
          <name><surname>Sapozhnykov</surname><given-names>Vitaliy</given-names></name>
          <name><surname>Harvey</surname><given-names>Thomas Ivan</given-names></name>
        </person-group>
        <article-title>Blocked microphone detection</article-title>
        <year iso-8601-date="2020">2020</year>
        <uri>https://patents.google.com/patent/US10412518B2/en</uri>
      </element-citation>
    </ref>
    <ref id="ref-sapozhnykov2020headset">
      <element-citation publication-type="patent">
        <person-group person-group-type="author">
          <name><surname>Sapozhnykov</surname><given-names>Vitaliy</given-names></name>
          <name><surname>Harvey</surname><given-names>Thomas Ivan</given-names></name>
          <name><surname>Erfaniansaeedi</surname><given-names>Nafiseh</given-names></name>
          <name><surname>Luke</surname><given-names>Robert</given-names></name>
        </person-group>
        <article-title>Headset on ear state detection</article-title>
        <year iso-8601-date="2019">2019</year>
        <uri>https://patents.google.com/patent/US10812889B2/en</uri>
      </element-citation>
    </ref>
    <ref id="ref-luke2016kalman">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Luke</surname><given-names>Robert</given-names></name>
          <name><surname>Wouters</surname><given-names>Jan</given-names></name>
        </person-group>
        <article-title>Kalman filter based estimation of auditory steady state response parameters</article-title>
        <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <volume>25</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1109/TNSRE.2016.2551302</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-DECHEVEIGNE2018206">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Cheveigné</surname><given-names>A. de</given-names></name>
          <name><surname>Wong</surname><given-names>D. D. E</given-names></name>
          <name><surname>Di Liberto</surname><given-names>G. M.</given-names></name>
          <name><surname>J.</surname><given-names>Hjortkjær</given-names></name>
          <name><surname>M.</surname><given-names>Slaney</given-names></name>
          <name><surname>Lalor</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Decoding the auditory brain with canonical component analysis</article-title>
        <source>NeuroImage</source>
        <year iso-8601-date="2018">2018</year>
        <volume>172</volume>
        <issn>1053-8119</issn>
        <uri>http://www.sciencedirect.com/science/article/pii/S1053811918300338</uri>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.033</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kim2017fast">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kim</surname><given-names>Jong Wook</given-names></name>
          <name><surname>Russell</surname><given-names>Spencer</given-names></name>
          <name><surname>Bello</surname><given-names>Juan</given-names></name>
        </person-group>
        <article-title>Fast music and audio processing using the Julia language</article-title>
        <source>Audio engineering society conference: 2017 AES international conference on semantic audio</source>
        <publisher-name>Audio Engineering Society</publisher-name>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
