<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3024</article-id>
<article-id pub-id-type="doi">10.21105/joss.03024</article-id>
<title-group>
<article-title>Groupyr: Sparse Group Lasso in Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-9276-9084</contrib-id>
<string-name>Adam Richie-Halford</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-5348-270X</contrib-id>
<string-name>Manjari Narayan</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-8985-2474</contrib-id>
<string-name>Noah Simon</string-name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-2686-1293</contrib-id>
<string-name>Jason Yeatman</string-name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0679-1985</contrib-id>
<string-name>Ariel Rokem</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>eScience Institute, University of Washington</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Psychiatry and Behavioral Sciences, Stanford
University</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Psychology, University of
Washington</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Department of Biostatistics, University of
Washington</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Graduate School of Education and Division of Developmental
and Behavioral Pediatrics, Stanford University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-12-25">
<day>25</day>
<month>12</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>58</issue>
<fpage>3024</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>group lasso</kwd>
<kwd>penalized regression</kwd>
<kwd>classification</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>For high-dimensional supervised learning, it is often beneficial to
  use domain-specific knowledge to improve the performance of
  statistical learning models. When the problem contains covariates
  which form groups, researchers can include this grouping information
  to find parsimonious representations of the relationship between
  covariates and targets. These groups may arise artificially, as from
  the polynomial expansion of a smaller feature space, or naturally, as
  from the anatomical grouping of different brain regions or the
  geographical grouping of different cities. When the number of features
  is large compared to the number of observations, one seeks a subset of
  the features which is sparse at both the group and global level.</p>
  <p>The sparse group lasso
  (<xref alt="Simon et al., 2013" rid="ref-simon2013sparse" ref-type="bibr">Simon
  et al., 2013</xref>) is a penalized regression technique designed for
  exactly these situations. It combines the original lasso
  (<xref alt="Tibshirani, 1996" rid="ref-tibshirani1996regression" ref-type="bibr">Tibshirani,
  1996</xref>), which induces global sparsity, with the group lasso
  (<xref alt="Yuan &amp; Lin, 2006" rid="ref-yuan2006model" ref-type="bibr">Yuan
  &amp; Lin, 2006</xref>), which induces group-level sparsity. It
  estimates a target variable <inline-formula><alternatives>
  <tex-math><![CDATA[\hat{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
  from a feature matrix <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>ùêó</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
  using</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \hat{y} = \mathbf{X} \hat{\beta},
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>ùêó</mml:mi></mml:mstyle><mml:mover><mml:mi>Œ≤</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>as depicted in
  <xref alt="Figure¬†1" rid="figU003Asgl_model">Figure¬†1</xref>, with
  color encoding the group structure of the covariates in
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>ùêó</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.
  The coefficients in <inline-formula><alternatives>
  <tex-math><![CDATA[\hat{\beta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>Œ≤</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
  characterize the relationship between the features and the target and
  must satisfy
  (<xref alt="Simon et al., 2013" rid="ref-simon2013sparse" ref-type="bibr">Simon
  et al., 2013</xref>)</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \hat{\beta} = \min_{\beta} \frac{1}{2}
  || y - \sum_{\ell = 1}^{G} \mathbf{X}^{(\ell)} \beta^{(\ell)} ||_2^2
  + (1 - \lambda) \alpha \sum_{\ell = 1}^{G} \sqrt{p_{\ell}} ||\beta^{(\ell)}||_2
  + \lambda \alpha ||\beta||_1,
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>Œ≤</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mo>min</mml:mo><mml:mi>Œ≤</mml:mi></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo>‚àí</mml:mo><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>‚Ñì</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>G</mml:mi></mml:munderover><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>ùêó</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚Ñì</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>Œ≤</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚Ñì</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>‚àí</mml:mo><mml:mi>Œª</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>Œ±</mml:mi><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>‚Ñì</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>G</mml:mi></mml:munderover><mml:msqrt><mml:msub><mml:mi>p</mml:mi><mml:mi>‚Ñì</mml:mi></mml:msub></mml:msqrt><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:msup><mml:mi>Œ≤</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚Ñì</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Œª</mml:mi><mml:mi>Œ±</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mi>Œ≤</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[G]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>
  is the total number of groups, <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}^{(\ell)}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>ùêó</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚Ñì</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
  is the submatrix of <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>ùêó</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  with columns belonging to group <inline-formula><alternatives>
  <tex-math><![CDATA[\ell]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚Ñì</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\beta^{(\ell)}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>Œ≤</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚Ñì</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
  is the coefficient vector of group <inline-formula><alternatives>
  <tex-math><![CDATA[\ell]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚Ñì</mml:mi></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives>
  <tex-math><![CDATA[p_{\ell}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>p</mml:mi><mml:mi>‚Ñì</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  is the length of <inline-formula><alternatives>
  <tex-math><![CDATA[\beta^{(\ell)}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>Œ≤</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚Ñì</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>.
  The model hyperparameter <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>
  controls the combination of the group-lasso and the lasso, with
  <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda=0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œª</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  giving the group lasso fit and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda=1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œª</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  yielding the lasso fit. The hyperparameter
  <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œ±</mml:mi></mml:math></alternatives></inline-formula>
  controls the overall strength of the regularization.</p>
  <fig>
    <caption><p>A linear model, <inline-formula><alternatives>
    <tex-math><![CDATA[y = \mathbf{X} \cdot \beta]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>ùêó</mml:mi></mml:mstyle><mml:mo>‚ãÖ</mml:mo><mml:mi>Œ≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
    with grouped covariates. The feature matrix
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mathbf{X}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>ùêó</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    is color-coded to reveal a group structure. The coefficients in
    <inline-formula><alternatives>
    <tex-math><![CDATA[\beta]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œ≤</mml:mi></mml:math></alternatives></inline-formula>
    follow the same grouping.
    <styled-content id="figU003Asgl_model"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="groupyr_linear_model.pdf" xlink:title="" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><italic>Groupyr</italic> is a Python library that implements the
  sparse group lasso as scikit-learn
  (<xref alt="Buitinck et al., 2013" rid="ref-sklearn_api" ref-type="bibr">Buitinck
  et al., 2013</xref>;
  <xref alt="Pedregosa et al., 2011" rid="ref-sklearn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) compatible estimators. It satisfies the need for
  grouped penalized regression models that can be used interoperably in
  researcher‚Äôs real-world scikit-learn workflows. Some pre-existing
  Python libraries come close to satisfying this need.
  <ext-link ext-link-type="uri" xlink:href="http://contrib.scikit-learn.org/lightning/"><italic>Lightning</italic></ext-link>
  (<xref alt="Blondel &amp; Pedregosa, 2016" rid="ref-lightning-2016" ref-type="bibr">Blondel
  &amp; Pedregosa, 2016</xref>) is a Python library for large-scale
  linear classification and regression. It supports many solvers with a
  combination of the L1 and L2 penalties. However, it does not allow the
  user to specify groups of covariates (see, for example,
  <ext-link ext-link-type="uri" xlink:href="https://github.com/scikit-learn-contrib/lightning/issues/39">this
  GitHub issue</ext-link>). Another Python package,
  <ext-link ext-link-type="uri" xlink:href="https://group-lasso.readthedocs.io/en/latest/#"><italic>group_lasso</italic></ext-link>
  (<xref alt="Moe, 2020" rid="ref-group-lasso" ref-type="bibr">Moe,
  2020</xref>), is a well-designed and well-documented implementation of
  the sparse group lasso. It meets the basic API requirements of
  scikit-learn compatible estimators. However, we found that our
  implementation in <italic>groupyr</italic>, which relies on the
  <italic>copt</italic> optimization library
  (<xref alt="Fabian Pedregosa, 2020" rid="ref-copt" ref-type="bibr">Fabian
  Pedregosa, 2020</xref>), was between two and ten times faster for the
  problem sizes that we encounter in our research (see the repository‚Äôs
  examples directory for a performance comparison). Additionally, we
  needed estimators with built-in cross-validation support using both
  grid search and sequential model based optimization strategies. For
  example, the speed and cross-validation enhancements were crucial to
  using <italic>groupyr</italic> in <italic>AFQ-Insight</italic>, a
  neuroinformatics research library
  (<xref alt="Richie-Halford et al., 2019" rid="ref-richiehalford2019multidimensional" ref-type="bibr">Richie-Halford
  et al., 2019</xref>).</p>
</sec>
<sec id="usage">
  <title>Usage</title>
  <p><italic>Groupyr</italic> is available on the Python Package Index
  (PyPI) and can be installed with</p>
  <preformat>pip install groupyr</preformat>
  <p><italic>Groupyr</italic> is compatible with the scikit-learn API
  and its estimators offer the same instantiate,
  <monospace>fit</monospace>, <monospace>predict</monospace> workflow
  that will be familiar to scikit-learn users. See the online
  documentation for a detailed description of the API and examples in
  both classification and regression settings. Here, we describe only
  the key differences necessary for scikit-learn users to get started
  with <italic>groupyr</italic>.</p>
  <p>For syntactic parallelism with the scikit-learn
  <monospace>ElasticNet</monospace> estimator, we use the keyword
  <monospace>l1_ratio</monospace> to refer to SGL‚Äôs
  <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>
  hyperparameter. In addition to keyword parameters shared with
  scikit-learn‚Äôs <monospace>ElasticNet</monospace>,
  <monospace>ElasticNetCV</monospace>,
  <monospace>LogisticRegression</monospace>, and
  <monospace>LogisticRegressionCV</monospace> estimators, users must
  specify the group assignments for the columns of the feature matrix
  <monospace>X</monospace>. This is done during estimator instantiation
  using the <monospace>groups</monospace> parameter, which accepts a
  list of numpy arrays, where the <inline-formula><alternatives>
  <tex-math><![CDATA[i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>-th
  array specifies the feature indices of the
  <inline-formula><alternatives>
  <tex-math><![CDATA[i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>-th
  group. If no grouping information is provided, the default behavior
  assigns all features to one group.</p>
  <p><italic>Groupyr</italic> also offers cross-validation estimators
  that automatically select the best values of the hyperparameters
  <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œ±</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>
  using either an exhaustive grid search (with
  <monospace>tuning_strategy=&quot;grid&quot;</monospace>) or sequential
  model based optimization (SMBO) using the scikit-optimize library
  (with <monospace>tuning_strategy=&quot;bayes&quot;</monospace>). For
  the grid search strategy, our implementation is more efficient than
  using the base estimator with scikit-learn‚Äôs
  <monospace>GridSearchCV</monospace> because it makes use of
  warm-starting, where the model is fit along a pre-defined
  regularization path and the solution from the previous fit is used as
  the initial guess for the current hyperparameter value. The randomness
  associated with SMBO complicates the use of a warm start strategy; it
  can be difficult to determine which of the previously attempted
  hyperparameter combinations should provide the initial guess for the
  current evaluation. However, even without warm-starting, we find that
  the SMBO strategy usually outperforms grid search because far fewer
  evaluations are needed to arrive at the optimal hyperparameters. We
  provide examples of both strategies (grid search for a classification
  example and SMBO for a regression example) in the online
  documentation.</p>
</sec>
<sec id="author-statements-and-acknowledgments">
  <title>Author statements and acknowledgments</title>
  <p>The first author (referred to as A.R.H. below) is the lead and
  corresponding author. The last author (referred to as A.R.) is the
  primary supervisor and is responsible for funding acquisition. All
  other authors are listed in alphabetical order by surname. We describe
  contributions to the paper using the CRediT taxonomy
  (<xref alt="Brand et al., 2015" rid="ref-credit" ref-type="bibr">Brand
  et al., 2015</xref>). Writing ‚Äì Original Draft: A.R.H.; Writing ‚Äì
  Review &amp; Editing: A.R.H., N.S., J.Y., and A.R.; Conceptualization
  and methodology: A.R.H., N.S., and A.R.; Software and data curation:
  A.R.H., M.N., and A.R.; Validation: A.R.H. and M.N.; Resources: A.R.H.
  and A.R; Visualization: A.R.H.; Supervision: N.S., J.Y., and A.R.;
  Project Administration: A.R.H; Funding Acquisition: A.R.;</p>
  <p>Groupyr development was supported through a grant from the Gordon
  and Betty Moore Foundation and from the Alfred P. Sloan Foundation to
  the University of Washington eScience Institute, as well as NIMH BRAIN
  Initiative grant 1RF1MH121868-01 to Ariel Rokem at the University of
  Washington and through cloud credits from the Google Cloud
  Platform.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-tibshirani1996regression">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
        </person-group>
        <article-title>Regression Shrinkage and Selection Via the Lasso</article-title>
        <source>Journal of the Royal Statistical Society: Series B (Methodological)</source>
        <year iso-8601-date="1996">1996</year>
        <volume>58</volume>
        <issue>1</issue>
        <uri>https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x</uri>
        <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-simon2013sparse">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Simon</surname><given-names>Noah</given-names></name>
          <name><surname>Friedman</surname><given-names>Jerome</given-names></name>
          <name><surname>Hastie</surname><given-names>Trevor</given-names></name>
          <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
        </person-group>
        <article-title>A sparse-group lasso</article-title>
        <source>Journal of Computational and Graphical Statistics</source>
        <publisher-name>Taylor &amp; Francis</publisher-name>
        <year iso-8601-date="2013">2013</year>
        <volume>22</volume>
        <issue>2</issue>
        <uri>https://doi.org/10.1080/10618600.2012.681250</uri>
        <pub-id pub-id-type="doi">10.1080/10618600.2012.681250</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-yuan2006model">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Yuan</surname><given-names>Ming</given-names></name>
          <name><surname>Lin</surname><given-names>Yi</given-names></name>
        </person-group>
        <article-title>Model selection and estimation in regression with grouped variables</article-title>
        <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
        <year iso-8601-date="2006">2006</year>
        <volume>68</volume>
        <issue>1</issue>
        <uri>https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x</uri>
        <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00532.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-copt">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Fabian Pedregosa</surname><given-names>Gideon Dresdner</given-names><suffix>Geoffrey Negiar</suffix></name>
        </person-group>
        <article-title>Copt: Composite optimization in python</article-title>
        <year iso-8601-date="2020">2020</year>
        <uri>http://openopt.github.io/copt/</uri>
        <pub-id pub-id-type="doi">10.5281/zenodo.1283339</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-sklearn_api">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Buitinck</surname><given-names>Lars</given-names></name>
          <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
          <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
          <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
          <name><surname>Mueller</surname><given-names>Andreas</given-names></name>
          <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
          <name><surname>Niculae</surname><given-names>Vlad</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
          <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
          <name><surname>Grobler</surname><given-names>Jaques</given-names></name>
          <name><surname>Layton</surname><given-names>Robert</given-names></name>
          <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
          <name><surname>Joly</surname><given-names>Arnaud</given-names></name>
          <name><surname>Holt</surname><given-names>Brian</given-names></name>
          <name><surname>Varoquaux</surname><given-names>Ga√´l</given-names></name>
        </person-group>
        <article-title>API design for machine learning software: Experiences from the scikit-learn project</article-title>
        <source>ECML PKDD workshop: Languages for data mining and machine learning</source>
        <year iso-8601-date="2013">2013</year>
        <uri>https://arxiv.org/abs/1309.0238</uri>
      </element-citation>
    </ref>
    <ref id="ref-sklearn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
          <name><surname>Varoquaux</surname><given-names>Ga√´l</given-names></name>
          <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
          <name><surname>Michel</surname><given-names>Vincent</given-names></name>
          <name><surname>Thirion</surname><given-names>Bertrand</given-names></name>
          <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
          <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
          <name><surname>Weiss</surname><given-names>Ron</given-names></name>
          <name><surname>Dubourg</surname><given-names>Vincent</given-names></name>
          <name><surname>Vanderplas</surname><given-names>Jake</given-names></name>
          <name><surname>Passos</surname><given-names>Alexandre</given-names></name>
          <name><surname>Cournapeau</surname><given-names>David</given-names></name>
          <name><surname>Brucher</surname><given-names>Matthieu</given-names></name>
          <name><surname>Perrot</surname><given-names>Matthieu</given-names></name>
          <name><surname>Duchesnay</surname><given-names>√âdouard</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2011-11">2011</year><month>11</month>
        <volume>12</volume>
        <issn>1532-4435</issn>
      </element-citation>
    </ref>
    <ref id="ref-richiehalford2019multidimensional">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Richie-Halford</surname><given-names>Adam</given-names></name>
          <name><surname>Yeatman</surname><given-names>Jason</given-names></name>
          <name><surname>Simon</surname><given-names>Noah</given-names></name>
          <name><surname>Rokem</surname><given-names>Ariel</given-names></name>
        </person-group>
        <article-title>Multidimensional analysis and detection of informative features in diffusion MRI measurements of human white matter</article-title>
        <source>bioRxiv</source>
        <publisher-name>Cold Spring Harbor Laboratory</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://www.biorxiv.org/content/early/2019/12/20/2019.12.19.882928</uri>
        <pub-id pub-id-type="doi">10.1101/2019.12.19.882928</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-lightning-2016">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
          <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
        </person-group>
        <article-title>Lightning: large-scale linear classification, regression and ranking in Python</article-title>
        <year iso-8601-date="2016">2016</year>
        <uri>https://doi.org/10.5281/zenodo.200504</uri>
        <pub-id pub-id-type="doi">10.5281/zenodo.200504</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-group-lasso">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Moe</surname><given-names>Yngve Mardal</given-names></name>
        </person-group>
        <article-title>Group lasso</article-title>
        <year iso-8601-date="2020">2020</year>
        <uri>https://github.com/yngvem/group-lasso</uri>
      </element-citation>
    </ref>
    <ref id="ref-credit">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Brand</surname><given-names>Amy</given-names></name>
          <name><surname>Allen</surname><given-names>Liz</given-names></name>
          <name><surname>Altman</surname><given-names>Micah</given-names></name>
          <name><surname>Hlava</surname><given-names>Marjorie</given-names></name>
          <name><surname>Scott</surname><given-names>Jo</given-names></name>
        </person-group>
        <article-title>Beyond authorship: Attribution, contribution, collaboration, and credit</article-title>
        <source>Learned Publishing</source>
        <year iso-8601-date="2015">2015</year>
        <volume>28</volume>
        <issue>2</issue>
        <uri>https://onlinelibrary.wiley.com/doi/abs/10.1087/20150211</uri>
        <pub-id pub-id-type="doi">10.1087/20150211</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
