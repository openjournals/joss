<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2844</article-id>
<article-id pub-id-type="doi">10.21105/joss.02844</article-id>
<title-group>
<article-title>c-lasso - a Python package for constrained sparse and
robust regression and classification</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<string-name>Léo Simpson</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Patrick L. Combettes</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-3821-7083</contrib-id>
<string-name>Christian L. Müller</string-name>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-4"/>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Technische Universität München</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Mathematics, North Carolina State University,
Raleigh</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Center for Computational Mathematics, Flatiron Institute,
New York</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Institute of Computational Biology, Helmholtz Zentrum
München</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Department of Statistics, Ludwig-Maximilians-Universität
München</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-10-09">
<day>9</day>
<month>10</month>
<year>2020</year>
</pub-date>
<volume>6</volume>
<issue>57</issue>
<fpage>2844</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>regression</kwd>
<kwd>classification</kwd>
<kwd>constrained regression</kwd>
<kwd>Lasso</kwd>
<kwd>Huber function</kwd>
<kwd>Square Hinge SVM</kwd>
<kwd>convex optimization</kwd>
<kwd>perspective function</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>We introduce <monospace>c-lasso</monospace>, a Python package that
  enables sparse and robust linear regression and classification with
  linear equality constraints. The underlying statistical forward model
  is assumed to be of the following form:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  y = X \beta + \sigma \epsilon \qquad \textrm{subject to} \qquad C\beta=0
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mi>ϵ</mml:mi><mml:mspace width="2.0em"></mml:mspace><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="2.0em"></mml:mspace><mml:mi>C</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>Here, <inline-formula><alternatives>
  <tex-math><![CDATA[X \in \mathbb{R}^{n\times d}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  is a given design matrix and the vector <inline-formula><alternatives>
  <tex-math><![CDATA[y \in \mathbb{R}^{n}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  is a continuous or binary response vector. The matrix
  <inline-formula><alternatives>
  <tex-math><![CDATA[C]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>C</mml:mi></mml:math></alternatives></inline-formula>
  is a general constraint matrix. The vector
  <inline-formula><alternatives>
  <tex-math><![CDATA[\beta \in \mathbb{R}^{d}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  contains the unknown coefficients and <inline-formula><alternatives>
  <tex-math><![CDATA[\sigma]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>σ</mml:mi></mml:math></alternatives></inline-formula>
  an unknown scale. Prominent use cases are (sparse) log-contrast
  regression with compositional data <inline-formula><alternatives>
  <tex-math><![CDATA[X]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>,
  requiring the constraint <inline-formula><alternatives>
  <tex-math><![CDATA[1_d^T \beta = 0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msubsup><mml:mn>1</mml:mn><mml:mi>d</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  (<xref alt="Aitchion &amp; Bacon-Shone, 1984" rid="ref-AitchisonU003A1984" ref-type="bibr">Aitchion
  &amp; Bacon-Shone, 1984</xref>) and the Generalized Lasso which is a
  <italic>special case</italic> of the described problem (see, e.g,
  (<xref alt="James et al., 2020" rid="ref-JamesU003A2020" ref-type="bibr">James
  et al., 2020</xref>), Example 3). The <monospace>c-lasso</monospace>
  package provides estimators for inferring unknown coefficients and
  scale (i.e., perspective M-estimators
  (<xref alt="Combettes &amp; Müller, 2020a" rid="ref-CombettesU003A2020a" ref-type="bibr">Combettes
  &amp; Müller, 2020a</xref>)) of the form</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
      \min_{\beta \in \mathbb{R}^d, \sigma \in \mathbb{R}_{0}} f\left(X\beta - y,{\sigma} \right) + \lambda \left\lVert \beta\right\rVert_1 \qquad \textrm{subject to} \qquad  C\beta = 0
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>d</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mspace width="2.0em"></mml:mspace><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="2.0em"></mml:mspace><mml:mi>C</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>for several convex loss functions <inline-formula><alternatives>
  <tex-math><![CDATA[f(\cdot,\cdot)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
  This includes the constrained Lasso, the constrained scaled Lasso,
  sparse Huber M-estimators with linear equality constraints, and
  constrained (Huberized) Square Hinge Support Vector Machines (SVMs)
  for classification.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Currently, there is no Python package available that can solve
  these ubiquitous statistical estimation problems in a fast and
  efficient manner. <monospace>c-lasso</monospace> provides algorithmic
  strategies, including path and proximal splitting algorithms, to solve
  the underlying convex optimization problems with provable convergence
  guarantees. The <monospace>c-lasso</monospace> package is intended to
  fill the gap between popular Python tools such as
  <ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/"><monospace>scikit-learn</monospace></ext-link>
  which cannot solve these constrained problems and general-purpose
  optimization solvers such as
  <ext-link ext-link-type="uri" xlink:href="https://www.cvxpy.org"><monospace>cvxpy</monospace></ext-link>
  that do not scale well for these problems and/or are inaccurate.
  <monospace>c-lasso</monospace> can solve the estimation problems at a
  single regularization level, across an entire regularization path, and
  includes three model selection strategies for determining the
  regularization parameter: a theoretically-derived fixed
  regularization, k-fold cross-validation, and stability selection. We
  show several use cases of the package, including an application of
  sparse log-contrast regression tasks for compositional microbiome
  data, and highlight the seamless integration into
  <monospace>R</monospace> via
  <ext-link ext-link-type="uri" xlink:href="https://rstudio.github.io/reticulate/"><monospace>reticulate</monospace></ext-link>.</p>
</sec>
<sec id="functionalities">
  <title>Functionalities</title>
  <sec id="gettingstarted">
    <title>Installation and problem instantiation</title>
    <p><monospace>c-lasso</monospace> is available on pip and can be
    installed in the shell using</p>
    <preformat>pip install c-lasso</preformat>
    <p><monospace>c-lasso</monospace> is a stand-alone package and not
    yet compatible with the <monospace>scikit-learn</monospace> API. The
    central object in the <monospace>c-lasso</monospace> package is the
    instantiation of a <monospace>c-lasso</monospace> problem.</p>
    <code language="python"># Import the main class of the package
from classo import classo_problem

# Define a c-lasso problem instance with default setting, 
# given data X, y, and constraints C.
problem  = classo_problem(X, y, C)</code>
    <p>We next describe what type of problem instances are available and
    how to solve them.</p>
  </sec>
  <sec id="formulations">
    <title>Statistical problem formulations</title>
    <p>Depending on the type of and the prior assumptions on the data,
    the noise <inline-formula><alternatives>
    <tex-math><![CDATA[\epsilon]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ϵ</mml:mi></mml:math></alternatives></inline-formula>,
    and the model parameters, <monospace>c-lasso</monospace> allows for
    different estimation problem formulations. More specifically, the
    package can solve the following four regression-type and two
    classification-type formulations:</p>
    <sec id="R1">
      <title><italic>R1</italic> Standard constrained Lasso
      regression:</title>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
          \min_{\beta \in \mathbb{R}^d} \left\lVert X\beta - y \right\rVert^2 + \lambda \left\lVert \beta\right\rVert_1 \qquad \textrm{subject to} \qquad  C\beta = 0
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mspace width="2.0em"></mml:mspace><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="2.0em"></mml:mspace><mml:mi>C</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This is the standard Lasso problem with linear equality
      constraints on the <inline-formula><alternatives>
      <tex-math><![CDATA[\beta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>β</mml:mi></mml:math></alternatives></inline-formula>
      vector. The objective function combines Least-Squares (LS) for
      model fitting with the <inline-formula><alternatives>
      <tex-math><![CDATA[L_1]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>-norm
      penalty for sparsity.</p>
      <code language="python"># Formulation R1
problem.formulation.huber = False
problem.formulation.concomitant = False
problem.formulation.classification = False</code>
    </sec>
    <sec id="R2">
      <title><italic>R2</italic> Constrained sparse Huber
      regression:</title>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
          \min_{\beta \in \mathbb{R}^d} h_{\rho} (X\beta - y) + \lambda \left\lVert \beta\right\rVert_1 \qquad \textrm{subject to} \qquad  C\beta = 0
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mspace width="2.0em"></mml:mspace><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="2.0em"></mml:mspace><mml:mi>C</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This regression problem uses the
      <ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Huber_loss">Huber
      loss</ext-link> <inline-formula><alternatives>
      <tex-math><![CDATA[h_{\rho}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      as objective function for robust model fitting with an
      <inline-formula><alternatives>
      <tex-math><![CDATA[L_1]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
      penalty and linear equality constraints on the
      <inline-formula><alternatives>
      <tex-math><![CDATA[\beta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>β</mml:mi></mml:math></alternatives></inline-formula>
      vector. The default parameter <inline-formula><alternatives>
      <tex-math><![CDATA[\rho]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ρ</mml:mi></mml:math></alternatives></inline-formula>
      is set to <inline-formula><alternatives>
      <tex-math><![CDATA[1.345]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>1.345</mml:mn></mml:math></alternatives></inline-formula>
      (<xref alt="Huber, 1981" rid="ref-HuberU003A1981" ref-type="bibr">Huber,
      1981</xref>).</p>
      <code language="python"># Formulation R2
problem.formulation.huber = True
problem.formulation.concomitant = False
problem.formulation.classification = False</code>
    </sec>
    <sec id="R3">
      <title><italic>R3</italic> Constrained scaled Lasso
      regression:</title>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
          \min_{\beta \in \mathbb{R}^d, \sigma \in \mathbb{R}_{0}} \frac{\left\lVert X\beta - y \right\rVert^2}{\sigma} + \frac{n}{2} \sigma + \lambda \left\lVert \beta\right\rVert_1 \qquad \textrm{subject to} \qquad  C\beta = 0
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>d</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:munder><mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>σ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mspace width="2.0em"></mml:mspace><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="2.0em"></mml:mspace><mml:mi>C</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This formulation is the default problem formulation in
      <monospace>c-lasso</monospace>. It is similar to
      <xref alt="R1" rid="R1"><italic>R1</italic></xref> but allows for
      joint estimation of the (constrained)
      <inline-formula><alternatives>
      <tex-math><![CDATA[\beta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>β</mml:mi></mml:math></alternatives></inline-formula>
      vector and the standard deviation <inline-formula><alternatives>
      <tex-math><![CDATA[\sigma]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>σ</mml:mi></mml:math></alternatives></inline-formula>
      in a concomitant fashion
      (<xref alt="Combettes &amp; Müller, 2020a" rid="ref-CombettesU003A2020a" ref-type="bibr">Combettes
      &amp; Müller, 2020a</xref>,
      <xref alt="2020b" rid="ref-CombettesU003A2020b" ref-type="bibr">2020b</xref>).</p>
      <code language="python"># Formulation R3
problem.formulation.huber = False
problem.formulation.concomitant = True
problem.formulation.classification = False</code>
    </sec>
    <sec id="R4">
      <title><italic>R4</italic> Constrained sparse Huber regression
      with concomitant scale estimation:</title>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
          \min_{\beta \in \mathbb{R}^d, \sigma \in  \mathbb{R}_{0}} \left( h_{\rho} \left( \frac{X\beta - y}{\sigma} \right) + n \right) \sigma + \lambda \left\lVert \beta\right\rVert_1 \qquad \textrm{subject to} \qquad  C\beta = 0
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>d</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>σ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mspace width="2.0em"></mml:mspace><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="2.0em"></mml:mspace><mml:mi>C</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This formulation combines
      <xref alt="R2" rid="R2"><italic>R2</italic></xref> and
      <xref alt="R3" rid="R3"><italic>R3</italic></xref> allowing robust
      joint estimation of the (constrained)
      <inline-formula><alternatives>
      <tex-math><![CDATA[\beta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>β</mml:mi></mml:math></alternatives></inline-formula>
      vector and the scale <inline-formula><alternatives>
      <tex-math><![CDATA[\sigma]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>σ</mml:mi></mml:math></alternatives></inline-formula>
      in a concomitant fashion
      (<xref alt="Combettes &amp; Müller, 2020a" rid="ref-CombettesU003A2020a" ref-type="bibr">Combettes
      &amp; Müller, 2020a</xref>,
      <xref alt="2020b" rid="ref-CombettesU003A2020b" ref-type="bibr">2020b</xref>).</p>
      <code language="python"># Formulation R4
problem.formulation.huber = True
problem.formulation.concomitant = True
problem.formulation.classification = False</code>
    </sec>
    <sec id="C1">
      <title><italic>C1</italic> Constrained sparse classification with
      Square Hinge loss:</title>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
          \min_{\beta \in \mathbb{R}^d} \sum_{i=1}^n l(y_i x_i^\top\beta) + \lambda \left\lVert \beta\right\rVert_1 \qquad \textrm{subject to} \qquad  C\beta = 0
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mspace width="2.0em"></mml:mspace><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="2.0em"></mml:mspace><mml:mi>C</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>where <inline-formula><alternatives>
      <tex-math><![CDATA[x_i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      denotes the <inline-formula><alternatives>
      <tex-math><![CDATA[i^{th}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
      row of <inline-formula><alternatives>
      <tex-math><![CDATA[X]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>,
      <inline-formula><alternatives>
      <tex-math><![CDATA[y_i \in \{-1,1\}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
      and <inline-formula><alternatives>
      <tex-math><![CDATA[l(\cdot)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      is defined for <inline-formula><alternatives>
      <tex-math><![CDATA[r \in \mathbb{R}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>
      as:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      l(r) = \begin{cases} (1-r)^2 & if \quad r \leq 1 \\ 0 &if \quad r \geq 1 \end{cases}
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1.0em"></mml:mspace><mml:mi>r</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1.0em"></mml:mspace><mml:mi>r</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This formulation is similar to
      <xref alt="R1" rid="R1"><italic>R1</italic></xref> but adapted for
      classification tasks using the Square Hinge loss with
      (constrained) sparse <inline-formula><alternatives>
      <tex-math><![CDATA[\beta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>β</mml:mi></mml:math></alternatives></inline-formula>
      vector estimation
      (<xref alt="Lee &amp; Lin, 2013" rid="ref-LeeU003A2013" ref-type="bibr">Lee
      &amp; Lin, 2013</xref>).</p>
      <code language="python"># Formulation C1
problem.formulation.huber = False
problem.formulation.concomitant = False
problem.formulation.classification = True</code>
    </sec>
    <sec id="C2">
      <title><italic>C2</italic> Constrained sparse classification with
      Huberized Square Hinge loss:</title>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
          \min_{\beta \in \mathbb{R}^d}  \sum_{i=1}^n  l_{\rho}(y_i x_i^\top\beta) + \lambda \left\lVert \beta\right\rVert_1 \qquad \textrm{subject to} \qquad  C\beta = 0 \,.
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>l</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mspace width="2.0em"></mml:mspace><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="2.0em"></mml:mspace><mml:mi>C</mml:mi><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.167em"></mml:mspace><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This formulation is similar to
      <xref alt="C1" rid="C1"><italic>C1</italic></xref> but uses the
      Huberized Square Hinge loss <inline-formula><alternatives>
      <tex-math><![CDATA[l_{\rho}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>l</mml:mi><mml:mi>ρ</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      for robust classification with (constrained) sparse
      <inline-formula><alternatives>
      <tex-math><![CDATA[\beta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>β</mml:mi></mml:math></alternatives></inline-formula>
      vector estimation
      (<xref alt="Rosset &amp; Zhu, 2007" rid="ref-RossetU003A2007" ref-type="bibr">Rosset
      &amp; Zhu, 2007</xref>):</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      l_{\rho}(r) = \begin{cases} (1-r)^2 &if \quad \rho \leq r \leq 1 \\ (1-\rho)(1+\rho-2r) & if \quad r \leq \rho \\ 0 &if \quad r \geq 1 \end{cases}
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1.0em"></mml:mspace><mml:mi>ρ</mml:mi><mml:mo>≤</mml:mo><mml:mi>r</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>r</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1.0em"></mml:mspace><mml:mi>r</mml:mi><mml:mo>≤</mml:mo><mml:mi>ρ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="1.0em"></mml:mspace><mml:mi>r</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This formulation can be selected in
      <monospace>c-lasso</monospace> as follows:</p>
      <code language="python"># Formulation C2
problem.formulation.huber = True
problem.formulation.concomitant = False
problem.formulation.classification = True</code>
    </sec>
  </sec>
  <sec id="method">
    <title>Optimization schemes</title>
    <p>The problem formulations <italic>R1</italic>-<italic>C2</italic>
    require different algorithmic strategies for efficiently solving the
    underlying optimization problems. The <monospace>c-lasso</monospace>
    package implements four published algorithms with provable
    convergence guarantees. The package also includes novel algorithmic
    extensions to solve Huber-type problems using the mean-shift
    formulation
    (<xref alt="Mishra &amp; Müller, 2019" rid="ref-MishraU003A2019" ref-type="bibr">Mishra
    &amp; Müller, 2019</xref>). The following algorithmic schemes are
    implemented:</p>
    <list list-type="bullet">
      <list-item>
        <p>Path algorithms (<italic>Path-Alg</italic>): This algorithm
        follows the proposal in
        (<xref alt="Gaines et al., 2018" rid="ref-GainesU003A2018" ref-type="bibr">Gaines
        et al., 2018</xref>;
        <xref alt="Jeon et al., 2020" rid="ref-JeonU003A2020" ref-type="bibr">Jeon
        et al., 2020</xref>) and uses the fact that the solution path
        along <inline-formula><alternatives>
        <tex-math><![CDATA[\lambda]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
        is piecewise-affine
        (<xref alt="Rosset &amp; Zhu, 2007" rid="ref-RossetU003A2007" ref-type="bibr">Rosset
        &amp; Zhu, 2007</xref>). We also provide a novel efficient
        procedure that allows to derive the solution for the concomitant
        problem <italic>R3</italic> along the path with little
        computational overhead.</p>
      </list-item>
      <list-item>
        <p>Douglas-Rachford-type splitting method (<italic>DR</italic>):
        This algorithm can solve all regression problems
        <italic>R1-R4</italic>. It is based on Doulgas-Rachford
        splitting in a higher-dimensional product space and makes use of
        the proximity operators of the perspective of the LS objective
        (<xref alt="Combettes &amp; Müller, 2020a" rid="ref-CombettesU003A2020a" ref-type="bibr">Combettes
        &amp; Müller, 2020a</xref>,
        <xref alt="2020b" rid="ref-CombettesU003A2020b" ref-type="bibr">2020b</xref>).
        The Huber problem with concomitant scale <italic>R4</italic> is
        reformulated as scaled Lasso problem with mean shift vector
        (<xref alt="Mishra &amp; Müller, 2019" rid="ref-MishraU003A2019" ref-type="bibr">Mishra
        &amp; Müller, 2019</xref>) and thus solved in (n + d)
        dimensions.</p>
      </list-item>
      <list-item>
        <p>Projected primal-dual splitting method
        (<italic>P-PDS</italic>): This algorithm is derived from
        (<xref alt="Briceño-Arias &amp; López Rivera, 2019" rid="ref-BricenoU003A2020" ref-type="bibr">Briceño-Arias
        &amp; López Rivera, 2019</xref>) and belongs to the class of
        proximal splitting algorithms, extending the classical
        Forward-Backward (FB) (aka proximal gradient descent) algorithm
        to handle an additional linear equality constraint via
        projection. In the absence of a linear constraint, the method
        reduces to FB.</p>
      </list-item>
      <list-item>
        <p>Projection-free primal-dual splitting method
        (<italic>PF-PDS</italic>): This algorithm is a special case of
        an algorithm proposed in
        (<xref alt="Combettes &amp; Pesquet, 2012" rid="ref-CombettesU003A2012" ref-type="bibr">Combettes
        &amp; Pesquet, 2012</xref>) (Eq. 4.5) and also belongs to the
        class of proximal splitting algorithms. The algorithm does not
        require projection operators which may be beneficial when C has
        a more complex structure. In the absence of a linear constraint,
        the method reduces to the Forward-Backward-Forward scheme.</p>
      </list-item>
    </list>
    <p>The following table summarizes the available algorithms and their
    recommended use for each problem:</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="8%" />
          <col width="23%" />
          <col width="23%" />
          <col width="23%" />
          <col width="23%" />
        </colgroup>
        <thead>
          <tr>
            <th></th>
            <th align="center"><italic>Path-Alg</italic></th>
            <th align="center"><italic>DR</italic></th>
            <th align="center"><italic>P-PDS</italic></th>
            <th align="center"><italic>PF-PDS</italic></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><xref alt="R1" rid="R1"><italic>R1</italic></xref></td>
            <td align="center">use for large
            <inline-formula><alternatives>
            <tex-math><![CDATA[\lambda]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
            and path computation</td>
            <td align="center">use for small
            <inline-formula><alternatives>
            <tex-math><![CDATA[\lambda]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center">possible</td>
            <td align="center">use for complex constraints</td>
          </tr>
          <tr>
            <td><xref alt="R2" rid="R2"><italic>R2</italic></xref></td>
            <td align="center">use for large
            <inline-formula><alternatives>
            <tex-math><![CDATA[\lambda]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
            and path computation</td>
            <td align="center">use for small
            <inline-formula><alternatives>
            <tex-math><![CDATA[\lambda]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center">possible</td>
            <td align="center">use for complex constraints</td>
          </tr>
          <tr>
            <td><xref alt="R3" rid="R3"><italic>R3</italic></xref></td>
            <td align="center">use for large
            <inline-formula><alternatives>
            <tex-math><![CDATA[\lambda]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
            and path computation</td>
            <td align="center">use for small
            <inline-formula><alternatives>
            <tex-math><![CDATA[\lambda]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center">-</td>
            <td align="center">-</td>
          </tr>
          <tr>
            <td><xref alt="R4" rid="R4"><italic>R4</italic></xref></td>
            <td align="center">-</td>
            <td align="center">only option</td>
            <td align="center">-</td>
            <td align="center">-</td>
          </tr>
          <tr>
            <td><xref alt="C1" rid="C1"><italic>C1</italic></xref></td>
            <td align="center">only option</td>
            <td align="center">-</td>
            <td align="center">-</td>
            <td align="center">-</td>
          </tr>
          <tr>
            <td><xref alt="C2" rid="C2"><italic>C2</italic></xref></td>
            <td align="center">only option</td>
            <td align="center">-</td>
            <td align="center">-</td>
            <td align="center">-</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>The following Python snippet shows how to select a specific
    algorithm:</p>
    <code language="python">problem.numerical_method = &quot;Path-Alg&quot; 
# Alternative options: &quot;DR&quot;, &quot;P-PDS&quot;, and &quot;PF-PDS&quot; </code>
  </sec>
  <sec id="model">
    <title>Computation modes and model selection</title>
    <p>The <monospace>c-lasso</monospace> package provides several
    computation modes and model selection schemes for tuning the
    regularization parameter.</p>
    <list list-type="bullet">
      <list-item>
        <p><italic>Fixed Lambda</italic>: This setting lets the user
        choose a fixed parameter <inline-formula><alternatives>
        <tex-math><![CDATA[\lambda]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
        or a proportion <inline-formula><alternatives>
        <tex-math><![CDATA[l \in [0,1]]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
        such that <inline-formula><alternatives>
        <tex-math><![CDATA[\lambda = l\times \lambda_{\max}]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo>max</mml:mo></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.
        The default value is a scale-dependent tuning parameter that has
        been derived in
        (<xref alt="Shi et al., 2016" rid="ref-ShiU003A2016" ref-type="bibr">Shi
        et al., 2016</xref>) and applied in
        (<xref alt="Combettes &amp; Müller, 2020b" rid="ref-CombettesU003A2020b" ref-type="bibr">Combettes
        &amp; Müller, 2020b</xref>).</p>
      </list-item>
      <list-item>
        <p><italic>Path Computation</italic>: This setting allows the
        computation of a solution path for
        <inline-formula><alternatives>
        <tex-math><![CDATA[\lambda]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
        parameters in an interval <inline-formula><alternatives>
        <tex-math><![CDATA[[\lambda_{\min}, \lambda_{\max}]]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo>min</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo>max</mml:mo></mml:msub><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
        The solution path is computed via the <italic>Path-Alg</italic>
        scheme or via warm-starts for other optimization schemes.</p>
      </list-item>
    </list>
    <list list-type="bullet">
      <list-item>
        <p><italic>Cross Validation</italic>: This setting allows the
        selection of the regularization parameter
        <inline-formula><alternatives>
        <tex-math><![CDATA[\lambda]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
        via k-fold cross validation for <inline-formula><alternatives>
        <tex-math><![CDATA[\lambda \in [\lambda_{\min}, \lambda_{\max}]]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo>min</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mo>max</mml:mo></mml:msub><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
        Both the Minimum Mean Squared Error (or Deviance) (MSE) and the
        “One-Standard-Error rule” (1SE) are available
        (<xref alt="Hastie et al., 2009" rid="ref-HastieU003A2009" ref-type="bibr">Hastie
        et al., 2009</xref>).</p>
      </list-item>
      <list-item>
        <p><italic>Stability Selection</italic>: This setting allows the
        selection of the <inline-formula><alternatives>
        <tex-math><![CDATA[\lambda]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
        via stability selection
        (<xref alt="Combettes &amp; Müller, 2020b" rid="ref-CombettesU003A2020b" ref-type="bibr">Combettes
        &amp; Müller, 2020b</xref>;
        <xref alt="Lin et al., 2014" rid="ref-LinU003A2014" ref-type="bibr">Lin
        et al., 2014</xref>;
        <xref alt="Meinshausen &amp; Bühlmann, 2010" rid="ref-MeinshausenU003A2010" ref-type="bibr">Meinshausen
        &amp; Bühlmann, 2010</xref>). Three modes are available:
        selection at a fixed <inline-formula><alternatives>
        <tex-math><![CDATA[\lambda]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
        (<xref alt="Combettes &amp; Müller, 2020b" rid="ref-CombettesU003A2020b" ref-type="bibr">Combettes
        &amp; Müller, 2020b</xref>), selection of the q
        <italic>first</italic> variables entering the path (default
        setting), and of the q <italic>largest coefficients</italic> (in
        absolute value) across the path
        (<xref alt="Meinshausen &amp; Bühlmann, 2010" rid="ref-MeinshausenU003A2010" ref-type="bibr">Meinshausen
        &amp; Bühlmann, 2010</xref>).</p>
      </list-item>
    </list>
    <p>The Python syntax to use a specific computation mode and model
    selection is exemplified below:</p>
    <code language="python"># Example how to perform ath computation and cross-validation:
problem.model_selection.LAMfixed = False
problem.model_selection.PATH = True
problem.model_selection.CV = True
problem.model_selection.StabSel = False

# Example how to add stability selection to the problem instance
problem.model_selection.StabSel = True</code>
    <p>Each model selection procedure has additional meta-parameters
    that are described in the
    <ext-link ext-link-type="uri" xlink:href="https://c-lasso.readthedocs.io/en/latest/">Documentation</ext-link>.</p>
  </sec>
</sec>
<sec id="numerical-benchmarks">
  <title>Numerical benchmarks</title>
  <p>To evaluate optimization accuracy and running time of the different
  algorithms available in <monospace>c-lasso</monospace>, we provide
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Leo-Simpson/c-lasso/tree/master/benchmark">micro-benchmark</ext-link>
  experiments which also include
  <ext-link ext-link-type="uri" xlink:href="https://www.cvxpy.org">cvxpy</ext-link>,
  an open source convex optimization software, for baseline comparison.
  All experiments have been computed using Python 3.9.1 on a
  <monospace>MacBook Air</monospace> with a
  <monospace>1.8 GHz Intel Core i5</monospace> processor and
  <monospace>8 Gb 1600 MHz DDR3</monospace> memory, operating on macOS
  High Sierra.</p>
  <p>Figure 1 summarizes the results for the <italic>Path-Alg</italic>,
  <italic>DR</italic>, and <italic>P-PDS</italic> algorithms solving the
  regression formulation <xref alt="R1" rid="R1">R1</xref> for different
  samples sizes <inline-formula><alternatives>
  <tex-math><![CDATA[n]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
  and problem dimensions <inline-formula><alternatives>
  <tex-math><![CDATA[p]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>p</mml:mi></mml:math></alternatives></inline-formula>
  on synthetic data (using <monospace>c-lasso</monospace>’s data
  generator). We observe that <monospace>c-lasso</monospace>’s
  algorithms are faster and more accurate than the
  <monospace>cvx</monospace> baseline. For instance, for
  <inline-formula><alternatives>
  <tex-math><![CDATA[d=500]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  features and <inline-formula><alternatives>
  <tex-math><![CDATA[n=500]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  samples, the <italic>Path-Alg</italic> algorithm is about
  <inline-formula><alternatives>
  <tex-math><![CDATA[70]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>70</mml:mn></mml:math></alternatives></inline-formula>
  times faster than <monospace>cvx</monospace>.</p>
  <fig>
    <caption><p>Average running times (left panel) of
    <italic>Path-Alg</italic> (blue), <italic>P-PDS</italic> (yellow),
    <italic>DR</italic> (green), and cvx (red) at fixed
    <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda = 0.1]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    and corresponding average objective function value differences (with
    respect to the function value obtained by the
    <italic>Path-Alg</italic> solution as baseline) (right panel). Mean
    (and standard deviation) running time is calculated over 20 data
    replications for each sample size/dimension scenario
    <inline-formula><alternatives>
    <tex-math><![CDATA[(n,d)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
    On a single data set, the reported running time of an algorithm is
    the average time of five algorithm runs (to guard against system
    background process fluctuations).</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="figures/figure_benchmark.png" xlink:title="" />
  </fig>
  <p>The complete reproducible micro-benchmark is avaialable
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Leo-Simpson/c-lasso/tree/master/benchmark">here</ext-link>.</p>
</sec>
<sec id="computational-examples">
  <title>Computational examples</title>
  <sec id="toy-example-using-synthetic-data">
    <title>Toy example using synthetic data</title>
    <p>We illustrate the workflow of the <monospace>c-lasso</monospace>
    package on synthetic data using the built-in routine
    <monospace>random_data</monospace> which enables the generation of
    test problem instances with normally distributed data
    <inline-formula><alternatives>
    <tex-math><![CDATA[X]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>,
    sparse coefficient vectors <inline-formula><alternatives>
    <tex-math><![CDATA[\beta]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>β</mml:mi></mml:math></alternatives></inline-formula>,
    and constraints <inline-formula><alternatives>
    <tex-math><![CDATA[C \in \mathbb{R}^{k\times d}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>C</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
    <p>Here, we use a problem instance with
    <inline-formula><alternatives>
    <tex-math><![CDATA[n=100]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
    <inline-formula><alternatives>
    <tex-math><![CDATA[d=100]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
    a <inline-formula><alternatives>
    <tex-math><![CDATA[\beta]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>β</mml:mi></mml:math></alternatives></inline-formula>
    with five non-zero components, <inline-formula><alternatives>
    <tex-math><![CDATA[\sigma=0.5]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
    and a zero-sum contraint.</p>
    <code language="python">





from classo import classo_problem, random_data

n, d, d_nonzero, k, sigma = 100, 100, 5, 1, 0.5
(X, C, y), sol = random_data(
  n, d, d_nonzero, k, sigma,
  zerosum = True, seed = 123
)
print(&quot;Relevant variables  : {}&quot;.format(numpy.nonzero(sol)[0]))

problem = classo_problem(X, y, C)

problem.formulation.huber = True
problem.formulation.concomitant = False
problem.formulation.rho = 1.5

problem.model_selection.LAMfixed = True
problem.model_selection.PATH = True
problem.model_selection.LAMfixedparameters.rescaled_lam = True
problem.model_selection.LAMfixedparameters.lam = 0.1

problem.solve()

print(problem.solution)</code>
    <p>We use
    <xref alt="formulation" rid="formulations">formulation</xref>
    <xref alt="R2" rid="R2"><italic>R2</italic></xref> with
    <inline-formula><alternatives>
    <tex-math><![CDATA[\rho=1.5]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
    <xref alt="computation mode and model selections" rid="model">computation
    mode and model selections</xref> <italic>Fixed Lambda</italic> with
    <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda = 0.1\lambda_{\max}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:msub><mml:mi>λ</mml:mi><mml:mo>max</mml:mo></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
    <italic>Path computation</italic>, and <italic>Stability
    Selection</italic> (as per default).</p>
    <p>The corresponding output reads:</p>
    <preformat>Relevant variables  : [43 47 74 79 84]

 LAMBDA FIXED : 
   Selected variables :  43    47    74    79    84    
   Running time :  0.294s

 PATH COMPUTATION : 
   Running time :  0.566s

 STABILITY SELECTION : 
   Selected variables :  43    47    74    79    84    
   Running time :  5.3s</preformat>
    <p><monospace>c-lasso</monospace> allows standard visualization of
    the computed solutions, e.g., coefficient plots at fixed
    <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>,
    the solution path, the stability selection profile at the selected
    <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>,
    and the stability selection profile across the entire path.</p>
    <fig>
      <caption><p>Visualizations after calling
      problem.solution</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="figures/synthetic.png" xlink:title="" />
    </fig>
    <p>For this tuned example, the solutions at the fixed lambda and
    with stability selection recover the oracle solution. The solution
    vectors are stored in <monospace>problem.solution</monospace> and
    can be directly acccessed for each mode/model selection.</p>
    <code language="python"># Access to the estimated coefficient vector at a fixed lambda 
problem.solution.LAMfixed.beta</code>
    <p>Note that the run time for this <inline-formula><alternatives>
    <tex-math><![CDATA[d=100]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>-dimensional
    example for a single path computation is about 0.5 seconds on a
    standard laptop.</p>
  </sec>
  <sec id="log-contrast-regression-on-gut-microbiome-data">
    <title>Log-contrast regression on gut microbiome data</title>
    <p>We next illustrate the application of
    <monospace>c-lasso</monospace> on the
    <ext-link ext-link-type="uri" xlink:href="https://github.com/Leo-Simpson/c-lasso/tree/master/examples/COMBO_data"><monospace>COMBO</monospace>
    microbiome dataset</ext-link>
    (<xref alt="Combettes &amp; Müller, 2020b" rid="ref-CombettesU003A2020b" ref-type="bibr">Combettes
    &amp; Müller, 2020b</xref>;
    <xref alt="Lin et al., 2014" rid="ref-LinU003A2014" ref-type="bibr">Lin
    et al., 2014</xref>;
    <xref alt="Shi et al., 2016" rid="ref-ShiU003A2016" ref-type="bibr">Shi
    et al., 2016</xref>). Here, the task is to predict the Body Mass
    Index (BMI) of <inline-formula><alternatives>
    <tex-math><![CDATA[n=96]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    participants from <inline-formula><alternatives>
    <tex-math><![CDATA[d=45]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    relative abundances of bacterial genera, and absolute calorie and
    fat intake measurements. The code snippet for this example is
    available in the
    <ext-link ext-link-type="uri" xlink:href="https://github.com/Leo-Simpson/c-lasso/README.md"><monospace>README.md</monospace></ext-link>
    and the
    <ext-link ext-link-type="uri" xlink:href="https://github.com/Leo-Simpson/c-lasso/blob/master/examples/example-notebook.ipynb">example
    notebook</ext-link>.</p>
    <fig>
      <caption><p>Stability selection profiles of problems R3/R4 on the
      COMBO data</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="figures/StabSelFilteredCOMBO.png" xlink:title="" />
    </fig>
    <p>Stability selection profiles using
    <xref alt="formulation" rid="formulations">formulation</xref>
    <xref alt="R3" rid="R3"><italic>R3</italic></xref> (left) and
    <xref alt="R4" rid="R4"><italic>R4</italic></xref>(right) on the
    COMBO dataset, reproducing Figure 5a in
    (<xref alt="Combettes &amp; Müller, 2020b" rid="ref-CombettesU003A2020b" ref-type="bibr">Combettes
    &amp; Müller, 2020b</xref>).</p>
  </sec>
  <sec id="calling-c-lasso-in-r">
    <title>Calling <monospace>c-lasso</monospace> in R</title>
    <p>The <monospace>c-lasso</monospace> package also integrates with
    <monospace>R</monospace> via the <monospace>R</monospace> package
    <ext-link ext-link-type="uri" xlink:href="https://rstudio.github.io/reticulate/"><monospace>reticulate</monospace></ext-link>.
    We refer to <monospace>reticulate</monospace>’s manual for technical
    details about connecting <monospace>python</monospace> environments
    and <monospace>R</monospace>. A successful use case of
    <monospace>c-lasso</monospace> is available in the
    <monospace>R</monospace> package
    <ext-link ext-link-type="uri" xlink:href="https://github.com/jacobbien/trac"><monospace>trac</monospace></ext-link>
    (<xref alt="Bien et al., 2020" rid="ref-BienU003A2020" ref-type="bibr">Bien
    et al., 2020</xref>), enabling tree-structured aggregation of
    predictors when features are rare.</p>
    <p>The code snippet below shows how <monospace>c-lasso</monospace>
    is called in <monospace>R</monospace> to perform regression at a
    fixed <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
    <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda = 0.1\lambda_{\max}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:msub><mml:mi>λ</mml:mi><mml:mo>max</mml:mo></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.
    In <monospace>R</monospace>, X and C need to be of
    <monospace>matrix</monospace> type, and y of
    <monospace>array</monospace> type.</p>
    <code language="r script">problem &lt;- classo$classo_problem(X = X, C = C, y = y) 
problem$model_selection$LAMfixed &lt;- TRUE
problem$model_selection$StabSel &lt;- FALSE
problem$model_selection$LAMfixedparameters$rescaled_lam &lt;- TRUE
problem$model_selection$LAMfixedparameters$lam &lt;- 0.1
problem$solve()

# Extract coefficent vector with tidy-verse
beta &lt;- as.matrix(map_dfc(problem$solution$LAMfixed$beta, as.numeric))</code>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The work of LS was conducted at and financially supported by the
  Center for Computational Mathematics (CCM), Flatiron Institute, New
  York, and the Institute of Computational Biology, Helmholtz Zentrum
  München. We thank Dr. Leslie Greengard (CCM and Courant Institute,
  NYU) for facilitating the initial contact between LS and CLM. The work
  of PLC was supported by the National Science Foundation under grant
  DMS-1818946.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-AitchisonU003A1984">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Aitchion</surname><given-names>J.</given-names></name>
          <name><surname>Bacon-Shone</surname><given-names>J.</given-names></name>
        </person-group>
        <article-title>Log contrast models for experiments with mixtures</article-title>
        <source>Biometrika</source>
        <year iso-8601-date="1984-08">1984</year><month>08</month>
        <volume>71</volume>
        <issue>2</issue>
        <issn>0006-3444</issn>
        <uri>https://doi.org/10.1093/biomet/71.2.323</uri>
        <pub-id pub-id-type="doi">10.1093/biomet/71.2.323</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BienU003A2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bien</surname><given-names>Jacob</given-names></name>
          <name><surname>Yan</surname><given-names>Xiaohan</given-names></name>
          <name><surname>Simpson</surname><given-names>Léo</given-names></name>
          <name><surname>Müller</surname><given-names>Christian L</given-names></name>
        </person-group>
        <article-title>Tree-Aggregated Predictive Modeling of Microbiome Data</article-title>
        <source>bioRxiv</source>
        <publisher-name>Cold Spring Harbor Laboratory</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <uri>https://www.biorxiv.org/content/early/2020/09/01/2020.09.01.277632</uri>
        <pub-id pub-id-type="doi">10.1101/2020.09.01.277632</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BricenoU003A2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Briceño-Arias</surname><given-names>Luis</given-names></name>
          <name><surname>López Rivera</surname><given-names>Sergio</given-names></name>
        </person-group>
        <article-title>A projected primal–dual method for solving constrained monotone inclusions</article-title>
        <source>Journal of Optimization Theory and Applications</source>
        <year iso-8601-date="2019">2019</year>
        <volume>180</volume>
        <issue>3</issue>
        <isbn>1573-2878</isbn>
        <uri>https://doi.org/10.1007/s10957-018-1430-2</uri>
        <pub-id pub-id-type="doi">10.1007/s10957-018-1430-2</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-CombettesU003A2012">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Combettes</surname><given-names>Patrick L.</given-names></name>
          <name><surname>Pesquet</surname><given-names>Jean-Christophe</given-names></name>
        </person-group>
        <article-title>Primal-Dual Splitting Algorithm for Solving Inclusions with Mixtures of Composite, Lipschitzian, and Parallel-Sum Type Monotone Operators</article-title>
        <source>Set-Valued and Variational Analysis</source>
        <year iso-8601-date="2012-06">2012</year><month>06</month>
        <volume>20</volume>
        <pub-id pub-id-type="doi">10.1007/s11228-011-0191-y</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-CombettesU003A2020a">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Combettes</surname><given-names>Patrick L.</given-names></name>
          <name><surname>Müller</surname><given-names>Christian L.</given-names></name>
        </person-group>
        <article-title>Perspective maximum likelihood-type estimation via proximal decomposition</article-title>
        <source>Electron. J. Statist.</source>
        <publisher-name>The Institute of Mathematical Statistics; the Bernoulli Society</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>14</volume>
        <issue>1</issue>
        <uri>https://doi.org/10.1214/19-EJS1662</uri>
        <pub-id pub-id-type="doi">10.1214/19-EJS1662</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-CombettesU003A2020b">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Combettes</surname><given-names>Patrick L.</given-names></name>
          <name><surname>Müller</surname><given-names>Christian L.</given-names></name>
        </person-group>
        <article-title>Regression models for compositional data: General log-contrast formulations, proximal optimization, and microbiome data applications</article-title>
        <source>Statistics in Biosciences</source>
        <year iso-8601-date="2020">2020</year>
        <isbn>1867-1772</isbn>
        <uri>https://doi.org/10.1007/s12561-020-09283-2</uri>
        <pub-id pub-id-type="doi">10.1007/s12561-020-09283-2</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-GainesU003A2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gaines</surname><given-names>Brian R.</given-names></name>
          <name><surname>Kim</surname><given-names>Juhyun</given-names></name>
          <name><surname>Zhou</surname><given-names>Hua</given-names></name>
        </person-group>
        <article-title>Algorithms for fitting the constrained lasso</article-title>
        <source>Journal of Computational and Graphical Statistics</source>
        <publisher-name>Taylor &amp; Francis</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>27</volume>
        <issue>4</issue>
        <uri> https://doi.org/10.1080/10618600.2018.1473777</uri>
        <pub-id pub-id-type="doi">10.1080/10618600.2018.1473777</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-JamesU003A2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>James</surname><given-names>Gareth M.</given-names></name>
          <name><surname>Paulson</surname><given-names>Courtney</given-names></name>
          <name><surname>Rusmevichientong</surname><given-names>Paat</given-names></name>
        </person-group>
        <article-title>Penalized and constrained optimization: An application to high-dimensional website advertising</article-title>
        <source>Journal of the American Statistical Association</source>
        <publisher-name>Taylor &amp; Francis</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>115</volume>
        <issue>529</issue>
        <uri>https://doi.org/10.1080/01621459.2019.1609970</uri>
        <pub-id pub-id-type="doi">10.1080/01621459.2019.1609970</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-HastieU003A2009">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Hastie</surname><given-names>T.</given-names></name>
          <name><surname>Tibshirani</surname><given-names>R.</given-names></name>
          <name><surname>Friedman</surname><given-names>J. H.</given-names></name>
        </person-group>
        <source>The elements of statistical learning: Data mining, inference, and prediction</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <isbn>9780387848846</isbn>
        <uri>https://books.google.fr/books?id=eBSgoAEACAAJ</uri>
        <pub-id pub-id-type="doi">10.1007/bf02985802</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-HuberU003A1981">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Huber</surname><given-names>P</given-names></name>
        </person-group>
        <source>Robust statistics</source>
        <publisher-name>John Wiley &amp; Sons Inc.</publisher-name>
        <year iso-8601-date="1981">1981</year>
        <isbn>0-471-41805-6</isbn>
      </element-citation>
    </ref>
    <ref id="ref-JeonU003A2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Jeon</surname><given-names>Jong-June</given-names></name>
          <name><surname>Kim</surname><given-names>Yongdai</given-names></name>
          <name><surname>Won</surname><given-names>Sungho</given-names></name>
          <name><surname>Choi</surname><given-names>Hosik</given-names></name>
        </person-group>
        <article-title>Primal path algorithm for compositional data analysis</article-title>
        <source>Computational Statistics &amp; Data Analysis</source>
        <year iso-8601-date="2020">2020</year>
        <volume>148</volume>
        <issn>0167-9473</issn>
        <uri>http://www.sciencedirect.com/science/article/pii/S0167947320300499</uri>
        <pub-id pub-id-type="doi">10.1016/j.csda.2020.106958</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-LeeU003A2013">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lee</surname><given-names>Ching-Pei</given-names></name>
          <name><surname>Lin</surname><given-names>Chih-Jen</given-names></name>
        </person-group>
        <article-title>A study on L2-loss (squared hinge-loss) multiclass SVM</article-title>
        <source>Neural computation</source>
        <year iso-8601-date="2013-03">2013</year><month>03</month>
        <volume>25</volume>
        <pub-id pub-id-type="doi">10.1162/NECO_a_00434</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-LinU003A2014">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lin</surname><given-names>Wei</given-names></name>
          <name><surname>Shi</surname><given-names>Pixu</given-names></name>
          <name><surname>Feng</surname><given-names>Rui</given-names></name>
          <name><surname>Li</surname><given-names>Hongzhe</given-names></name>
        </person-group>
        <article-title>Variable selection in regression with compositional covariates</article-title>
        <source>Biometrika</source>
        <year iso-8601-date="2014">2014</year>
        <volume>101</volume>
        <issue>4</issue>
        <issn>14643510</issn>
        <pub-id pub-id-type="doi">10.1093/biomet/asu031</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MeinshausenU003A2010">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Meinshausen</surname><given-names>Nicolai</given-names></name>
          <name><surname>Bühlmann</surname><given-names>Peter</given-names></name>
        </person-group>
        <article-title>Stability selection</article-title>
        <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
        <year iso-8601-date="2010">2010</year>
        <volume>72</volume>
        <issue>4</issue>
        <uri>https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00740.x</uri>
        <pub-id pub-id-type="doi">10.1111/j.1467-9868.2010.00740.x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MishraU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mishra</surname><given-names>Aditya</given-names></name>
          <name><surname>Müller</surname><given-names>Christian L.</given-names></name>
        </person-group>
        <article-title>Robust regression with compositional covariates</article-title>
        <year iso-8601-date="2019">2019</year>
        <uri>http://arxiv.org/abs/1909.04990</uri>
      </element-citation>
    </ref>
    <ref id="ref-RossetU003A2007">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Rosset</surname><given-names>Saharon</given-names></name>
          <name><surname>Zhu</surname><given-names>Ji</given-names></name>
        </person-group>
        <article-title>Piecewise linear regularized solution paths</article-title>
        <source>Annals of Statistics</source>
        <year iso-8601-date="2007">2007</year>
        <volume>35</volume>
        <issue>3</issue>
        <issn>00905364</issn>
        <pub-id pub-id-type="doi">10.1214/009053606000001370</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-ShiU003A2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shi</surname><given-names>Pixu</given-names></name>
          <name><surname>Zhang</surname><given-names>Anru</given-names></name>
          <name><surname>Li</surname><given-names>Hongzhe</given-names></name>
        </person-group>
        <article-title>Regression analysis for microbiome compositional data</article-title>
        <source>Annals of Applied Statistics</source>
        <year iso-8601-date="2016">2016</year>
        <volume>10</volume>
        <issue>2</issue>
        <issn>19417330</issn>
        <uri>https://arxiv.org/abs/1603.00974</uri>
        <pub-id pub-id-type="doi">10.1214/16-AOAS928</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
