<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">984</article-id>
<article-id pub-id-type="doi">10.21105/joss.00984</article-id>
<title-group>
<article-title>Mobile Gaze Mapping: A Python package for mapping mobile
gaze data to a fixed target stimulus</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<string-name>Jeff J MacInnes</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Shariq Iqbal</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<string-name>John Pearson</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Elizabeth N. Johnson</string-name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Institute for Learning and Brain Sciences, University of
Washington, Seattle, WA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Center for Cognitive Neuroscience, Duke University, Durham,
NC</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>University of Southern California, Los Angeles,
CA</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Wharton Neuroscience Initiative, University of
Pennsylvania, Philadelphia, PA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2018-08-03">
<day>3</day>
<month>8</month>
<year>2018</year>
</pub-date>
<volume>3</volume>
<issue>31</issue>
<fpage>984</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>openCV</kwd>
<kwd>computer vision</kwd>
<kwd>eye tracking</kwd>
<kwd>mobile eye tracking</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Mobile eye-trackers allow for measures like gaze position to be
  recorded under naturalistic conditions where an individual is free to
  move around. Gaze position is typically recorded relative to an
  outward facing camera attached to the eye-tracker and approximating
  the point-of-view of the individual wearing the device. As such, gaze
  position is recorded relative to the individual’s position and
  orientation, which changes as the participant moves. Since gaze
  position is recorded without any reference to fixed objects in the
  environment, this poses a challenge for studying how an individual
  views a particular stimulus over time.</p>
  <p>This toolkit addresses this challenge by automatically identifying
  the target stimulus on every frame of the recording and mapping the
  gaze positions to a fixed representation of the stimulus.</p>
  <p>It does this by identifying matching keypoints between the
  reference stimulus and each frame of the video. Keypoints are obtained
  using the Scale Invariant Feature Transform algorithm
  (SIFT)(<xref alt="Lowe, 2004" rid="ref-Lowe2004" ref-type="bibr">Lowe,
  2004</xref>), and matches between keypoints are found using the Fast
  Approximate Nearest Neighbor search algorithm
  (FLANN)(<xref alt="Muja &amp; Lowe, 2009" rid="ref-visapp09" ref-type="bibr">Muja
  &amp; Lowe, 2009</xref>), both implemented in
  OpenCV(<xref alt="Bradski, 2000" rid="ref-opencv_library" ref-type="bibr">Bradski,
  2000</xref>). (We note that while use of the SIFT algorithm is free
  for non-commercial, research, and educational purposes, all commercial
  applications require a purchased license.)</p>
  <p>Once matching keypoints have been identified, we determine the 2D
  linear transformation that maps keypoints from the video frame to key
  points on the reference image. Once determined, this same
  transformation is applied to the gaze position sample corresponding to
  the given video frame, resulting in gaze position being expressed in
  terms of the pixel coordinate system of the reference image.</p>
  <p>For a more detailed overview of this process, and the results of
  using this toolkit to compare gaze accuracy and precision across 3
  popular models of mobile eye-trackers, please see
  (<xref alt="MacInnes et al., 2018" rid="ref-MacInnes299925" ref-type="bibr">MacInnes
  et al., 2018</xref>)</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>This work was supported by Duke University’s Bass Connections -
  Brain &amp; Society, and the Big Data to Knowledge Initiative
  (K01-ES-025442) awarded to JP.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Lowe2004">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lowe</surname><given-names>David G.</given-names></name>
        </person-group>
        <article-title>Distinctive image features from scale-invariant keypoints</article-title>
        <source>International Journal of Computer Vision</source>
        <year iso-8601-date="2004-11-01">2004</year><month>11</month><day>01</day>
        <volume>60</volume>
        <issue>2</issue>
        <issn>1573-1405</issn>
        <uri>https://doi.org/10.1023/B:VISI.0000029664.99615.94</uri>
        <pub-id pub-id-type="doi">10.1023/B:VISI.0000029664.99615.94</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-visapp09">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Muja</surname><given-names>Marius</given-names></name>
          <name><surname>Lowe</surname><given-names>David G.</given-names></name>
        </person-group>
        <article-title>FAST APPROXIMATE NEAREST NEIGHBORS WITH AUTOMATIC ALGORITHM CONFIGURATION</article-title>
        <publisher-name>INSTICC; SciTePress</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <isbn>978-989-8111-69-2</isbn>
        <pub-id pub-id-type="doi">10.5220/0001787803310340</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-opencv_library">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bradski</surname><given-names>G.</given-names></name>
        </person-group>
        <article-title>The OpenCV Library</article-title>
        <source>Dr. Dobb’s Journal of Software Tools</source>
        <year iso-8601-date="2000">2000</year>
      </element-citation>
    </ref>
    <ref id="ref-MacInnes299925">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>MacInnes</surname><given-names>Jeff J</given-names></name>
          <name><surname>Iqbal</surname><given-names>Shariq</given-names></name>
          <name><surname>Pearson</surname><given-names>John</given-names></name>
          <name><surname>Johnson</surname><given-names>Elizabeth N</given-names></name>
        </person-group>
        <article-title>Wearable eye-tracking for research: Automated dynamic gaze mapping and accuracy/precision comparisons across devices</article-title>
        <source>bioRxiv</source>
        <publisher-name>Cold Spring Harbor Laboratory</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <uri>https://www.biorxiv.org/content/early/2018/06/28/299925</uri>
        <pub-id pub-id-type="doi">10.1101/299925</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
