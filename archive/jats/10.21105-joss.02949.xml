<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2949</article-id>
<article-id pub-id-type="doi">10.21105/joss.02949</article-id>
<title-group>
<article-title>NiaAML: AutoML framework based on stochastic
population-based nature-inspired algorithms</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-3897-9774</contrib-id>
<string-name>Luka Pečnik</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6418-1272</contrib-id>
<string-name>Iztok Fister Jr.</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Maribor, Faculty of Electrical Engineering
and Computer Science</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-12-20">
<day>20</day>
<month>12</month>
<year>2020</year>
</pub-date>
<volume>6</volume>
<issue>61</issue>
<fpage>2949</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>AutoML</kwd>
<kwd>classification</kwd>
<kwd>hyperparameter optimization</kwd>
<kwd>nature-inspired algorithms</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The field of Automated Machine Learning (AutoML) has been developed
  to automate data preprocessing and search for optimal algorithms
  together with their hyperparameters in order to discover the best
  possible ML pipeline for an input dataset
  (<xref alt="Hutter et al., 2019" rid="ref-bookHutter" ref-type="bibr">Hutter
  et al., 2019</xref>). AutoML can be modeled as a continuous
  optimization problem with several potential optimization methods
  considered. Stochastic population-based nature-inspired algorithms
  (<xref alt="Engelbrecht, 2007" rid="ref-engelbrecht2007computational" ref-type="bibr">Engelbrecht,
  2007</xref>;
  <xref alt="Yang, 2014" rid="ref-yang2014" ref-type="bibr">Yang,
  2014</xref>) are a popular class of tools for dealing with such
  continuous optimization problems. These algorithms are inspired mainly
  by the biological behavior of various species living in nature
  (<xref alt="Fister Jr et al., 2013" rid="ref-fister2013brief" ref-type="bibr">Fister
  Jr et al., 2013</xref>). Such algorithms are composed of a population
  of individuals that undergo different variation operations during the
  evolution process which results in new populations. The Python
  framework we have developed, NiaAML, incorporates these stochastic
  algorithms to search for the most suitable classification pipeline in
  a dataset
  (<xref alt="Fister Jr. et al., 2020" rid="ref-Fister2020Continuous" ref-type="bibr">Fister
  Jr. et al., 2020</xref>).</p>
  <p>The framework is developed in a layer style layout architecture
  consisting of several components, including feature selection
  algorithms, feature transformation algorithms and classifiers. Its
  task is to find a perfect combination of components with proper
  classifier hyperparameter settings to build an efficient, yet
  customizable classification pipeline with the help of a popular
  collection of nature-inspired algorithms, named NiaPy
  (<xref alt="Vrbančič et al., 2018" rid="ref-Vrbančič2018" ref-type="bibr">Vrbančič
  et al., 2018</xref>). NiaAML incorporates two types of optimizations,
  the first involves finding the optimal set of components for the
  pipeline, and the second involves tuning the hyperparameters. Users
  can freely choose the ML components to be included into the
  optimization process, as well as select suitable fitness functions to
  be used for evaluation of candidate pipelines. Input data can be in
  the form of numerical and categorical features, as well as missing
  attributes, while pipelines are exported and imported as binary files
  for post-hoc use. Further, they can be exported as user-friendly text
  files that contain all of the relevant information about the pipeline
  and its components. A graphical outline of the NiaAML method is
  presented in
  <xref alt="Figure 1" rid="figU003ANiaAMLflow">Figure 1</xref>.</p>
  <fig>
    <caption><p>NiaAML
    flow.<styled-content id="figU003ANiaAMLflow"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="niaamlFlow.png" xlink:title="" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Searching for an optimal classification pipeline in ML that
  provides the best results for a particular classification task
  involves a lot of domain-specific knowledge and numerous
  trial-and-error approaches. For instance, several conditions must be
  fulfilled to adequately apply an ML algorithm, such as proper
  preparation and preprocessing of input data, and selection of
  appropriate classifiers as well as their parameters. Due to this,
  mostly ML experts and data scientists have been able to handle such
  tasks in the past. However, empirical evidence shows that automation
  of the optimal classifier selection process, feature preprocessing
  steps and their hyperparameters, can be dealt with by non-experts as
  well
  (<xref alt="Feurer et al., 2015" rid="ref-NIPS2015_11d0e628" ref-type="bibr">Feurer
  et al., 2015</xref>;
  <xref alt="Guyon et al., 2015" rid="ref-7280767" ref-type="bibr">Guyon
  et al., 2015</xref>).</p>
  <p>With the rise of AutoML methods, dealing with ML has also become
  available to researchers from other fields. Compared to similar Python
  AutoML frameworks, such as TPOT
  (<xref alt="Olson &amp; Moore, 2019" rid="ref-tpot" ref-type="bibr">Olson
  &amp; Moore, 2019</xref>) and auto-sklearn
  (<xref alt="Feurer et al., 2015" rid="ref-NIPS2015_11d0e628" ref-type="bibr">Feurer
  et al., 2015</xref>), NiaAML offers the following benefits:</p>
  <list list-type="order">
    <list-item>
      <p>It is fully modeled as a continuous optimization problem, which
      means that arbitrary stochastic nature-inspired population-based
      algorithms can be used for solving this task without any special
      modifications of their internal mechanisms.</p>
    </list-item>
    <list-item>
      <p>The search for the optimal combination of ML components and
      proper classifier hyperparameters can be conducted
      concurrently.</p>
    </list-item>
    <list-item>
      <p>Its layer style architecture allows the straightforward
      addition of new ML components.</p>
    </list-item>
    <list-item>
      <p>Every pipeline in the output is feasible and functional in
      cases of the correct specified hyperparameters domains.</p>
    </list-item>
    <list-item>
      <p>A Graphical User Interface (GUI) further simplifies the work
      for users without expert domain knowledge.</p>
    </list-item>
  </list>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-fister2013brief">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Fister Jr</surname><given-names>Iztok</given-names></name>
          <name><surname>Yang</surname><given-names>Xin-She</given-names></name>
          <name><surname>Fister</surname><given-names>Iztok</given-names></name>
          <name><surname>Brest</surname><given-names>Janez</given-names></name>
          <name><surname>Fister</surname><given-names>Dušan</given-names></name>
        </person-group>
        <article-title>A brief review of nature-inspired algorithms for optimization</article-title>
        <source>arXiv preprint arXiv:1307.4186</source>
        <year iso-8601-date="2013">2013</year>
      </element-citation>
    </ref>
    <ref id="ref-Fister2020Continuous">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Fister Jr.</surname><given-names>Iztok</given-names></name>
          <name><surname>Zorman</surname><given-names>Milan</given-names></name>
          <name><surname>Fister</surname><given-names>Dušan</given-names></name>
          <name><surname>Fister</surname><given-names>Iztok</given-names></name>
        </person-group>
        <article-title>Continuous optimizers for automatic design and evaluation of classification pipelines</article-title>
        <source>Frontier applications of nature inspired computation</source>
        <year iso-8601-date="2020">2020</year>
        <pub-id pub-id-type="doi">10.1007/978-981-15-2133-1_13</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Vrbančič2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Vrbančič</surname><given-names>Grega</given-names></name>
          <name><surname>Brezočnik</surname><given-names>Lucija</given-names></name>
          <name><surname>Mlakar</surname><given-names>Uroš</given-names></name>
          <name><surname>Fister</surname><given-names>Dušan</given-names></name>
          <name><surname>Fister</surname><given-names>Iztok</given-names></name>
        </person-group>
        <article-title>NiaPy: Python microframework for building nature-inspired algorithms</article-title>
        <source>Journal of Open Source Software</source>
        <publisher-name>The Open Journal</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>3</volume>
        <issue>23</issue>
        <uri>https://doi.org/10.21105/joss.00613</uri>
        <pub-id pub-id-type="doi">10.21105/joss.00613</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-NIPS2015_11d0e628">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Feurer</surname><given-names>Matthias</given-names></name>
          <name><surname>Klein</surname><given-names>Aaron</given-names></name>
          <name><surname>Eggensperger</surname><given-names>Katharina</given-names></name>
          <name><surname>Springenberg</surname><given-names>Jost</given-names></name>
          <name><surname>Blum</surname><given-names>Manuel</given-names></name>
          <name><surname>Hutter</surname><given-names>Frank</given-names></name>
        </person-group>
        <article-title>Efficient and robust automated machine learning</article-title>
        <source>Advances in neural information processing systems</source>
        <person-group person-group-type="editor">
          <name><surname>Cortes</surname><given-names>C.</given-names></name>
          <name><surname>Lawrence</surname><given-names>N.</given-names></name>
          <name><surname>Lee</surname><given-names>D.</given-names></name>
          <name><surname>Sugiyama</surname><given-names>M.</given-names></name>
          <name><surname>Garnett</surname><given-names>R.</given-names></name>
        </person-group>
        <publisher-name>Curran Associates, Inc.</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>28</volume>
        <uri>https://proceedings.neurips.cc/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-7280767">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Guyon</surname><given-names>I.</given-names></name>
          <name><surname>Bennett</surname><given-names>K.</given-names></name>
          <name><surname>Cawley</surname><given-names>G.</given-names></name>
          <name><surname>Escalante</surname><given-names>H. J.</given-names></name>
          <name><surname>Escalera</surname><given-names>S.</given-names></name>
          <string-name>Tin Kam Ho</string-name>
          <name><surname>Macià</surname><given-names>N.</given-names></name>
          <name><surname>Ray</surname><given-names>B.</given-names></name>
          <name><surname>Saeed</surname><given-names>M.</given-names></name>
          <name><surname>Statnikov</surname><given-names>A.</given-names></name>
          <name><surname>Viegas</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Design of the 2015 ChaLearn AutoML challenge</article-title>
        <source>2015 international joint conference on neural networks (IJCNN)</source>
        <year iso-8601-date="2015">2015</year>
        <volume></volume>
        <pub-id pub-id-type="doi">10.1109/IJCNN.2015.7280767</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-tpot">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Olson</surname><given-names>Randal S.</given-names></name>
          <name><surname>Moore</surname><given-names>Jason H.</given-names></name>
        </person-group>
        <article-title>TPOT: A tree-based pipeline optimization tool for automating machine learning</article-title>
        <source>Automated machine learning</source>
        <publisher-name>Springer International Publishing</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <uri>https://doi.org/10.1007/978-3-030-05318-5_8</uri>
        <pub-id pub-id-type="doi">10.1007/978-3-030-05318-5_8</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-bookHutter">
      <element-citation publication-type="book">
        <source>Automated machine learning - methods, systems, challenges</source>
        <person-group person-group-type="editor">
          <name><surname>Hutter</surname><given-names>Frank</given-names></name>
          <name><surname>Kotthoff</surname><given-names>Lars</given-names></name>
          <name><surname>Vanschoren</surname><given-names>Joaquin</given-names></name>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <isbn>978-3-030-05317-8</isbn>
        <uri>https://doi.org/10.1007/978-3-030-05318-5</uri>
        <pub-id pub-id-type="doi">10.1007/978-3-030-05318-5</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-engelbrecht2007computational">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Engelbrecht</surname><given-names>Andries P</given-names></name>
        </person-group>
        <source>Computational intelligence: An introduction</source>
        <publisher-name>John Wiley &amp; Sons</publisher-name>
        <year iso-8601-date="2007">2007</year>
        <pub-id pub-id-type="doi">10.1002/9780470512517</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-yang2014">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Yang</surname><given-names>Xin-She</given-names></name>
        </person-group>
        <article-title>Nature-inspired optimization algorithms</article-title>
        <source>Nature-inspired optimization algorithms</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <uri>https://doi.org/10.1016/b978-0-12-416743-8.00017-8</uri>
        <pub-id pub-id-type="doi">10.1016/b978-0-12-416743-8.00017-8</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
