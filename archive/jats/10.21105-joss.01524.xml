<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1524</article-id>
<article-id pub-id-type="doi">10.21105/joss.01524</article-id>
<title-group>
<article-title>RL: Generic reinforcement learning codebase in
TensorFlow</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<string-name>Bryan M. Li</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Alexander Cowen-Rivers</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Piotr Kozakowski</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>David Tao</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Siddhartha Rao Kamalakara</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Nitarshan Rajkumar</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Hariharan Sezhiyan</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Sicong Huang</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Aidan N. Gomez</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>FOR.ai</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-06-12">
<day>12</day>
<month>6</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>42</issue>
<fpage>1524</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Reinforcement Learning</kwd>
<kwd>Tensorflow</kwd>
<kwd>Gym</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="abstract">
  <title>Abstract</title>
  <p>Vast reinforcement learning (RL) research groups, such as DeepMind
  and OpenAI, have their internal (private) reinforcement learning
  codebases, which enable quick prototyping and comparing of ideas to
  many state-of-the-art (SOTA) methods. We argue the five fundamental
  properties of a sophisticated research codebase are: modularity,
  reproducibility, many RL algorithms pre-implemented, speed and ease of
  running on different hardware/ integration with visualization
  packages. Currently, there does not exist any RL codebase, to the
  author’s knowledge, which contains all the five properties,
  particularly with TensorBoard logging and abstracting away cloud
  hardware such as TPU’s from the user. The codebase aims to help distil
  the best research practices into the community as well as ease the
  entry access and accelerate the pace of the field. More detailed
  documentation can be found
  <ext-link ext-link-type="uri" xlink:href="https://rl-codebase.readthedocs.io/en/latest/">here</ext-link>.</p>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>There are currently various implementations available for
  reinforcement learning codebase like OpenAI baselines
  (<xref alt="Dhariwal et al., 2017" rid="ref-dhariwalU003A2017" ref-type="bibr">Dhariwal
  et al., 2017</xref>), Stable baselines
  (<xref alt="Hill, 2019" rid="ref-hillU003A2019" ref-type="bibr">Hill,
  2019</xref>), Tensorforce
  (<xref alt="Schaarschmidt et al., 2017" rid="ref-schaarschmidtU003A2017" ref-type="bibr">Schaarschmidt
  et al., 2017</xref>), Ray rllib
  (<xref alt="Liang et al., 2017" rid="ref-liangU003A2017" ref-type="bibr">Liang
  et al., 2017</xref>), Intel Coach
  (<xref alt="Caspi et al., 2017" rid="ref-caspiU003A2017" ref-type="bibr">Caspi
  et al., 2017</xref>), Keras-RL
  (<xref alt="Plappert, 2019" rid="ref-kerasrlU003A2019" ref-type="bibr">Plappert,
  2019</xref>), Dopamine baselines
  (<xref alt="Castro et al., 2018" rid="ref-castroU003A2018" ref-type="bibr">Castro
  et al., 2018</xref>) and TF-Agents
  (<xref alt="Sergio Guadarrama, 2018" rid="ref-TFAgents" ref-type="bibr">Sergio
  Guadarrama, 2018</xref>). Ray rllib
  (<xref alt="Liang et al., 2017" rid="ref-liangU003A2017" ref-type="bibr">Liang
  et al., 2017</xref>) is amongst the strongest of existing RL
  frameworks, supporting; distributed operations, TensorFlow
  (<xref alt="Abadi et al., 2016" rid="ref-abadiU003A2016" ref-type="bibr">Abadi
  et al., 2016</xref>), PyTorch
  (<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>) and multi-agent reinforcement learning (MARL).
  Unlike Ray rllib, we choose to focus on Tensorflow support, allowing
  us to integrate specific framework visualisation and experiment
  tracking into our codebase. On top of this, we are developing a
  Kuberenetes script for MacOS and Linux users to connect to any cloud
  computing platform, such as Google TPU’s, Amazon AWS etc. Most other
  frameworks are plagued with problems like usability issues (difficult
  to get started and increment over), very little modularity in code
  (no/ little hierarchy and code reuse), no asynchronous training
  support, weak support for TensorBoard logging and so on. All these
  problems are solved by our project, which is a generic codebase built
  for reinforcement learning (RL) research in Tensorflow
  (<xref alt="Schaarschmidt et al., 2017" rid="ref-schaarschmidtU003A2017" ref-type="bibr">Schaarschmidt
  et al., 2017</xref>), with favoured RL agents pre-implemented as well
  as integration with OpenAI Gym
  (<xref alt="Brockman et al., 2016" rid="ref-brockmanU003A2016" ref-type="bibr">Brockman
  et al., 2016</xref>) environment focusing on quick prototyping and
  visualisation.</p>
  <p>Deep Reinforcement Learning Reinforcement learning refers to a
  paradigm in artificial intelligence where an agent performs a sequence
  of actions in an environment to maximise rewards
  (<xref alt="Sutton &amp; Barto, 1998" rid="ref-suttonU003A1998" ref-type="bibr">Sutton
  &amp; Barto, 1998</xref>). It is in many ways more general and
  challenging than supervised learning since it requires no labels to
  train on; instead, the agent interacts continuously with the
  environment, gathering more and more data and guiding its learning
  process.</p>
</sec>
<sec id="introduction-for-airl">
  <title>Introduction: for-ai/rl</title>
  <p>Further to the core ideas mentioned in the beginning, a good
  research codebase should enable good development practices such as
  continually checkpointing the model’s parameters as well as instantly
  restoring them to the latest checkpoint when available. Moreover, it
  should be composed of simple, interchangeable building blocks, making
  it easy to understand and to prototype new research ideas.</p>
  <p>We will first introduce the framework for this project, and then we
  will detail significant components. Lastly, we will discuss how to get
  started with training an agent under this framework.</p>
  <p>This codebase allows training RL agents by a training script as
  simple as the below for loop.</p>
  <code language="python">for epoch in range(epochs):
    state = env.reset()
    for step in range(max_episode_steps):
        last_state = state
        action = agent.act(state)
        state, reward, done = env.step(action)
        agent.observe(last_state, action, reward, state)
        agent.update()</code>
  <p>To accomplish this, we chose to modularise the codebase in the
  hierarchy shown below.</p>
  <preformat>rl_codebase
|- train.py
|---&gt; memory
|   |- registry.py
|---&gt; hparams
|   |- registry.py
|---&gt; envs
|   |- registry.py
|---&gt; models
|   |- registry.py
|---&gt; agents
|   |- registry.py
|   |---&gt; algos
|   |   |- registry.py
|   |   |---&gt; act_select
|   |   |   |- registry.py
|   |   |---&gt; grad_comp
|   |   |   |- registry.py</preformat>
  <p>Our modularisation enables simple and easy-to-read implementation
  of each component, such as the Agent, Algo and Environment class, as
  shown below.</p>
  <code language="python">class Agent:
    self.model: Model
    self.algo: Algo

    def observe(last_state, action, reward, new_state)
    def act(state) -&gt; action
    def update()

class Algo(Agent):
    def select_action(distribution) -&gt; action
    def compute_gradients(trajectory, parameters) -&gt; gradients

class Environment:
    def reset() -&gt; state
    def step(action) -&gt; state, reward, done</code>
  <p>The codebase includes agents like Deep Q Network
  (<xref alt="Mnih et al., 2013" rid="ref-mnihU003A2013" ref-type="bibr">Mnih
  et al., 2013</xref>), Noisy DQN
  (<xref alt="Plappert et al., 2017" rid="ref-plappertU003A2017" ref-type="bibr">Plappert
  et al., 2017</xref>), Vanilla Policy Gradient
  (<xref alt="Sutton et al., 2000" rid="ref-sutton2000" ref-type="bibr">Sutton
  et al., 2000</xref>), Deep Deterministic Policy Gradient
  (<xref alt="Silver et al., 2014" rid="ref-silver2014deterministic" ref-type="bibr">Silver
  et al., 2014</xref>) and Proximal Policy Optimization
  (<xref alt="Schulman et al., 2017" rid="ref-schulmanU003A2017" ref-type="bibr">Schulman
  et al., 2017</xref>). The project also includes simple random sampling
  and proportional prioritized experience replay approaches, support for
  Discrete and Box environments, option to render environment replay and
  record the replay in a video. The project also gives the possibility
  to conduct model-free asynchronous training, setting hyperparameters
  for your algorithm of choice, modularized action and gradient update
  functions and option to show your training logs in a TensorBoard
  summary.</p>
  <p>In order to run an experiment, run:</p>
  <p><monospace>python train.py --sys ... --hparams ... --output_dir ....</monospace></p>
  <p>Ideally, “train.py” should never need to be modified for any of the
  typical single agent environments. It covers the logging of reward,
  checkpointing, loading, rendering environment/ dealing with crashes
  and saving the experiments hyperparameters, which takes a significant
  workload off the average reinforcement learning researcher.</p>
  <preformat>“--sys”(str) defines the system chosen to run experiment with;  
  e.g. “local” for running on the local machine. 
“--env”(str) specifies the environment. 
“--hparam_override”(str) overrides hyperparameters. 
“--train_steps”(int) sets training length. 
“--test_episodes”(int) tests episodes.
“--eval_episodes”(int) sets Validation episodes.
“--training&quot;(bool) freeze model weights is set to False. 
“--copies”(int) set the number of times to perform multiple 
  versions of training/testing.
“--render”(bool) turns rendering on/ off. 
“--record_video”(bool) records the video with, which outputs a .mp4 
  of each recorded episode.
“--num_workers&quot;(int) seamlessly brings our synchronous agent 
  into an asynchronous agent.</preformat>
</sec>
<sec id="full-example">
  <title>Full Example</title>
  <p>Before you run a full examples, it would be to your benefit to
  install the following:</p>
  <list list-type="bullet">
    <list-item>
      <p>Nvidia CUDA on machines with GPUs to enable faster training.
      Installation instructions
      <ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/cuda-downloads">here</ext-link></p>
    </list-item>
    <list-item>
      <p>Tensorboard for training visualization. Install by running
      <monospace>pip install tensorboard</monospace></p>
    </list-item>
  </list>
  <p>This tuturial will make use of a Conda environment as the preferred
  package manager. Installation instructions can be found
  <ext-link ext-link-type="uri" xlink:href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/">here</ext-link>.</p>
  <p>After installing Conda, create and activate an environment, and
  install all dependencies within that environment:</p>
  <preformat>conda create -n rl python=3.6
conda activate rl
sh setup.sh</preformat>
  <p>To run locally, we will train DQN on the
  <monospace>Carpole-v1</monospace> Gym environment:</p>
  <preformat># start training
python train.py --sys local --hparams dqn_cartpole --output_dir /tmp/rl-testing
# run tensorboard
tensorboard --logdir /tmp/rl-testing
# test agent
python train.py --sys local --hparams dqn_cartpole --output_dir /tmp/rl-testing --test_only --render</preformat>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>We have outlined the benefits of using a highly modularised
  reinforcement learning codebase. The next stages of development for
  the RL codebase are implementing more SOTA model-free RL techniques
  (GAE, Rainbow, SAC, IMPALA), introducing model-based approaches, such
  as World Models
  (<xref alt="Ha &amp; Schmidhuber, 2018" rid="ref-haU003A2018" ref-type="bibr">Ha
  &amp; Schmidhuber, 2018</xref>), integrating into an open-sourced
  experiment managing tool and expanding the codebases compatibility
  with a broader range of environments, such as Habitat
  (<xref alt="Savva et al., 2019" rid="ref-savvaU003A2019" ref-type="bibr">Savva
  et al., 2019</xref>). We would also like to see automatic
  hyperparameter optimization techniques to be integrated, such as
  Bayesian Optimization method which was crucial to the success of some
  of DeepMinds most considerable reinforcement learning feats
  (<xref alt="Chen et al., 2018" rid="ref-chenU003A2018" ref-type="bibr">Chen
  et al., 2018</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to thank all other members of For.ai, for useful
  discussions and feedback.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-suttonU003A1998">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Sutton</surname><given-names>Richard S</given-names></name>
          <name><surname>Barto</surname><given-names>Andrew G</given-names></name>
        </person-group>
        <article-title>Introduction to reinforcement learning. Vol. 135</article-title>
        <publisher-name>Cambridge: MIT press</publisher-name>
        <year iso-8601-date="1998">1998</year>
      </element-citation>
    </ref>
    <ref id="ref-dhariwalU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
          <name><surname>Hesse</surname><given-names>Christopher</given-names></name>
          <name><surname>Klimov</surname><given-names>Oleg</given-names></name>
          <name><surname>Nichol</surname><given-names>Alex</given-names></name>
          <name><surname>Plappert</surname><given-names>Matthias</given-names></name>
          <name><surname>Radford</surname><given-names>Alec</given-names></name>
          <name><surname>Schulman</surname><given-names>John</given-names></name>
          <name><surname>Sidor</surname><given-names>Szymon</given-names></name>
          <name><surname>Wu</surname><given-names>Yuhuai</given-names></name>
          <name><surname>Zhokhov</surname><given-names>Peter</given-names></name>
        </person-group>
        <article-title>Openai baselines</article-title>
        <year iso-8601-date="2017">2017</year>
        <uri>https://github.com/openai/baselines</uri>
      </element-citation>
    </ref>
    <ref id="ref-schaarschmidtU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schaarschmidt</surname><given-names>Michael</given-names></name>
          <name><surname>Kuhnle</surname><given-names>Alexander</given-names></name>
          <name><surname>Fricke</surname><given-names>Kai</given-names></name>
        </person-group>
        <article-title>Tensorforce: A tensorflow library for applied reinforcement learning</article-title>
        <year iso-8601-date="2017">2017</year>
        <uri>https://github.com/tensorflow/tensorflow</uri>
      </element-citation>
    </ref>
    <ref id="ref-liangU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Liang</surname><given-names>Eric</given-names></name>
          <name><surname>Liaw</surname><given-names>Richard</given-names></name>
          <name><surname>Nishihara</surname><given-names>Robert</given-names></name>
          <name><surname>Moritz</surname><given-names>Philipp</given-names></name>
          <name><surname>Fox</surname><given-names>Roy</given-names></name>
          <name><surname>Gonzalez</surname><given-names>Joseph</given-names></name>
          <name><surname>Goldberg</surname><given-names>Ken</given-names></name>
          <name><surname>Stoica</surname><given-names>Ion</given-names></name>
        </person-group>
        <article-title>Ray rllib: A composable and scalable reinforcement learning library</article-title>
        <source>arXiv preprint arXiv:1712.09381</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-caspiU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Caspi</surname><given-names>Itai</given-names></name>
          <name><surname>Leibovich</surname><given-names>Gal</given-names></name>
          <name><surname>Novik</surname><given-names>Gal</given-names></name>
        </person-group>
        <article-title>Reinforcement learning coach.(dec. 2017)</article-title>
        <year iso-8601-date="2017">2017</year>
        <volume>1134899</volume>
        <uri>https://doi.org/10.5281/zenodo.1134899</uri>
        <pub-id pub-id-type="doi">10.5281/zenodo.1134899</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-abadiU003A2016">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
          <name><surname>Barham</surname><given-names>Paul</given-names></name>
          <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
          <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
          <name><surname>Davis</surname><given-names>Andy</given-names></name>
          <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
          <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
          <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
          <name><surname>Isard</surname><given-names>Michael</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Tensorflow: A system for large-scale machine learning</article-title>
        <source>12th USENIX symposium on operating systems design and implementation OSDI 16)</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-brockmanU003A2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Brockman</surname><given-names>Greg</given-names></name>
          <name><surname>Cheung</surname><given-names>Vicki</given-names></name>
          <name><surname>Pettersson</surname><given-names>Ludwig</given-names></name>
          <name><surname>Schneider</surname><given-names>Jonas</given-names></name>
          <name><surname>Schulman</surname><given-names>John</given-names></name>
          <name><surname>Tang</surname><given-names>Jie</given-names></name>
          <name><surname>Zaremba</surname><given-names>Wojciech</given-names></name>
        </person-group>
        <article-title>Openai gym</article-title>
        <source>arXiv preprint arXiv:1606.01540</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-kerasrlU003A2019">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Plappert</surname><given-names>Matthias</given-names></name>
        </person-group>
        <article-title>Keras-rl/keras-rl</article-title>
        <source>GitHub</source>
        <year iso-8601-date="2019-03">2019</year><month>03</month>
        <uri>https://github.com/keras-rl/keras-rl</uri>
      </element-citation>
    </ref>
    <ref id="ref-castroU003A2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Castro</surname><given-names>Pablo Samuel</given-names></name>
          <name><surname>Moitra</surname><given-names>Subhodeep</given-names></name>
          <name><surname>Gelada</surname><given-names>Carles</given-names></name>
          <name><surname>Kumar</surname><given-names>Saurabh</given-names></name>
          <name><surname>Bellemare</surname><given-names>Marc G</given-names></name>
        </person-group>
        <article-title>Dopamine: A research framework for deep reinforcement learning</article-title>
        <source>arXiv preprint arXiv:1812.06110</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-mnihU003A2013">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mnih</surname><given-names>Volodymyr</given-names></name>
          <name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name>
          <name><surname>Silver</surname><given-names>David</given-names></name>
          <name><surname>Graves</surname><given-names>Alex</given-names></name>
          <name><surname>Antonoglou</surname><given-names>Ioannis</given-names></name>
          <name><surname>Wierstra</surname><given-names>Daan</given-names></name>
          <name><surname>Riedmiller</surname><given-names>Martin</given-names></name>
        </person-group>
        <article-title>Playing atari with deep reinforcement learning</article-title>
        <source>arXiv preprint arXiv:1312.5602</source>
        <year iso-8601-date="2013">2013</year>
      </element-citation>
    </ref>
    <ref id="ref-plappertU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Plappert</surname><given-names>Matthias</given-names></name>
          <name><surname>Houthooft</surname><given-names>Rein</given-names></name>
          <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
          <name><surname>Sidor</surname><given-names>Szymon</given-names></name>
          <name><surname>Chen</surname><given-names>Richard Y</given-names></name>
          <name><surname>Chen</surname><given-names>Xi</given-names></name>
          <name><surname>Asfour</surname><given-names>Tamim</given-names></name>
          <name><surname>Abbeel</surname><given-names>Pieter</given-names></name>
          <name><surname>Andrychowicz</surname><given-names>Marcin</given-names></name>
        </person-group>
        <article-title>Parameter space noise for exploration</article-title>
        <source>arXiv preprint arXiv:1706.01905</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-sutton2000">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Sutton</surname><given-names>Richard S</given-names></name>
          <name><surname>McAllester</surname><given-names>David A.</given-names></name>
          <name><surname>Singh</surname><given-names>Satinder P.</given-names></name>
          <name><surname>Mansour</surname><given-names>Yishay</given-names></name>
        </person-group>
        <article-title>Policy gradient methods for reinforcement learning with function approximation</article-title>
        <source>Advances in Neural Information Processing Systems 12</source>
        <person-group person-group-type="editor">
          <name><surname>Solla</surname><given-names>S. A.</given-names></name>
          <name><surname>Leen</surname><given-names>T. K.</given-names></name>
          <name><surname>Müller</surname><given-names>K.</given-names></name>
        </person-group>
        <publisher-name>MIT Press</publisher-name>
        <year iso-8601-date="2000">2000</year>
      </element-citation>
    </ref>
    <ref id="ref-silver2014deterministic">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Silver</surname><given-names>David</given-names></name>
          <name><surname>Lever</surname><given-names>Guy</given-names></name>
          <name><surname>Heess</surname><given-names>Nicolas</given-names></name>
          <name><surname>Degris</surname><given-names>Thomas</given-names></name>
          <name><surname>Wierstra</surname><given-names>Daan</given-names></name>
          <name><surname>Riedmiller</surname><given-names>Martin</given-names></name>
        </person-group>
        <article-title>Deterministic policy gradient algorithms</article-title>
        <source>Proceedings of the 31st International Conference on Machine Learning</source>
        <person-group person-group-type="editor">
          <name><surname>Xing</surname><given-names>Eric P.</given-names></name>
          <name><surname>Jebara</surname><given-names>Tony</given-names></name>
        </person-group>
        <publisher-name>PMLR</publisher-name>
        <publisher-loc>Bejing, China</publisher-loc>
        <year iso-8601-date="2014">2014</year>
        <volume>32</volume>
        <uri>http://proceedings.mlr.press/v32/silver14.html</uri>
      </element-citation>
    </ref>
    <ref id="ref-schulmanU003A2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schulman</surname><given-names>John</given-names></name>
          <name><surname>Wolski</surname><given-names>Filip</given-names></name>
          <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
          <name><surname>Radford</surname><given-names>Alec</given-names></name>
          <name><surname>Klimov</surname><given-names>Oleg</given-names></name>
        </person-group>
        <article-title>Proximal policy optimization algorithms</article-title>
        <source>arXiv preprint arXiv:1707.06347</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-savvaU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Savva</surname><given-names>Manolis</given-names></name>
          <name><surname>Kadian</surname><given-names>Abhishek</given-names></name>
          <name><surname>Maksymets</surname><given-names>Oleksandr</given-names></name>
          <name><surname>Zhao</surname><given-names>Yili</given-names></name>
          <name><surname>Wijmans</surname><given-names>Erik</given-names></name>
          <name><surname>Jain</surname><given-names>Bhavana</given-names></name>
          <name><surname>Straub</surname><given-names>Julian</given-names></name>
          <name><surname>Liu</surname><given-names>Jia</given-names></name>
          <name><surname>Koltun</surname><given-names>Vladlen</given-names></name>
          <name><surname>Malik</surname><given-names>Jitendra</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Habitat: A platform for embodied ai research</article-title>
        <source>arXiv preprint arXiv:1904.01201</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-chenU003A2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Chen</surname><given-names>Yutian</given-names></name>
          <name><surname>Huang</surname><given-names>Aja</given-names></name>
          <name><surname>Wang</surname><given-names>Ziyu</given-names></name>
          <name><surname>Antonoglou</surname><given-names>Ioannis</given-names></name>
          <name><surname>Schrittwieser</surname><given-names>Julian</given-names></name>
          <name><surname>Silver</surname><given-names>David</given-names></name>
          <name><surname>Freitas</surname><given-names>Nando de</given-names></name>
        </person-group>
        <article-title>Bayesian optimization in alphago</article-title>
        <source>arXiv preprint arXiv:1812.06855</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-haU003A2018">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Ha</surname><given-names>David</given-names></name>
          <name><surname>Schmidhuber</surname><given-names>Jürgen</given-names></name>
        </person-group>
        <article-title>Recurrent world models facilitate policy evolution</article-title>
        <source>Advances in neural information processing systems</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-TFAgents">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Sergio Guadarrama</surname><given-names>Oscar Ramirez</given-names><suffix>Anoop Korattikara</suffix></name>
        </person-group>
        <article-title>TF-agents: A library for reinforcement learning in TensorFlow</article-title>
        <year iso-8601-date="2018">2018</year>
        <uri>&quot;https://github.com/tensorflow/agents&quot;</uri>
      </element-citation>
    </ref>
    <ref id="ref-hillU003A2019">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Hill</surname></name>
        </person-group>
        <article-title>Hill-a/stable-baselines</article-title>
        <year iso-8601-date="2019-06">2019</year><month>06</month>
        <uri>https://github.com/hill-a/stable-baselines</uri>
      </element-citation>
    </ref>
    <ref id="ref-paszke2017automatic">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Paszke</surname><given-names>Adam</given-names></name>
          <name><surname>Gross</surname><given-names>Sam</given-names></name>
          <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
          <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
          <name><surname>Yang</surname><given-names>Edward</given-names></name>
          <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
          <name><surname>Lin</surname><given-names>Zeming</given-names></name>
          <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
          <name><surname>Antiga</surname><given-names>Luca</given-names></name>
          <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        </person-group>
        <article-title>Automatic differentiation in PyTorch</article-title>
        <source>NIPS autodiff workshop</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
