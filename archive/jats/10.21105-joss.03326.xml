<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3326</article-id>
<article-id pub-id-type="doi">10.21105/joss.03326</article-id>
<title-group>
<article-title>libfmp: A Python Package for Fundamentals of Music
Processing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6062-7524</contrib-id>
<string-name>Meinard Müller</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1383-4541</contrib-id>
<string-name>Frank Zalkow</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>International Audio Laboratories Erlangen</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-04-27">
<day>27</day>
<month>4</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>63</issue>
<fpage>3326</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Music information retrieval</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The revolution in music distribution, storage, and consumption has
  fueled tremendous interest in developing techniques and tools for
  organizing, structuring, retrieving, navigating, and presenting
  music-related data. As a result, the academic field of music
  information retrieval (MIR) has matured over the last 20 years into an
  independent research area related to many different disciplines,
  including engineering, computer science, mathematics, and musicology.
  In this contribution, we introduce the Python package libfmp, which
  provides implementations of well-established model-based algorithms
  for various MIR tasks (with a focus on the audio domain), including
  beat tracking, onset detection, chord recognition, music
  synchronization, version identification, music segmentation, novelty
  detection, and audio decomposition. Such traditional approaches not
  only yield valuable baselines for modern data-driven strategies (e.g.,
  using deep learning) but are also instructive from an educational
  viewpoint deepening the understanding of the MIR task and music data
  at hand. Our libfmp package is inspired and closely follows
  conventions as introduced by librosa, which is a widely used Python
  library containing standardized and flexible reference implementations
  of many common methods in audio and music processing
  (<xref alt="McFee et al., 2015" rid="ref-McFeeRLEMBN15_librosa_Python" ref-type="bibr">McFee
  et al., 2015</xref>). While the two packages overlap concerning basic
  feature extraction and MIR algorithms, libfmp contains several
  reference implementations of advanced music processing pipelines not
  yet covered by librosa (or other open-source software). Whereas the
  librosa package is intended to facilitate the high-level composition
  of basic methods into complex pipelines, a major emphasis of libfmp is
  on the educational side, promoting the understanding of MIR concepts
  by closely following the textbook on Fundamentals of Music Processing
  (FMP)
  (<xref alt="Müller, 2015" rid="ref-Mueller15_FMP_SPRINGER" ref-type="bibr">Müller,
  2015</xref>). In this way, we hope that libfmp constitutes a valuable
  complement to existing open-source toolboxes such as librosa while
  fostering education and research in MIR.</p>
</sec>
<sec id="introduction">
  <title>Introduction</title>
  <p>Thanks to the rich and challenging domain of music, the MIR field
  has many things to offer to signal processing and other research
  disciplines
  (<xref alt="Müller et al., 2019" rid="ref-MuellerPMV19_Editorial_IEEE-SPM" ref-type="bibr">Müller
  et al., 2019</xref>). In particular, many MIR tasks can motivate
  application scenarios to introduce, explain, and study techniques for
  audio processing, time-series analysis, and information retrieval.
  Furthermore, the increasing availability of suitably designed software
  packages and freely accessible web-based frameworks have made
  education in MIR, signal processing, and general computer science more
  interactive. Since its beginnings, the MIR community has contributed
  several excellent toolboxes that provide modular source code for
  processing and analyzing music signals. Prominent examples are
  essentia
  (<xref alt="Bogdanov et al., 2013" rid="ref-BogdanovWGGHMRSZS13_essentia_ISMIR" ref-type="bibr">Bogdanov
  et al., 2013</xref>), madmom
  (<xref alt="Böck et al., 2016" rid="ref-BoeckKSKW16_madmom_ACM-MM" ref-type="bibr">Böck
  et al., 2016</xref>), Marsyas
  (<xref alt="Tzanetakis, 2009" rid="ref-Tzanetakis09_MARSYAS_ACM-MM" ref-type="bibr">Tzanetakis,
  2009</xref>), or the MIRtoolbox
  (<xref alt="Lartillot &amp; Toiviainen, 2007" rid="ref-LartillotT07_MirToolbox_ISMIR" ref-type="bibr">Lartillot
  &amp; Toiviainen, 2007</xref>). These toolboxes are mainly designed
  for research-oriented access to audio processing, yielding code for
  audio feature extraction and various MIR applications. Providing
  modular and readable code in combination with advanced MIR pipelines,
  the librosa package
  (<xref alt="McFee et al., 2015" rid="ref-McFeeRLEMBN15_librosa_Python" ref-type="bibr">McFee
  et al., 2015</xref>) strives for a low barrier to entry MIR research,
  thus representing a prominent example of building a bridge between
  education and research.</p>
  <p>Motivated by educational considerations, the FMP notebooks, which
  are a comprehensive collection of educational material for teaching
  and learning fundamentals of music processing (FMP), were recently
  introduced
  (<xref alt="Müller &amp; Zalkow, 2019" rid="ref-MuellerZ19_FMP_ISMIR" ref-type="bibr">Müller
  &amp; Zalkow, 2019</xref>). Closely following the textbook by Müller
  (<xref alt="2015" rid="ref-Mueller15_FMP_SPRINGER" ref-type="bibr">2015</xref>),
  the FMP notebooks comprise detailed textbook-like explanations of
  central techniques and algorithms combined with Python code examples
  that illustrate how to implement the methods. All components,
  including the introductions of MIR scenarios, illustrations, sound
  examples, technical concepts, mathematical details, and code examples,
  are integrated into a unified framework based on Jupyter notebooks.
  Thus, the FMP notebooks provide an interactive framework for students
  to learn about and experiment with signal processing and MIR
  algorithms. However, when students transition from being learners to
  becoming researchers, they may outgrow the FMP notebooks and begin
  developing their own DSP methods and programs
  (<xref alt="Müller et al., 2021" rid="ref-MuellerMK_MusicEducation_IEEE-SPM" ref-type="bibr">Müller
  et al., 2021</xref>). The libfmp package is designed to aid students
  with this transition by providing proper software support for building
  and experimenting with complex MIR pipelines. Whereas the FMP
  notebooks offer an introduction to fundamental concepts in MIR step by
  step, the libfmp package bundles the presented core concepts in the
  form of well-documented and easy-to-use Python functions. While
  integrating and building on the librosa package, libfmp offers
  alternative implementations of basic concepts in signal processing
  (comparing and discussing differences in the FMP notebooks). As a main
  contribution, libfmp provides various reference implementations of
  previously published MIR methods, not yet covered by librosa and other
  publicly available Python software.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Going beyond and complementing existing Python packages, the libfmp
  package contains (previously unpublished) reference implementations of
  MIR algorithms from the literature as well as new Python
  implementations of previously published MATLAB toolboxes. For example,
  libfmp includes the core functionality of the MATLAB Tempogram Toolbox
  [Grosche &amp; Müller
  (<xref alt="2011a" rid="ref-GroscheM11_PLP_TASLP" ref-type="bibr">2011a</xref>);
  Grosche &amp; Müller
  (<xref alt="2011b" rid="ref-GroscheM11_TempogramToolbox_ISMIR-lateBreaking" ref-type="bibr">2011b</xref>)]<xref ref-type="fn" rid="fn1">1</xref>
  for computing various mid-level tempogram representations
  (<xref alt="Grosche et al., 2010" rid="ref-GroscheMK10_TempogramCyclic_ICASSP" ref-type="bibr">Grosche
  et al., 2010</xref>) and for extracting beat and local pulse
  information
  (<xref alt="Grosche &amp; Müller, 2011a" rid="ref-GroscheM11_PLP_TASLP" ref-type="bibr">Grosche
  &amp; Müller, 2011a</xref>). Furthermore, libfmp extends the MATLAB SM
  Toolbox [Müller et al.
  (<xref alt="2014" rid="ref-MuellerJG14_SM-Toolbox_AES" ref-type="bibr">2014</xref>)]<xref ref-type="fn" rid="fn2">2</xref>
  for computing and enhancing similarity matrices with applications to
  music structure analysis. In particular, it contains a Python
  reference implementation for audio thumbnailing as introduced by
  Müller et al.
  (<xref alt="2013" rid="ref-MuellerJG13_StructureAnaylsis_IEEE-TASLP" ref-type="bibr">2013</xref>)
  and novel functionality for generating synthetic self-similarity
  matrices from reference annotations. Other examples of previously
  unpublished reference implementations from the literature include the
  computation of tempo curves
  (<xref alt="Müller et al., 2009" rid="ref-MuellerKSEC09_TempoParametersFromRecordings_ISMIR" ref-type="bibr">Müller
  et al., 2009</xref>), the fragment-level retrieval of different music
  performances
  (<xref alt="Müller et al., 2005" rid="ref-MuellerKC05_ChromaFeatures_ISMIR" ref-type="bibr">Müller
  et al., 2005</xref>), and the separation of audio signals into
  harmonic, percussive, and residual components
  (<xref alt="Driedger et al., 2014" rid="ref-DriedgerMD14_SeparationHP_ISMIR" ref-type="bibr">Driedger
  et al., 2014</xref>). Also, libfmp contains an implementation of a
  novel baseline algorithm for tuning estimation. Finally, as shown in
  the FMP notebooks, the functions of libfmp can be used to conduct
  systematic series of experiments as described in the literature for
  tasks such as musically informed audio decomposition
  (<xref alt="Ewert &amp; Müller, 2012" rid="ref-EwertM12_ScoreInformedNMF_ICASSP" ref-type="bibr">Ewert
  &amp; Müller, 2012</xref>) and automated chord recognition using
  different chroma feature types
  (<xref alt="Jiang et al., 2011" rid="ref-JiangGKM11_Chord_AES" ref-type="bibr">Jiang
  et al., 2011</xref>).</p>
</sec>
<sec id="design-choices">
  <title>Design Choices</title>
  <p>When designing libfmp, we had different objectives in mind. First,
  we tried to keep a close connection to the textbook by Müller
  (<xref alt="2015" rid="ref-Mueller15_FMP_SPRINGER" ref-type="bibr">2015</xref>)
  and the FMP notebooks
  (<xref alt="Müller &amp; Zalkow, 2019" rid="ref-MuellerZ19_FMP_ISMIR" ref-type="bibr">Müller
  &amp; Zalkow, 2019</xref>), thus establishing a close relationship
  between theory and practice. To this end, we carefully adopted naming
  conventions and the code structure to match the textbook’s
  mathematical notions and algorithmic concepts. Furthermore, we split
  the package libfmp into subpackages (called B, C1, C2, , C8)
  corresponding to the textbook’s chapters. Furthermore, the docstring
  of each libfmp function specifies the FMP notebook where the function
  is introduced (typically in a step-by-step fashion interleaved with
  additional explanations). Second, we followed many of the design
  principles suggested by librosa
  (<xref alt="McFee et al., 2015" rid="ref-McFeeRLEMBN15_librosa_Python" ref-type="bibr">McFee
  et al., 2015</xref>) to keep the entry barrier for students and
  researchers who may not be programming experts low. In particular, the
  programming style is kept explicit and straightforward with a flat,
  functional hierarchy. Third, based on expert knowledge, we specified a
  meaningful variable preset within each function, which allows users to
  experiment with the code immediately.</p>
  <p>The code of libmfp is hosted in a publicly available GitHub
  repository.<xref ref-type="fn" rid="fn3">3</xref> We also provide an
  API documentation for the libfmp
  functions,<xref ref-type="fn" rid="fn4">4</xref> which complements the
  educational step-by-step explanations given in the FMP
  notebooks.<xref ref-type="fn" rid="fn5">5</xref> Finally, we included
  the libfmp package into the Python package index PyPi, such that
  libfmp can be installed with the standard Python package manager
  pip.<xref ref-type="fn" rid="fn6">6</xref></p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The libfmp package builds on results, material, and insights that
  have been obtained in close collaboration with different people. We
  would like to express our gratitude to former and current students,
  collaborators, and colleagues who have influenced and supported us in
  creating this package, including Vlora Arifi-Müller, Stefan Balke,
  Michael Krause, Brian McFee, Sebastian Rosenzweig, Fabian-Robert
  Stöter, Steve Tjoa, Angel Villar-Corrales, Christof Weiß, and Tim
  Zunner. We also thank the German Research Foundation (DFG) for various
  research grants that allowed us for conducting fundamental research in
  music processing (in particular, DFG-MU 2686/11-1, DFG-MU 2686/12-1).
  The International Audio Laboratories Erlangen are a joint institution
  of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) and
  Fraunhofer Institute for Integrated Circuits IIS.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-McFeeRLEMBN15_librosa_Python">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>McFee</surname><given-names>Brian</given-names></name>
          <name><surname>Raffel</surname><given-names>Colin</given-names></name>
          <name><surname>Liang</surname><given-names>Dawen</given-names></name>
          <name><surname>Ellis</surname><given-names>Daniel P. W.</given-names></name>
          <name><surname>McVicar</surname><given-names>Matt</given-names></name>
          <name><surname>Battenberg</surname><given-names>Eric</given-names></name>
          <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
        </person-group>
        <article-title>Librosa: Audio and music signal analysis in Python</article-title>
        <source>Proceedings the python science conference</source>
        <publisher-loc>Austin, Texas, USA</publisher-loc>
        <year iso-8601-date="2015">2015</year>
        <pub-id pub-id-type="doi">10.25080/Majora-7b98e3ed-003</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Mueller15_FMP_SPRINGER">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
        </person-group>
        <source>Fundamentals of music processing</source>
        <publisher-name>Springer Verlag</publisher-name>
        <year iso-8601-date="2015">2015</year>
      </element-citation>
    </ref>
    <ref id="ref-MuellerPMV19_Editorial_IEEE-SPM">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>Pardo</surname><given-names>Bryan</given-names></name>
          <name><surname>Mysore</surname><given-names>Gautham J.</given-names></name>
          <name><surname>Välimäki</surname><given-names>Vesa</given-names></name>
        </person-group>
        <article-title>Recent advances in music signal processing</article-title>
        <source>IEEE Signal Processing Magazine</source>
        <year iso-8601-date="2019">2019</year>
        <volume>36</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1109/MSP.2018.2876190</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BogdanovWGGHMRSZS13_essentia_ISMIR">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Bogdanov</surname><given-names>Dmitry</given-names></name>
          <name><surname>Wack</surname><given-names>Nicolas</given-names></name>
          <name><surname>Gómez</surname><given-names>Emilia</given-names></name>
          <name><surname>Gulati</surname><given-names>Sankalp</given-names></name>
          <name><surname>Herrera</surname><given-names>Perfecto</given-names></name>
          <name><surname>Mayor</surname><given-names>Oscar</given-names></name>
          <name><surname>Roma</surname><given-names>Gerard</given-names></name>
          <name><surname>Salamon</surname><given-names>Justin</given-names></name>
          <name><surname>Zapata</surname><given-names>José R.</given-names></name>
          <name><surname>Serra</surname><given-names>Xavier</given-names></name>
        </person-group>
        <article-title>Essentia: An audio analysis library for music information retrieval</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <publisher-loc>Curitiba, Brazil</publisher-loc>
        <year iso-8601-date="2013">2013</year>
        <pub-id pub-id-type="doi">10.5281/zenodo.1415016</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BoeckKSKW16_madmom_ACM-MM">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Böck</surname><given-names>Sebastian</given-names></name>
          <name><surname>Korzeniowski</surname><given-names>Filip</given-names></name>
          <name><surname>Schlüter</surname><given-names>Jan</given-names></name>
          <name><surname>Krebs</surname><given-names>Florian</given-names></name>
          <name><surname>Widmer</surname><given-names>Gerhard</given-names></name>
        </person-group>
        <article-title>Madmom: A new Python audio and music signal processing library</article-title>
        <source>Proceedings of the ACM international conference on multimedia (ACM-MM)</source>
        <publisher-loc>Amsterdam, The Netherlands</publisher-loc>
        <year iso-8601-date="2016">2016</year>
        <pub-id pub-id-type="doi">10.1145/2964284.2973795</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Tzanetakis09_MARSYAS_ACM-MM">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Tzanetakis</surname><given-names>George</given-names></name>
        </person-group>
        <article-title>Music analysis, retrieval and synthesis of audio signals MARSYAS</article-title>
        <source>Proceedings of the ACM international conference on multimedia (ACM-MM)</source>
        <publisher-loc>Vancouver, British Columbia, Canada</publisher-loc>
        <year iso-8601-date="2009">2009</year>
        <pub-id pub-id-type="doi">10.1145/1631272.1631459</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-LartillotT07_MirToolbox_ISMIR">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Lartillot</surname><given-names>Olivier</given-names></name>
          <name><surname>Toiviainen</surname><given-names>Petri</given-names></name>
        </person-group>
        <article-title>MIR in MATLAB (II): A toolbox for musical feature extraction from audio</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <publisher-loc>Vienna, Austria</publisher-loc>
        <year iso-8601-date="2007">2007</year>
        <pub-id pub-id-type="doi">10.5281/zenodo.1417145</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MuellerZ19_FMP_ISMIR">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>Zalkow</surname><given-names>Frank</given-names></name>
        </person-group>
        <article-title>FMP Notebooks: Educational material for teaching and learning fundamentals of music processing</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <publisher-loc>Delft, The Netherlands</publisher-loc>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.5281/zenodo.3527872</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MuellerMK_MusicEducation_IEEE-SPM">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>McFee</surname><given-names>Brian</given-names></name>
          <name><surname>Kinnaird</surname><given-names>Katherine</given-names></name>
        </person-group>
        <article-title>Interactive learning of signal processing through music</article-title>
        <source>IEEE Signal Processing Magazine</source>
        <year iso-8601-date="2021">2021</year>
        <volume>Accepted for publication</volume>
        <issue></issue>
      </element-citation>
    </ref>
    <ref id="ref-GroscheM11_PLP_TASLP">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Grosche</surname><given-names>Peter</given-names></name>
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
        </person-group>
        <article-title>Extracting predominant local pulse information from music recordings</article-title>
        <source>IEEE Transactions on Audio, Speech, and Language Processing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <volume>19</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.1109/TASL.2010.2096216</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-GroscheM11_TempogramToolbox_ISMIR-lateBreaking">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Grosche</surname><given-names>Peter</given-names></name>
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
        </person-group>
        <article-title>Tempogram Toolbox: MATLAB tempo and pulse analysis of music recordings</article-title>
        <source>Demos and late breaking news of the international society for music information retrieval conference (ISMIR)</source>
        <publisher-loc>Miami, Florida, USA</publisher-loc>
        <year iso-8601-date="2011">2011</year>
      </element-citation>
    </ref>
    <ref id="ref-GroscheMK10_TempogramCyclic_ICASSP">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Grosche</surname><given-names>Peter</given-names></name>
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>Kurth</surname><given-names>Frank</given-names></name>
        </person-group>
        <article-title>Cyclic tempogram – a mid-level tempo representation for music signals</article-title>
        <source>Proceedings of IEEE international conference on acoustics, speech, and signal processing (ICASSP)</source>
        <publisher-loc>Dallas, Texas, USA</publisher-loc>
        <year iso-8601-date="2010">2010</year>
        <pub-id pub-id-type="doi">10.1109/ICASSP.2010.5495219</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MuellerJG14_SM-Toolbox_AES">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>Jiang</surname><given-names>Nanzhu</given-names></name>
          <name><surname>Grohganz</surname><given-names>Harald</given-names></name>
        </person-group>
        <article-title>SM Toolbox: MATLAB implementations for computing and enhancing similarity matrices</article-title>
        <source>Proceedings of the audio engineering society (AES) conference on semantic audio</source>
        <publisher-loc>London, UK</publisher-loc>
        <year iso-8601-date="2014">2014</year>
      </element-citation>
    </ref>
    <ref id="ref-MuellerJG13_StructureAnaylsis_IEEE-TASLP">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>Jiang</surname><given-names>Nanzhu</given-names></name>
          <name><surname>Grosche</surname><given-names>Peter</given-names></name>
        </person-group>
        <article-title>A robust fitness measure for capturing repetitions in music recordings with applications to audio thumbnailing</article-title>
        <source>IEEE Transactions on Audio, Speech, and Language Processing</source>
        <year iso-8601-date="2013">2013</year>
        <volume>21</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1109/TASL.2012.2227732</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MuellerKSEC09_TempoParametersFromRecordings_ISMIR">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>Konz</surname><given-names>Verena</given-names></name>
          <name><surname>Scharfstein</surname><given-names>Andi</given-names></name>
          <name><surname>Ewert</surname><given-names>Sebastian</given-names></name>
          <name><surname>Clausen</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>Towards automated extraction of tempo parameters from expressive music recordings</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <publisher-loc>Kobe, Japan</publisher-loc>
        <year iso-8601-date="2009">2009</year>
        <pub-id pub-id-type="doi">10.5281/zenodo.1416024</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MuellerKC05_ChromaFeatures_ISMIR">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>Kurth</surname><given-names>Frank</given-names></name>
          <name><surname>Clausen</surname><given-names>Michael</given-names></name>
        </person-group>
        <article-title>Audio matching via chroma-based statistical features</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <publisher-loc>London, UK</publisher-loc>
        <year iso-8601-date="2005">2005</year>
        <pub-id pub-id-type="doi">10.5281/zenodo.1416800</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-DriedgerMD14_SeparationHP_ISMIR">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Driedger</surname><given-names>Jonathan</given-names></name>
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
          <name><surname>Disch</surname><given-names>Sascha</given-names></name>
        </person-group>
        <article-title>Extending harmonic–percussive separation of audio signals</article-title>
        <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
        <publisher-loc>Taipei, Taiwan</publisher-loc>
        <year iso-8601-date="2014">2014</year>
        <pub-id pub-id-type="doi">10.5281/zenodo.1415226</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-EwertM12_ScoreInformedNMF_ICASSP">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Ewert</surname><given-names>Sebastian</given-names></name>
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
        </person-group>
        <article-title>Using score-informed constraints for NMF-based source separation</article-title>
        <source>Proceedings of the IEEE international conference on acoustics, speech, and signal processing (ICASSP)</source>
        <publisher-loc>Kyoto, Japan</publisher-loc>
        <year iso-8601-date="2012">2012</year>
        <pub-id pub-id-type="doi">10.1109/ICASSP.2012.6287834</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-JiangGKM11_Chord_AES">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Jiang</surname><given-names>Nanzhu</given-names></name>
          <name><surname>Grosche</surname><given-names>Peter</given-names></name>
          <name><surname>Konz</surname><given-names>Verena</given-names></name>
          <name><surname>Müller</surname><given-names>Meinard</given-names></name>
        </person-group>
        <article-title>Analyzing chroma feature types for automated chord recognition</article-title>
        <source>Proceedings of the AES conference on semantic audio</source>
        <publisher-loc>Ilmenau, Germany</publisher-loc>
        <year iso-8601-date="2011">2011</year>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://www.audiolabs-erlangen.de/resources/MIR/tempogramtoolbox">https://www.audiolabs-erlangen.de/resources/MIR/tempogramtoolbox</ext-link></p>
  </fn>
  <fn id="fn2">
    <label>2</label><p><ext-link ext-link-type="uri" xlink:href="https://www.audiolabs-erlangen.de/resources/MIR/SMtoolbox">https://www.audiolabs-erlangen.de/resources/MIR/SMtoolbox</ext-link></p>
  </fn>
  <fn id="fn3">
    <label>3</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/meinardmueller/libfmp">https://github.com/meinardmueller/libfmp</ext-link></p>
  </fn>
  <fn id="fn4">
    <label>4</label><p><ext-link ext-link-type="uri" xlink:href="https://meinardmueller.github.io/libfmp">https://meinardmueller.github.io/libfmp</ext-link></p>
  </fn>
  <fn id="fn5">
    <label>5</label><p><ext-link ext-link-type="uri" xlink:href="https://www.audiolabs-erlangen.de/FMP">https://www.audiolabs-erlangen.de/FMP</ext-link></p>
  </fn>
  <fn id="fn6">
    <label>6</label><p><ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/libfmp">https://pypi.org/project/libfmp</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
