<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2395</article-id>
<article-id pub-id-type="doi">10.21105/joss.02395</article-id>
<title-group>
<article-title>gobbli: A uniform interface to deep learning for text in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4127-3198</contrib-id>
<string-name>Jason Nance</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-3117-6239</contrib-id>
<string-name>Peter Baumgartner</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>RTI International</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-05-20">
<day>20</day>
<month>5</month>
<year>2020</year>
</pub-date>
<volume>6</volume>
<issue>62</issue>
<fpage>2395</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>deep learning</kwd>
<kwd>data science</kwd>
<kwd>classification</kwd>
<kwd>natural language processing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Machine learning has long been used to address natural language
  processing (NLP) tasks like sentiment analysis
  (<xref alt="Pang &amp; Lee, 2008" rid="ref-PangU003A2008" ref-type="bibr">Pang
  &amp; Lee, 2008</xref>) and document classification
  (<xref alt="Aggarwal &amp; Zhai, 2012" rid="ref-AggarwalU003A2012" ref-type="bibr">Aggarwal
  &amp; Zhai, 2012</xref>). Traditional approaches to these tasks
  require numerous labeled examples from the specific domains in which
  they will be applied. Such algorithms can only use the available
  training data, which is often limited in size and diversity, to learn
  to understand natural language. In the last few years, transfer
  learning
  (<xref alt="Weiss et al., 2016" rid="ref-WeissU003A2016" ref-type="bibr">Weiss
  et al., 2016</xref>) has caused a paradigm shift in NLP. Rather than
  training distinct task-specific models from scratch, a transfer
  learning model first learns the rules of language from a large,
  diverse text corpus during an extensive self-supervised training
  regimen. Self-supervised tasks are formulated such that unlabeled data
  can be used to train supervised models
  (<xref alt="Raina et al., 2007" rid="ref-RainaU003A2007" ref-type="bibr">Raina
  et al., 2007</xref>); an example is masked language modeling, where a
  subset of words in a document are masked out, and the model predicts
  the masked words
  (<xref alt="Devlin et al., 2018" rid="ref-DevlinU003A2018" ref-type="bibr">Devlin
  et al., 2018</xref>). The transfer learning model thus learns a rich
  representation of language, which can be fine-tuned to solve specific
  problems. According to Torrey &amp; Shavlik
  (<xref alt="2010" rid="ref-TorreyU003A2010" ref-type="bibr">2010</xref>),
  this approach mimics how humans reuse their general understanding of
  language across tasks. Transfer learning has not only rapidly advanced
  the state of the art in classification but has enabled near-human
  performance on more advanced tasks like question answering
  (<xref alt="Raffel et al., 2019" rid="ref-RaffelU003A2019" ref-type="bibr">Raffel
  et al., 2019</xref>;
  <xref alt="Rajpurkar et al., 2016" rid="ref-RajpurkarU003A2016" ref-type="bibr">Rajpurkar
  et al., 2016</xref>) and natural language inference
  (<xref alt="Bowman et al., 2015" rid="ref-BowmanU003A2015" ref-type="bibr">Bowman
  et al., 2015</xref>;
  <xref alt="Rajpurkar et al., 2016" rid="ref-RajpurkarU003A2016" ref-type="bibr">Rajpurkar
  et al., 2016</xref>;
  <xref alt="Williams et al., 2018" rid="ref-WilliamsU003A2018" ref-type="bibr">Williams
  et al., 2018</xref>).</p>
  <p>While the performance gains on benchmark tasks are undeniable,
  applied researchers face challenges using transfer learning models to
  solve new problems. A wide variety of models are being developed by
  disparate research teams using different technologies
  (<xref alt="Liu et al., 2019" rid="ref-LiuU003A2019" ref-type="bibr">Liu
  et al., 2019</xref>;
  <xref alt="Raffel et al., 2019" rid="ref-RaffelU003A2019" ref-type="bibr">Raffel
  et al., 2019</xref>;
  <xref alt="Sun et al., 2019" rid="ref-SunU003A2019" ref-type="bibr">Sun
  et al., 2019</xref>). A practitioner may therefore be required to
  learn a new programming language, a deep learning library, a
  containerization technology, and a model interface whenever they want
  to evaluate the feasibility of a new model on a custom task.
  <monospace>gobbli</monospace> was developed to address this
  problem.</p>
  <p><monospace>gobbli</monospace> is a Python library intended to
  bridge state-of-the-art research in natural language processing and
  application to real-world problems. The library defines a simple
  interface for training classification models, producing predictions,
  and generating embeddings. Several models implementing the interface
  are available using programmatically-created Docker containers to
  abstract away differences in underlying deep learning libraries and
  model hyperparameters. This approach allows users to easily evaluate
  models and compare performance across model types without spending
  time adapting their dataset and use case to each model. Compared to
  other deep learning libraries used for NLP like
  <monospace>transformers</monospace>
  (<xref alt="Wolf et al., 2019" rid="ref-WolfU003A2019" ref-type="bibr">Wolf
  et al., 2019</xref>) and <monospace>fastai</monospace>
  (<xref alt="Howard et al., 2018" rid="ref-HowardU003A2018" ref-type="bibr">Howard
  et al., 2018</xref>), <monospace>gobbli</monospace> is designed to
  emphasize simplicity and interoperability rather than customization
  and performance in order to make deep learning more accessible to
  applied researchers.</p>
  <p>Beyond its model wrappers, <monospace>gobbli</monospace> provides
  several helpful utilities for NLP practitioners. Data augmentation has
  emerged as a popular technique to improve model performance when
  training data is limited. Multiple methods for data augmentation are
  implemented in <monospace>gobbli</monospace>, including
  backtranslation
  (<xref alt="Shleifer, 2019" rid="ref-ShleiferU003A2019" ref-type="bibr">Shleifer,
  2019</xref>), word replacement
  (<xref alt="Wei &amp; Zou, 2019" rid="ref-WeiU003A2019" ref-type="bibr">Wei
  &amp; Zou, 2019</xref>), and contextual augmentation
  (<xref alt="Kobayashi, 2018" rid="ref-KobayashiU003A2018" ref-type="bibr">Kobayashi,
  2018</xref>). These methods can be used independently of
  <monospace>gobbli</monospace> models for interoperability with other
  modeling libraries. <monospace>gobbli</monospace> also bundles a set
  of interactive web applications built using Streamlit
  (<xref alt="Teixeira et al., 2018" rid="ref-TeixeiraU003A2018" ref-type="bibr">Teixeira
  et al., 2018</xref>) which can be used to explore a dataset, visualize
  embeddings, evaluate model performance, and explain model predictions
  without writing any code.</p>
  <p><monospace>gobbli</monospace> was developed from experiences on
  client contracts and internal projects at RTI International. It is
  intended for use by anyone solving problems using applied NLP,
  including researchers, students, and industry practitioners.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>Work on <monospace>gobbli</monospace> was funded by RTI
  International.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-PangU003A2008">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pang</surname><given-names>Bo</given-names></name>
          <name><surname>Lee</surname><given-names>Lillian</given-names></name>
        </person-group>
        <article-title>Opinion mining and sentiment analysis</article-title>
        <source>Foundations and Trends in Information Retrieval</source>
        <publisher-name>Now Publishers Inc.</publisher-name>
        <year iso-8601-date="2008">2008</year>
        <volume>2</volume>
        <issue>1-2</issue>
      </element-citation>
    </ref>
    <ref id="ref-AggarwalU003A2012">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Aggarwal</surname><given-names>Charu C</given-names></name>
          <name><surname>Zhai</surname><given-names>ChengXiang</given-names></name>
        </person-group>
        <article-title>A survey of text classification algorithms</article-title>
        <source>Mining text data</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <pub-id pub-id-type="doi">10.1007/978-1-4614-3223-4_6</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-WeissU003A2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Weiss</surname><given-names>Karl</given-names></name>
          <name><surname>Khoshgoftaar</surname><given-names>Taghi M</given-names></name>
          <name><surname>Wang</surname><given-names>DingDing</given-names></name>
        </person-group>
        <article-title>A survey of transfer learning</article-title>
        <source>Journal of Big Data</source>
        <publisher-name>SpringerOpen</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <volume>3</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
    <ref id="ref-TorreyU003A2010">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Torrey</surname><given-names>Lisa</given-names></name>
          <name><surname>Shavlik</surname><given-names>Jude</given-names></name>
        </person-group>
        <article-title>Transfer learning</article-title>
        <source>Handbook of research on machine learning applications and trends: Algorithms, methods, and techniques</source>
        <publisher-name>IGI Global</publisher-name>
        <year iso-8601-date="2010">2010</year>
      </element-citation>
    </ref>
    <ref id="ref-RajpurkarU003A2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Rajpurkar</surname><given-names>Pranav</given-names></name>
          <name><surname>Zhang</surname><given-names>Jian</given-names></name>
          <name><surname>Lopyrev</surname><given-names>Konstantin</given-names></name>
          <name><surname>Liang</surname><given-names>Percy</given-names></name>
        </person-group>
        <article-title>SQuAD: 100,000+ questions for machine comprehension of text</article-title>
        <source>arXiv preprint arXiv:1606.05250</source>
        <year iso-8601-date="2016">2016</year>
        <pub-id pub-id-type="doi">10.18653/v1/d16-1264</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-SunU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Sun</surname><given-names>Yu</given-names></name>
          <name><surname>Wang</surname><given-names>Shuohuan</given-names></name>
          <name><surname>Li</surname><given-names>Yukun</given-names></name>
          <name><surname>Feng</surname><given-names>Shikun</given-names></name>
          <name><surname>Chen</surname><given-names>Xuyi</given-names></name>
          <name><surname>Zhang</surname><given-names>Han</given-names></name>
          <name><surname>Tian</surname><given-names>Xin</given-names></name>
          <name><surname>Zhu</surname><given-names>Danxiang</given-names></name>
          <name><surname>Tian</surname><given-names>Hao</given-names></name>
          <name><surname>Wu</surname><given-names>Hua</given-names></name>
        </person-group>
        <article-title>ERNIE: Enhanced representation through knowledge integration</article-title>
        <source>arXiv preprint arXiv:1904.09223</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-RaffelU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Raffel</surname><given-names>Colin</given-names></name>
          <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
          <name><surname>Roberts</surname><given-names>Adam</given-names></name>
          <name><surname>Lee</surname><given-names>Katherine</given-names></name>
          <name><surname>Narang</surname><given-names>Sharan</given-names></name>
          <name><surname>Matena</surname><given-names>Michael</given-names></name>
          <name><surname>Zhou</surname><given-names>Yanqi</given-names></name>
          <name><surname>Li</surname><given-names>Wei</given-names></name>
          <name><surname>Liu</surname><given-names>Peter J</given-names></name>
        </person-group>
        <article-title>Exploring the limits of transfer learning with a unified text-to-text transformer</article-title>
        <source>arXiv preprint arXiv:1910.10683</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-LiuU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Liu</surname><given-names>Xiaodong</given-names></name>
          <name><surname>He</surname><given-names>Pengcheng</given-names></name>
          <name><surname>Chen</surname><given-names>Weizhu</given-names></name>
          <name><surname>Gao</surname><given-names>Jianfeng</given-names></name>
        </person-group>
        <article-title>Multi-task deep neural networks for natural language understanding</article-title>
        <source>arXiv preprint arXiv:1901.11504</source>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.18653/v1/p19-1441</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-WolfU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wolf</surname><given-names>Thomas</given-names></name>
          <name><surname>Debut</surname><given-names>Lysandre</given-names></name>
          <name><surname>Sanh</surname><given-names>Victor</given-names></name>
          <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
          <name><surname>Delangue</surname><given-names>Clement</given-names></name>
          <name><surname>Moi</surname><given-names>Anthony</given-names></name>
          <name><surname>Cistac</surname><given-names>Pierric</given-names></name>
          <name><surname>Rault</surname><given-names>Tim</given-names></name>
          <name><surname>Louf</surname><given-names>R’emi</given-names></name>
          <name><surname>Funtowicz</surname><given-names>Morgan</given-names></name>
          <name><surname>Brew</surname><given-names>Jamie</given-names></name>
        </person-group>
        <article-title>HuggingFace’s transformers: State-of-the-art natural language processing</article-title>
        <source>ArXiv</source>
        <year iso-8601-date="2019">2019</year>
        <volume>abs/1910.03771</volume>
      </element-citation>
    </ref>
    <ref id="ref-HowardU003A2018">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Howard</surname><given-names>Jeremy</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Fastai</article-title>
        <publisher-name>https://github.com/fastai/fastai; GitHub</publisher-name>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-WeiU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wei</surname><given-names>Jason W</given-names></name>
          <name><surname>Zou</surname><given-names>Kai</given-names></name>
        </person-group>
        <article-title>EDA: Easy data augmentation techniques for boosting performance on text classification tasks</article-title>
        <source>arXiv preprint arXiv:1901.11196</source>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.18653/v1/d19-1670</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-ShleiferU003A2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shleifer</surname><given-names>Sam</given-names></name>
        </person-group>
        <article-title>Low resource text classification with ULMFit and backtranslation</article-title>
        <source>arXiv preprint arXiv:1903.09244</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-TeixeiraU003A2018">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Teixeira</surname><given-names>Thiago</given-names></name>
          <name><surname>Treuille</surname><given-names>Adrien</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Streamlit</article-title>
        <publisher-name>https://github.com/streamlit/streamlit; GitHub</publisher-name>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-WilliamsU003A2018">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Williams</surname><given-names>Adina</given-names></name>
          <name><surname>Nangia</surname><given-names>Nikita</given-names></name>
          <name><surname>Bowman</surname><given-names>Samuel</given-names></name>
        </person-group>
        <article-title>A broad-coverage challenge corpus for sentence understanding through inference</article-title>
        <source>Proceedings of the 2018 conference of the north american chapter of the association for computational linguistics: Human language technologies, volume 1 (long papers)</source>
        <publisher-name>Association for Computational Linguistics</publisher-name>
        <publisher-loc>New Orleans, Louisiana</publisher-loc>
        <year iso-8601-date="2018">2018</year>
        <uri>http://aclweb.org/anthology/N18-1101</uri>
        <pub-id pub-id-type="doi">10.18653/v1/n18-1101</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-BowmanU003A2015">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bowman</surname><given-names>Samuel R</given-names></name>
          <name><surname>Angeli</surname><given-names>Gabor</given-names></name>
          <name><surname>Potts</surname><given-names>Christopher</given-names></name>
          <name><surname>Manning</surname><given-names>Christopher D</given-names></name>
        </person-group>
        <article-title>A large annotated corpus for learning natural language inference</article-title>
        <source>arXiv preprint arXiv:1508.05326</source>
        <year iso-8601-date="2015">2015</year>
        <pub-id pub-id-type="doi">10.18653/v1/d15-1075</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-RainaU003A2007">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Raina</surname><given-names>Rajat</given-names></name>
          <name><surname>Battle</surname><given-names>Alexis</given-names></name>
          <name><surname>Lee</surname><given-names>Honglak</given-names></name>
          <name><surname>Packer</surname><given-names>Benjamin</given-names></name>
          <name><surname>Ng</surname><given-names>Andrew Y</given-names></name>
        </person-group>
        <article-title>Self-taught learning: Transfer learning from unlabeled data</article-title>
        <source>Proceedings of the 24th international conference on machine learning</source>
        <year iso-8601-date="2007">2007</year>
      </element-citation>
    </ref>
    <ref id="ref-DevlinU003A2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Devlin</surname><given-names>Jacob</given-names></name>
          <name><surname>Chang</surname><given-names>Ming-Wei</given-names></name>
          <name><surname>Lee</surname><given-names>Kenton</given-names></name>
          <name><surname>Toutanova</surname><given-names>Kristina</given-names></name>
        </person-group>
        <article-title>BERT: Pre-training of deep bidirectional transformers for language understanding</article-title>
        <source>arXiv preprint arXiv:1810.04805</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-KobayashiU003A2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kobayashi</surname><given-names>Sosuke</given-names></name>
        </person-group>
        <article-title>Contextual augmentation: Data augmentation by words with paradigmatic relations</article-title>
        <source>arXiv preprint arXiv:1805.06201</source>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.18653/v1/n18-2072</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
