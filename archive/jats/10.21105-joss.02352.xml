<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2352</article-id>
<article-id pub-id-type="doi">10.21105/joss.02352</article-id>
<title-group>
<article-title>HPX - The C++ Standard Library for Parallelism and
Concurrency</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-8712-2806</contrib-id>
<string-name>Hartmut Kaiser</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-3922-8419</contrib-id>
<string-name>Patrick Diehl</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Adrian S. Lemoine</string-name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7995-5226</contrib-id>
<string-name>Bryce Adelstein Lelbach</string-name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6439-8404</contrib-id>
<string-name>Parsa Amini</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Agustín Berge</string-name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6552-2833</contrib-id>
<string-name>John Biddiscombe</string-name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7979-2906</contrib-id>
<string-name>Steven R. Brandt</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0525-3667</contrib-id>
<string-name>Nikunj Gupta</string-name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-2620-9438</contrib-id>
<string-name>Thomas Heller</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7064-8417</contrib-id>
<string-name>Kevin Huck</string-name>
<xref ref-type="aff" rid="aff-8"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6654-6856</contrib-id>
<string-name>Zahra Khatami</string-name>
<xref ref-type="aff" rid="aff-7"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-4624-4647</contrib-id>
<string-name>Alireza Kheirkhahan</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5531-0458</contrib-id>
<string-name>Auriane Reverdell</string-name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-9496-8044</contrib-id>
<string-name>Shahrzad Shirzad</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7238-8935</contrib-id>
<string-name>Mikael Simberg</string-name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6619-7115</contrib-id>
<string-name>Bibek Wagle</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-3065-4959</contrib-id>
<string-name>Weile Wei</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-1000-4887</contrib-id>
<string-name>Tianyi Zhang</string-name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Center for Computation &amp; Technology, Louisiana State
University, LA, Baton Rouge, United States of America</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Exasol, Erlangen, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Indian Institute of Technology, Roorkee,
India</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Swiss National Supercomputing Centre, Lugano,
Switzerland</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>NVIDIA, CA, Santa Clara, United States of
America</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>STE||AR Group</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>Oracle, CA, Redwood City, United States of
America</institution>
</institution-wrap>
</aff>
<aff id="aff-8">
<institution-wrap>
<institution>Oregon Advanced Computing Institute for Science and Society
(OACISS), University of Oregon, OR, Eugene, United States of
America</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>5</volume>
<issue>53</issue>
<fpage>2352</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>concurrency</kwd>
<kwd>task-based run time system</kwd>
<kwd>parallelism</kwd>
<kwd>distributed</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The new challenges presented by exascale system architectures have
  resulted in difficulty achieving the desired scalability using
  traditional distributed-memory runtimes. Asynchronous many-task
  systems (AMT) are based on a new paradigm showing promise in
  addressing these challenges, providing application developers with a
  productive and performant approach to programming on next generation
  systems.</p>
  <p>HPX is a C++ Library for concurrency and parallelism that is
  developed by The STE||AR Group, an international group of
  collaborators working in the field of distributed and parallel
  programming
  (<xref alt="Heller et al., 2017" rid="ref-heller2017hpx" ref-type="bibr">Heller
  et al., 2017</xref>;
  <xref alt="Kaiser et al., n.d." rid="ref-hpx_github" ref-type="bibr">Kaiser
  et al., n.d.</xref>;
  <xref alt="Tabbal et al., 2011" rid="ref-tabbal2011preliminary" ref-type="bibr">Tabbal
  et al., 2011</xref>). It is a runtime system written using modern C++
  techniques that are linked as part of an application. HPX exposes
  extended services and functionalities supporting the implementation of
  parallel, concurrent, and distributed capabilities for applications in
  any domain; it has been used in scientific computing, gaming,
  finances, data mining, and other fields.</p>
  <p>The HPX AMT runtime system attempts to solve some problems the
  community is facing when it comes to creating scalable parallel
  applications that expose excellent parallel efficiency and a high
  resource utilization. First, it exposes a C++ standards conforming API
  that unifies syntax and semantics for local and remote operations.
  This significantly simplifies writing codes that strive to effectively
  utilize different types of available parallelism in today’s machines
  in a coordinated way (i.e., on-node, off-node, and accelerator-based
  parallelism). Second, HPX implements an asynchronous C++ standard
  programming model that has the emergent property of semi-automatic
  parallelization of the user’s code. The provided API (especially when
  used in conjunction with the new C++20 <monospace>co_await</monospace>
  keyword
  (<xref alt="Standard ISO/IEC, 2020" rid="ref-standard2020programming" ref-type="bibr">Standard
  ISO/IEC, 2020</xref>)) enables intrinsic overlap of computation and
  communication, prefers moving work to data over moving data to work,
  and exposes minimal overheads from its lightweight threading
  subsystem, ensuring efficient fine-grained parallelization and
  minimal-overhead synchronization and context switching. This
  programming model natively ensures high-system utilization and perfect
  scalability.</p>
  <p>A detailed comparison of HPX with various other AMTs is given in
  (<xref alt="Thoman et al., 2018" rid="ref-thoman2018taxonomy" ref-type="bibr">Thoman
  et al., 2018</xref>). Some notable AMT solutions are: Uintah
  (<xref alt="Germain et al., 2000" rid="ref-germain2000uintah" ref-type="bibr">Germain
  et al., 2000</xref>), Chapel
  (<xref alt="Chamberlain et al., 2007" rid="ref-chamberlain2007parallel" ref-type="bibr">Chamberlain
  et al., 2007</xref>), Charm++
  (<xref alt="Kale &amp; Krishnan, 1993" rid="ref-kale1993charm" ref-type="bibr">Kale
  &amp; Krishnan, 1993</xref>), Kokkos
  (<xref alt="Edwards et al., 2014" rid="ref-edwards2014kokkos" ref-type="bibr">Edwards
  et al., 2014</xref>), Legion
  (<xref alt="Bauer et al., 2012" rid="ref-bauer2012legion" ref-type="bibr">Bauer
  et al., 2012</xref>), and PaRSEC
  (<xref alt="Bosilca et al., 2013" rid="ref-bosilca2013parsec" ref-type="bibr">Bosilca
  et al., 2013</xref>). Note that we only refer to distributed memory
  solutions, since this is an important feature for scientific
  applications to run large scale simulations. The major showpiece of
  HPX compared to the mentioned distributed AMTs is its future-proof C++
  standards conforming API and the exposed asynchronous programming
  model.</p>
  <p>HPX’s main goal is to improve efficiency and scalability of
  parallel applications by increasing resource utilization and reducing
  synchronization overheads through providing an asynchronous API and
  employing adaptive scheduling. The consequent use of
  <italic>Futures</italic> intrinsically enables overlap of computation
  and communication and constraint-based synchronization. HPX is able to
  maintain a balanced load among all the available resources resulting
  in significantly reducing processor starvation and effective latencies
  while controlling overheads. HPX fully conforms to the C++ ISO
  standards and implements the standardized concurrency mechanisms and
  parallelism facilities. Further, HPX extends those facilities to
  distributed use cases, thus enabling syntactic and semantic
  equivalence of local and remote operations on the API level. HPX uses
  the concept of C++ <italic>Futures</italic> to transform sequential
  algorithms into wait-free asynchronous executions. The use of
  <italic>Futurization</italic> enables the automatic creation of
  dynamic data flow execution trees of potentially millions of
  lightweight HPX tasks executed in the proper order. HPX also provides
  a work-stealing task scheduler that takes care of fine-grained
  parallelizations and automatic load balancing. Furthermore, HPX
  implements functionalities proposed as part of the ongoing C++
  standardization process.</p>
  <fig>
    <caption><p>Sketch of HPX’s architecture with all the components and
    their
    interactions.<styled-content id="figU003Aarchitecture"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="hpx_architecture.pdf" xlink:title="" />
  </fig>
  <p><xref alt="Figure 1" rid="figU003Aarchitecture">Figure 1</xref>
  sketches HPX’s architecture. The components of HPX and their
  references are listed below:</p>
  <p><bold>Threading Subsystem</bold>
  (<xref alt="Kaiser et al., 2009" rid="ref-kaiser2009parallex" ref-type="bibr">Kaiser
  et al., 2009</xref>) The thread manager manages the light-weight user
  level threads created by HPX. These light-weight threads have
  extremely short context switching times, resulting in reduced
  latencies even for very short operations. This also ensures reduced
  synchronization overheads for coordinating execution between different
  threads. HPX provides a set of scheduling policies that enable the
  user to flexibly customize the execution of HPX threads. Work-stealing
  and work-sharing policies ensure automatic local load balancing of
  tasks, which is important for achieving high system utilization and
  good scalability of the user’s code.</p>
  <p><bold>Active Global Address Space (AGAS)</bold>
  (<xref alt="Amini &amp; Kaiser, 2019" rid="ref-amini2019agas" ref-type="bibr">Amini
  &amp; Kaiser, 2019</xref>;
  <xref alt="Kaiser et al., 2014" rid="ref-kaiser2014hpx" ref-type="bibr">Kaiser
  et al., 2014</xref>) To support distributed objects, HPX supports a
  component for resolving global addresses that extends the Partitioned
  Global Address Space (PGAS) model, enabling dynamic runtime-based
  resource allocation and data placement. This layer enables HPX to
  expose a uniform API for local and remote execution. Unlike PGAS, AGAS
  provides the user with the ability to transparently move global
  objects in between nodes of a distributed computer system without
  changing the object’s global address. This capability is fundamental
  for supporting load balancing via object migration.</p>
  <p><bold>Parcel Transport Layer</bold>
  (<xref alt="Biddiscombe et al., 2017" rid="ref-biddiscombe2017zero" ref-type="bibr">Biddiscombe
  et al., 2017</xref>;
  <xref alt="Kaiser et al., 2009" rid="ref-kaiser2009parallex" ref-type="bibr">Kaiser
  et al., 2009</xref>) This component is an active-message networking
  layer. The parcelport leverages AGAS in order to deliver messages to
  and to launch functions on global objects regardless of their current
  placement in a distributed system. Additionally, its asynchronous
  protocol enables the parcelport to implicitly overlap communication
  and computation. The parcelport is modular to support multiple
  communication library backends. By default, HPX supports TCP/IP,
  Message Passing Interface (MPI), and libfabric
  (<xref alt="Daiß et al., 2019" rid="ref-daiss2019piz" ref-type="bibr">Daiß
  et al., 2019</xref>).</p>
  <p><bold>Performance counters</bold>
  (<xref alt="Grubel, 2016" rid="ref-grubel2016dynamic" ref-type="bibr">Grubel,
  2016</xref>) HPX provides its users with a uniform suite of globally
  accessible performance counters to monitor system metrics
  <italic>in-situ</italic>. These counters have their names registered
  with AGAS, which enables the users to easily query for different
  metrics at runtime. Additionally, HPX provides an API for users to
  create their own application-specific counters to gather information
  customized to their own application. These user-defined counters are
  exposed through the same interface as their predefined counterparts.
  By default, HPX provides performance counters for its own components,
  such as networking, AGAS operations, thread scheduling, and various
  statistics.</p>
  <p><bold>Policy Engine/Policies</bold>
  (<xref alt="Huck et al., 2015" rid="ref-huck2015autonomic" ref-type="bibr">Huck
  et al., 2015</xref>;
  <xref alt="Khatami et al., 2017" rid="ref-khatami2017hpx" ref-type="bibr">Khatami
  et al., 2017</xref>;
  <xref alt="Laberge et al., 2019" rid="ref-laberge2019scheduling" ref-type="bibr">Laberge
  et al., 2019</xref>) Often, modern applications must adapt to runtime
  environments to ensure acceptable performance. Autonomic Performance
  Environment for Exascale (APEX) enables this flexibility by measuring
  HPX tasks, monitoring system utilization, and accepting user provided
  policies that are triggered by defined events. In this way, features
  such as parcel coalescing
  (<xref alt="B. Wagle et al., 2018" rid="ref-wagle2018methodology" ref-type="bibr">B.
  Wagle et al., 2018</xref>) can adapt to the current phase of an
  application or even state of a system.</p>
  <p><bold>Accelerator Support</bold> HPX has support for several
  methods of integration with GPUs: HPXCL
  (<xref alt="Diehl, Seshadri, et al., 2018" rid="ref-diehl2018integration" ref-type="bibr">Diehl,
  Seshadri, et al., 2018</xref>;
  <xref alt="Stumpf et al., 2018" rid="ref-martin_stumpf_2018_1409043" ref-type="bibr">Stumpf
  et al., 2018</xref>) and HPX.Compute
  (<xref alt="Copik &amp; Kaiser, 2017" rid="ref-copik2017using" ref-type="bibr">Copik
  &amp; Kaiser, 2017</xref>). HPXCL provides users the ability to manage
  GPU kernels through a global object. This enables HPX to coordinate
  the launching and synchronization of CPU and GPU code. HPX.Compute
  (<xref alt="Copik &amp; Kaiser, 2017" rid="ref-copik2017using" ref-type="bibr">Copik
  &amp; Kaiser, 2017</xref>) aims to provide a single-source solution to
  heterogeneity by automatically generating GPU kernels from C++ code.
  This enables HPX to launch both CPU and GPU kernels as dictated by the
  current state of the system. Support for integrating HPX with Kokkos
  (<xref alt="Edwards et al., 2014" rid="ref-edwards2014kokkos" ref-type="bibr">Edwards
  et al., 2014</xref>) is currently being developed. This integration
  already has added HPX as an asynchronous backend to Kokkos and will
  expose Kokkos’ accelerator functionalities through HPX’s asynchronous
  APIs in a C++ standards-conforming way.</p>
  <p><bold>Local Control Objects (synchronization support
  facilities)</bold> HPX has support for many of the C++20 primitives,
  such as <monospace>hpx::latch</monospace>,
  <monospace>hpx::barrier</monospace>, and
  <monospace>hpx::counting_semaphore</monospace> to synchronize the
  execution of different threads allowing overlapping computation and
  communication. These facilities fully conform to the C++20 standard
  (<xref alt="Standard ISO/IEC, 2020" rid="ref-standard2020programming" ref-type="bibr">Standard
  ISO/IEC, 2020</xref>). For asynchronous computing HPX provides
  <monospace>hpx::async</monospace> and
  <monospace>hpx::future</monospace>; see the second example in the next
  section.</p>
  <p><bold>Software Resilience</bold> HPX supports software-level
  resilience
  (<xref alt="Gupta et al., 2020" rid="ref-gupta2020implementing" ref-type="bibr">Gupta
  et al., 2020</xref>) through its resiliency API, such as
  <monospace>hpx::async_replay</monospace> and
  <monospace>hpx::async_replicate</monospace> and its dataflow
  counterparts <monospace>hpx::dataflow_replay</monospace> and
  <monospace>hpx::dataflow_replicate</monospace>. These APIs are
  resilient against memory bit flips and other hardware errors. HPX
  provides an easy method to port codes to the resilient API by
  replacing <monospace>hpx::async</monospace> or
  <monospace>hpx::dataflow</monospace> with its resilient API
  counterparts everywhere in the code without making any other
  changes.</p>
  <p><bold>C++ Standards conforming API</bold> HPX implements all the
  C++17 parallel algorithms
  (<xref alt="Standard ISO/IEC, 2017" rid="ref-standard2017programming" ref-type="bibr">Standard
  ISO/IEC, 2017</xref>) and extends those with asynchronous versions.
  Here, HPX provides the <monospace>hpx::execution::seq</monospace> and
  <monospace>hpx::execution::par</monospace> execution policies, and (as
  an extension) their asynchronous equivalents
  <monospace>hpx::execution::seq(hpx::execution::task)</monospace> and
  <monospace>hpx::execution::par(hpx::execution::task)</monospace> (see
  the first code example below). HPX also implements the C++20
  concurrency facilities and APIs
  (<xref alt="Standard ISO/IEC, 2020" rid="ref-standard2020programming" ref-type="bibr">Standard
  ISO/IEC, 2020</xref>), such as <monospace>hpx::jthread</monospace>,
  <monospace>hpx::latch</monospace>,
  <monospace>hpx::barrier</monospace>, etc.</p>
</sec>
<sec id="applications">
  <title>Applications</title>
  <p>HPX is utilized in a diverse set of applications:</p>
  <list list-type="bullet">
    <list-item>
      <p>Scientific computing</p>
      <list list-type="bullet">
        <list-item>
          <p><ext-link ext-link-type="uri" xlink:href="https://github.com/STEllAR-GROUP/octotiger">Octo-Tiger</ext-link>
          (<xref alt="Daiß et al., 2019" rid="ref-daiss2019piz" ref-type="bibr">Daiß
          et al., 2019</xref>;
          <xref alt="Heller et al., 2019" rid="ref-heller2019harnessing" ref-type="bibr">Heller
          et al., 2019</xref>;
          <xref alt="Pfander et al., 2018" rid="ref-pfander2018accelerating" ref-type="bibr">Pfander
          et al., 2018</xref>), an astrophysics code for stellar
          mergers.</p>
        </list-item>
        <list-item>
          <p><ext-link ext-link-type="uri" xlink:href="https://github.com/gentryx/libgeodecomp">libGeoDecomp</ext-link>
          (<xref alt="Schäfer &amp; Fey, 2008" rid="ref-SchaferU003A2008U003ALGLU003A1431669.1431721" ref-type="bibr">Schäfer
          &amp; Fey, 2008</xref>), an auto-parallelizing library to
          speed up stencil-code-based computer simulations.</p>
        </list-item>
        <list-item>
          <p><ext-link ext-link-type="uri" xlink:href="https://github.com/nonlocalmodels">NLMech</ext-link>
          (<xref alt="Diehl, Jha, et al., 2018" rid="ref-diehl2018implementation" ref-type="bibr">Diehl,
          Jha, et al., 2018</xref>), a simulation tool for non-local
          models, e.g. Peridynamics.</p>
        </list-item>
        <list-item>
          <p><ext-link ext-link-type="uri" xlink:href="https://github.com/CompFUSE/DCA">Dynamical
          Cluster Approximation</ext-link> (DCA++)
          (<xref alt="Hähner et al., 2020" rid="ref-hahner2020dca" ref-type="bibr">Hähner
          et al., 2020</xref>), a high-performance research software
          framework to solve quantum many-body problems with cutting
          edge quantum cluster algorithms.</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p>Libraries</p>
      <list list-type="bullet">
        <list-item>
          <p><ext-link ext-link-type="uri" xlink:href="https://github.com/STEllAR-GROUP/hpxMP">hpxMP</ext-link>
          (<xref alt="Zhang et al., 2019" rid="ref-zhang2019introduction" ref-type="bibr">Zhang
          et al., 2019</xref>,
          <xref alt="2020" rid="ref-zhang2020supporting" ref-type="bibr">2020</xref>)
          a modern OpenMP implementation leveraging HPX that supports
          shared memory multithread programming.</p>
        </list-item>
        <list-item>
          <p><ext-link ext-link-type="uri" xlink:href="https://github.com/kokkos/kokkos">Kokkos</ext-link>
          (<xref alt="Carter Edwards et al., 2014" rid="ref-10.1016U002Fj.jpdc.2014.07.003" ref-type="bibr">Carter
          Edwards et al., 2014</xref>), the C++ Performance Portability
          Programming EcoSystem.</p>
        </list-item>
        <list-item>
          <p><ext-link ext-link-type="uri" xlink:href="https://github.com/STEllAR-GROUP/phylanx">Phylanx</ext-link>
          (<xref alt="Tohid et al., 2018" rid="ref-tohid2018asynchronous" ref-type="bibr">Tohid
          et al., 2018</xref>;
          <xref alt="Bibek Wagle et al., 2019" rid="ref-wagle2019runtime" ref-type="bibr">Bibek
          Wagle et al., 2019</xref>) An Asynchronous Distributed C++
          Array Processing Toolkit.</p>
        </list-item>
      </list>
    </list-item>
  </list>
  <p>For a updated list of applications, we refer to the corresponding
  <ext-link ext-link-type="uri" xlink:href="https://hpx.stellar-group.org/hpx-users/">HPX
  website</ext-link>.</p>
</sec>
<sec id="example-code">
  <title>Example code</title>
  <p>The following is an example of HPX’s parallel algorithms API using
  execution policies as defined in the C++17 standard
  (<xref alt="Standard ISO/IEC, 2017" rid="ref-standard2017programming" ref-type="bibr">Standard
  ISO/IEC, 2017</xref>). HPX implements all the parallel algorithms
  defined therein. The parallel algorithms extend the classic STL
  algorithms by adding a first argument (called execution policy). The
  <monospace>hpx::execution::seq</monospace> implies sequential
  execution while <monospace>hpx::execution::par</monospace> will
  execute the algorithm in parallel. HPX’s parallel algorithm library
  API is completely standards conforming.</p>
  <code language="c++">#include &lt;hpx/hpx.hpp&gt;
#include &lt;iostream&gt;
#include &lt;vector&gt;

int main()
{
 std::vector&lt;int&gt; values = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};

 // Compute the sum in a sequential fashion
 int sum1 = hpx::reduce(
 hpx::execution::seq, values.begin(), values.end(), 0);
 std::cout &lt;&lt; sum1 &lt;&lt; '\n'; // will print 55

 // Compute the sum in a parallel fashion based on a range of values
 int sum2 = hpx::ranges::reduce(hpx::execution::par, values, 0);
 std::cout &lt;&lt; sum2 &lt;&lt; '\n'; // will print 55 as well

 return 0;
}</code>
  <p>Example for the HPX’s concurrency API where the Taylor series for
  the <inline-formula><alternatives>
  <tex-math><![CDATA[\sin(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>sin</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  function is computed. The Taylor series is given by,</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[ \sin(x) \approx = \sum\limits_{n=0}^N (-1)^{n-1} \frac{x^{2n}}{(2n)!}.]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>sin</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>!</mml:mi></mml:mrow></mml:mfrac><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>For the concurrent computation, the interval
  <inline-formula><alternatives>
  <tex-math><![CDATA[[0, N]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  is split in two partitions from <inline-formula><alternatives>
  <tex-math><![CDATA[[0, N/2]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mi>/</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[[(N/2)+1, N]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>N</mml:mi><mml:mi>/</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
  and these are computed asynchronously using
  <monospace>hpx::async</monospace>. Note that each asynchronous
  function call returns an <monospace>hpx::future</monospace> which is
  needed to synchronize the collection of the partial results. The
  future has a <monospace>get()</monospace> method that returns the
  result once the computation of the Taylor function finished. If the
  result is not ready yet, the current thread is suspended until the
  result is ready. Only if <monospace>f1</monospace> and
  <monospace>f2</monospace> are ready, the overall result will be
  printed to the standard output stream.</p>
  <code language="c++">#include &lt;hpx/hpx.hpp&gt;
#include &lt;cmath&gt;
#include &lt;iostream&gt;

// Define the partial taylor function
double taylor(size_t begin, size_t end, size_t n, double x)
{
 double denom = factorial(2 * n);
 double res = 0;
 for (size_t i = begin; i != end; ++i)
 {
 res += std::pow(-1, i - 1) * std::pow(x, 2 * n) / denom;
 }
 return res;
}

int main()
{
 // Compute the Talor series sin(2.0) for 100 iterations
 size_t n = 100;

 // Launch two concurrent computations of each partial result
 hpx::future&lt;double&gt; f1 = hpx::async(taylor, 0, n / 2, n, 2.);
 hpx::future&lt;double&gt; f2 = hpx::async(taylor, (n / 2) + 1, n, n, 2.);

 // Introduce a barrier to gather the results
 double res = f1.get() + f2.get();

 // Print the result
 std::cout &lt;&lt; &quot;Sin(2.) = &quot; &lt;&lt; res &lt;&lt; std::endl;
}</code>
  <p>Please report any bugs or feature requests on the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/STEllAR-GROUP/hpx">HPX
  GitHub page</ext-link>.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We would like to acknowledge the National Science Foundation (NSF),
  the U.S. Department of Energy (DoE), the Defense Technical Information
  Center (DTIC), the Defense Advanced Research Projects Agency (DARPA),
  the Center for Computation and Technology (CCT) at Louisiana State
  University (LSU), the Swiss National Supercomputing Centre (CSCS), the
  Department of Computer Science 3 - Computer Architecture at the
  University of Erlangen Nuremberg who fund and support our work, and
  the Heterogeneous System Architecture (HSA) Foundation.</p>
  <p>We would also like to thank the following organizations for
  granting us allocations of their compute resources: LSU HPC, Louisiana
  Optical Network Iniative (LONI), the Extreme Science and Engineering
  Discovery Environment (XSEDE), the National Energy Research Scientific
  Computing Center (NERSC), the Oak Ridge Leadership Computing Facility
  (OLCF), Swiss National Supercomputing Centre (CSCS/ETHZ), the Juelich
  Supercomputing Centre (JSC), and the Gauss Center for
  Supercomputing.</p>
  <p>At the time the paper was written, HPX was directly funded by the
  following grants:</p>
  <list list-type="bullet">
    <list-item>
      <p>The National Science Foundation through awards 1339782 (STORM)
      and 1737785 (Phylanx).</p>
    </list-item>
    <list-item>
      <p>The Department of Energy (DoE) through the awards
      DE-AC52-06NA25396 (FLeCSI) DE-NA0003525 (Resilience), and
      DE-AC05-00OR22725 (DCA++).</p>
    </list-item>
    <list-item>
      <p>The Defense Technical Information Center (DTIC) under contract
      FA8075-14-D-0002/0007.</p>
    </list-item>
    <list-item>
      <p>The Bavarian Research Foundation (Bayerische
      Forschungsstiftung) through the grant AZ-987-11.</p>
    </list-item>
    <list-item>
      <p>The European Commission’s Horizon 2020 programme through the
      grant H2020-EU.1.2.2. 671603 (AllScale).</p>
    </list-item>
  </list>
  <p>For a updated list of previous and current funding, we refer to the
  corresponding
  <ext-link ext-link-type="uri" xlink:href="http://hpx.stellar-group.org/funding-acknowledgements/">HPX
  website</ext-link>.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-wagle2019runtime">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Wagle</surname><given-names>Bibek</given-names></name>
          <name><surname>Monil</surname><given-names>Mohammad Alaul Haque</given-names></name>
          <name><surname>Huck</surname><given-names>Kevin</given-names></name>
          <name><surname>Malony</surname><given-names>Allen D</given-names></name>
          <name><surname>Serio</surname><given-names>Adrian</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
        </person-group>
        <article-title>Runtime Adaptive Task Inlining on Asynchronous Multitasking Runtime Systems</article-title>
        <source>Proceedings of the 48th International Conference on Parallel Processing</source>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.1145/3337821.3337915</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-tohid2018asynchronous">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Tohid</surname><given-names>R</given-names></name>
          <name><surname>Wagle</surname><given-names>Bibek</given-names></name>
          <name><surname>Shirzad</surname><given-names>Shahrzad</given-names></name>
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Serio</surname><given-names>Adrian</given-names></name>
          <name><surname>Kheirkhahan</surname><given-names>Alireza</given-names></name>
          <name><surname>Amini</surname><given-names>Parsa</given-names></name>
          <name><surname>Williams</surname><given-names>Katy</given-names></name>
          <name><surname>Isaacs</surname><given-names>Kate</given-names></name>
          <name><surname>Huck</surname><given-names>Kevin</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Asynchronous Execution of Python Code on Task-Based Runtime Systems</article-title>
        <source>2018 IEEE/ACM 4th International Workshop on Extreme Scale Programming Models and Middleware (ESPM2)</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.1109/espm2.2018.00009</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-standard2017programming">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <string-name>Standard ISO/IEC</string-name>
        </person-group>
        <article-title>ISO International Standard ISO/IEC 14882:2017(E) - Programming Language C++</article-title>
        <source>Geneva, Switzerland: International Organization for Standardization (ISO)</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-standard2020programming">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <string-name>Standard ISO/IEC</string-name>
        </person-group>
        <article-title>ISO International Standard ISO/IEC 14882:2020(E) - Programming Language C++. [Working draft]</article-title>
        <source>Geneva, Switzerland: International Organization for Standardization (ISO)</source>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-thoman2018taxonomy">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Thoman</surname><given-names>Peter</given-names></name>
          <name><surname>Dichev</surname><given-names>Kiril</given-names></name>
          <name><surname>Heller</surname><given-names>Thomas</given-names></name>
          <name><surname>Iakymchuk</surname><given-names>Roman</given-names></name>
          <name><surname>Aguilar</surname><given-names>Xavier</given-names></name>
          <name><surname>Hasanov</surname><given-names>Khalid</given-names></name>
          <name><surname>Gschwandtner</surname><given-names>Philipp</given-names></name>
          <name><surname>Lemarinier</surname><given-names>Pierre</given-names></name>
          <name><surname>Markidis</surname><given-names>Stefano</given-names></name>
          <name><surname>Jordan</surname><given-names>Herbert</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>A Taxonomy of Task-based Parallel Programming Technologies for High-performance Computing</article-title>
        <source>The Journal of Supercomputing</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>74</volume>
        <issue>4</issue>
      </element-citation>
    </ref>
    <ref id="ref-heller2017hpx">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Heller</surname><given-names>Thomas</given-names></name>
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Byerly</surname><given-names>Zachary</given-names></name>
          <name><surname>Biddiscombe</surname><given-names>John</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
        </person-group>
        <article-title>HPX–An Open Source C++ Standard Library for Parallelism and Concurrency</article-title>
        <source>Proceedings of OpenSuCo</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-laberge2019scheduling">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Laberge</surname><given-names>Gabriel</given-names></name>
          <name><surname>Shirzad</surname><given-names>Shahrzad</given-names></name>
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Prudhomme</surname><given-names>Serge</given-names></name>
          <name><surname>Lemoine</surname><given-names>Adrian S</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Scheduling Optimization of Parallel Linear Algebra Algorithms Using Supervised Learning</article-title>
        <source>2019 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC)</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.1109/mlhpc49564.2019.00009</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hpx_github">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Heller</surname><given-names>Thomas</given-names></name>
          <name><surname>Simberg</surname><given-names>Mikael</given-names></name>
          <name><surname>Berge</surname><given-names>Agustin</given-names></name>
          <name><surname>Biddiscombe</surname><given-names>John</given-names></name>
          <name><surname>Reverdell</surname><given-names>Auriane</given-names></name>
          <name><surname>Huck</surname><given-names>Kevin</given-names></name>
          <name><surname>Lemoine</surname><given-names>Adrian S.</given-names></name>
          <name><surname>Stobaugh</surname><given-names>Rebecca</given-names></name>
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Lelbach</surname><given-names>Bryce Adelstein</given-names></name>
          <name><surname>Wei</surname><given-names>Weile</given-names></name>
        </person-group>
        <source>HPX: The C++ Standards Library for Parallelism and Concurrency</source>
        <publisher-name>Zenodo</publisher-name>
        <uri>https://github.com/STEllAR-GROUP/hpx</uri>
        <pub-id pub-id-type="doi">10.5281/zenodo.598202</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-copik2017using">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Copik</surname><given-names>Marcin</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
        </person-group>
        <article-title>Using SYCL as an implementation framework for hpx. compute</article-title>
        <source>Proceedings of the 5th International Workshop on OpenCL</source>
        <year iso-8601-date="2017">2017</year>
        <pub-id pub-id-type="doi">10.1145/3078155.3078187</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-diehl2018integration">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Seshadri</surname><given-names>Madhavan</given-names></name>
          <name><surname>Heller</surname><given-names>Thomas</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
        </person-group>
        <article-title>Integration of CUDA Processing within the C++ Library for Parallelism and Concurrency (HPX)</article-title>
        <source>2018 IEEE/ACM 4th International Workshop on Extreme Scale Programming Models and Middleware (ESPM2), pages=19–28</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.1109/espm2.2018.00006</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-huck2015autonomic">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Huck</surname><given-names>Kevin A</given-names></name>
          <name><surname>Porterfield</surname><given-names>Allan</given-names></name>
          <name><surname>Chaimov</surname><given-names>Nick</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Malony</surname><given-names>Allen D</given-names></name>
          <name><surname>Sterling</surname><given-names>Thomas</given-names></name>
          <name><surname>Fowler</surname><given-names>Rob</given-names></name>
        </person-group>
        <article-title>An autonomic performance environment for exascale</article-title>
        <source>Supercomputing Frontiers and Innovations</source>
        <year iso-8601-date="2015">2015</year>
        <volume>2</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.14529/jsfi150305</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kaiser2014hpx">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Heller</surname><given-names>Thomas</given-names></name>
          <name><surname>Adelstein-Lelbach</surname><given-names>Bryce</given-names></name>
          <name><surname>Serio</surname><given-names>Adrian</given-names></name>
          <name><surname>Fey</surname><given-names>Dietmar</given-names></name>
        </person-group>
        <article-title>HPX: A Task based Programming Model in a Global Address Space</article-title>
        <source>Proceedings of the 8th International Conference on Partitioned Global Address Space Programming Models</source>
        <year iso-8601-date="2014">2014</year>
      </element-citation>
    </ref>
    <ref id="ref-daiss2019piz">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Daiß</surname><given-names>Gregor</given-names></name>
          <name><surname>Amini</surname><given-names>Parsa</given-names></name>
          <name><surname>Biddiscombe</surname><given-names>John</given-names></name>
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Frank</surname><given-names>Juhan</given-names></name>
          <name><surname>Huck</surname><given-names>Kevin</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Marcello</surname><given-names>Dominic</given-names></name>
          <name><surname>Pfander</surname><given-names>David</given-names></name>
          <name><surname>Pfüger</surname><given-names>Dirk</given-names></name>
        </person-group>
        <article-title>From Piz-Daint to the Stars: Simulation of Stellar Mergers using High-level abstractions</article-title>
        <source>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</source>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.1177/1094342018819744</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-amini2019agas">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Amini</surname><given-names>P.</given-names></name>
          <name><surname>Kaiser</surname><given-names>H.</given-names></name>
        </person-group>
        <article-title>Assessing the Performance Impact of using an Active Global Address Space in HPX: A Case for AGAS</article-title>
        <source>2019 IEEE/ACM Third Annual Workshop on Emerging Parallel and Distributed Runtime Systems and Middleware (IPDRM)</source>
        <year iso-8601-date="2019">2019</year>
        <volume></volume>
        <pub-id pub-id-type="doi">10.1109/ipdrm49579.2019.00008</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kaiser2009parallex">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Brodowicz</surname><given-names>Maciek</given-names></name>
          <name><surname>Sterling</surname><given-names>Thomas</given-names></name>
        </person-group>
        <article-title>ParalleX: An Advanced Parallel Execution Model for Scaling-impaired Applications</article-title>
        <source>2009 International Conference on Parallel Processing Workshops</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2009">2009</year>
        <pub-id pub-id-type="doi">10.1109/icppw.2009.14</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-biddiscombe2017zero">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Biddiscombe</surname><given-names>John</given-names></name>
          <name><surname>Heller</surname><given-names>Thomas</given-names></name>
          <name><surname>Bikineev</surname><given-names>Anton</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
        </person-group>
        <article-title>Zero Copy Serialization using RMA in the Distributed Task-Based HPX Runtime</article-title>
        <source>14th International Conference on Applied Computing</source>
        <publisher-name>IADIS, International Association for Development of the Information Society</publisher-name>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-grubel2016dynamic">
      <element-citation publication-type="thesis">
        <person-group person-group-type="author">
          <name><surname>Grubel</surname><given-names>Patricia A</given-names></name>
        </person-group>
        <article-title>Dynamic Adaptation in HPX: A Task-based Parallel Runtime System</article-title>
        <publisher-name>New Mexico State University</publisher-name>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-martin_stumpf_2018_1409043">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Stumpf</surname><given-names>Martin</given-names></name>
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Seshadri</surname><given-names>Madhavan</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Heller</surname><given-names>Thomas</given-names></name>
          <name><surname>Howard</surname><given-names>Damond</given-names></name>
          <name><surname>Biddiscombe</surname><given-names>John</given-names></name>
          <name><surname>Viklund</surname><given-names>Lars</given-names></name>
          <name><surname>Katz</surname><given-names>Omer</given-names></name>
        </person-group>
        <source>STEllAR-GROUP/hpxcl: Initial release</source>
        <publisher-name>Zenodo</publisher-name>
        <year iso-8601-date="2018-09">2018</year><month>09</month>
        <uri>https://doi.org/10.5281/zenodo.1409043</uri>
        <pub-id pub-id-type="doi">10.5281/zenodo.1409043</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-tabbal2011preliminary">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tabbal</surname><given-names>Alexandre</given-names></name>
          <name><surname>Anderson</surname><given-names>Matthew</given-names></name>
          <name><surname>Brodowicz</surname><given-names>Maciej</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Sterling</surname><given-names>Thomas</given-names></name>
        </person-group>
        <article-title>Preliminary design examination of the ParalleX system from a Software and Hardware Perspective</article-title>
        <source>ACM SIGMETRICS Performance Evaluation Review</source>
        <publisher-name>ACM New York, NY, USA</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <volume>38</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1145/1964218.1964232</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-SchaferU003A2008U003ALGLU003A1431669.1431721">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Schäfer</surname><given-names>Andreas</given-names></name>
          <name><surname>Fey</surname><given-names>Dietmar</given-names></name>
        </person-group>
        <article-title>LibGeoDecomp: A Grid-Enabled Library for Geometric Decomposition Codes</article-title>
        <source>Proceedings of the 15th European PVM/MPI Users’ Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface</source>
        <publisher-name>Springer-Verlag</publisher-name>
        <publisher-loc>Berlin, Heidelberg</publisher-loc>
        <year iso-8601-date="2008">2008</year>
        <isbn>978-3-540-87474-4</isbn>
        <uri>http://dx.doi.org/10.1007/978-3-540-87475-1_39</uri>
        <pub-id pub-id-type="doi">10.1007/978-3-540-87475-1_39</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-heller2019harnessing">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Heller</surname><given-names>Thomas</given-names></name>
          <name><surname>Lelbach</surname><given-names>Bryce Adelstein</given-names></name>
          <name><surname>Huck</surname><given-names>Kevin A</given-names></name>
          <name><surname>Biddiscombe</surname><given-names>John</given-names></name>
          <name><surname>Grubel</surname><given-names>Patricia</given-names></name>
          <name><surname>Koniges</surname><given-names>Alice E</given-names></name>
          <name><surname>Kretz</surname><given-names>Matthias</given-names></name>
          <name><surname>Marcello</surname><given-names>Dominic</given-names></name>
          <name><surname>Pfander</surname><given-names>David</given-names></name>
          <name><surname>Serio</surname><given-names>Adrian</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Harnessing Billions of Tasks for a Scalable Portable Hydrodynamic Simulation of the Merger of two Stars</article-title>
        <source>The International Journal of High Performance Computing Applications</source>
        <publisher-name>SAGE Publications Sage UK: London, England</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>33</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1177/1094342018819744</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-pfander2018accelerating">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Pfander</surname><given-names>David</given-names></name>
          <name><surname>Daiß</surname><given-names>Gregor</given-names></name>
          <name><surname>Marcello</surname><given-names>Dominic</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Pflüger</surname><given-names>Dirk</given-names></name>
        </person-group>
        <article-title>Accelerating Octo-Tiger: Stellar Mergers on Intel Knights Landing with HPX</article-title>
        <source>Proceedings of the International Workshop on OpenCL</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-diehl2018implementation">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Jha</surname><given-names>Prashant K</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Lipton</surname><given-names>Robert</given-names></name>
          <name><surname>Levesque</surname><given-names>Martin</given-names></name>
        </person-group>
        <article-title>Implementation of Peridynamics utilizing HPX–the C++ Standard Library for Parallelism and Concurrency</article-title>
        <source>arXiv preprint arXiv:1806.06917</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-wagle2018methodology">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Wagle</surname><given-names>B.</given-names></name>
          <name><surname>Kellar</surname><given-names>S.</given-names></name>
          <name><surname>Serio</surname><given-names>A.</given-names></name>
          <name><surname>Kaiser</surname><given-names>H.</given-names></name>
        </person-group>
        <article-title>Methodology for Adaptive Active Message Coalescing in Task Based Runtime Systems</article-title>
        <source> 2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)</source>
        <year iso-8601-date="2018">2018</year>
        <volume></volume>
        <pub-id pub-id-type="doi">10.1109/ipdpsw.2018.00173</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-khatami2017hpx">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Khatami</surname><given-names>Zahra</given-names></name>
          <name><surname>Troska</surname><given-names>Lukas</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
          <name><surname>Ramanujam</surname><given-names>J.</given-names></name>
          <name><surname>Serio</surname><given-names>Adrian</given-names></name>
        </person-group>
        <article-title>HPX Smart Executors</article-title>
        <source>Proceedings of the Third International Workshop on Extreme Scale Programming Models and Middleware</source>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year iso-8601-date="2017">2017</year>
        <isbn>9781450351331</isbn>
        <uri>https://doi.org/10.1145/3152041.3152084</uri>
        <pub-id pub-id-type="doi">10.1145/3152041.3152084</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-gupta2020implementing">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Gupta</surname><given-names>Nikunj</given-names></name>
          <name><surname>Mayo</surname><given-names>Jackson R.</given-names></name>
          <name><surname>Lemoine</surname><given-names>Adrian S.</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
        </person-group>
        <article-title>Implementing Software Resiliency in HPX for Extreme Scale Computing</article-title>
        <year iso-8601-date="2020">2020</year>
        <uri>https://arxiv.org/abs/2004.07203</uri>
        <pub-id pub-id-type="doi">10.2172/1614897</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-zhang2019introduction">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Zhang</surname><given-names>Tianyi</given-names></name>
          <name><surname>Shirzad</surname><given-names>Shahrzad</given-names></name>
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Tohid</surname><given-names>R</given-names></name>
          <name><surname>Wei</surname><given-names>Weile</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
        </person-group>
        <article-title>An Introduction to hpxMP: A Modern OpenMP Implementation Leveraging HPX, An Asynchronous Many-Task System</article-title>
        <source>Proceedings of the International Workshop on OpenCL</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-zhang2020supporting">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Zhang</surname><given-names>Tianyi</given-names></name>
          <name><surname>Shirzad</surname><given-names>Shahrzad</given-names></name>
          <name><surname>Wagle</surname><given-names>Bibek</given-names></name>
          <name><surname>Lemoine</surname><given-names>Adrian S</given-names></name>
          <name><surname>Diehl</surname><given-names>Patrick</given-names></name>
          <name><surname>Kaiser</surname><given-names>Hartmut</given-names></name>
        </person-group>
        <article-title>Supporting OpenMP 5.0 Tasks in hpxMP–A study of an OpenMP implementation within Task Based Runtime Systems</article-title>
        <source>arXiv preprint arXiv:2002.07970</source>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-germain2000uintah">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Germain</surname><given-names>J Davison de St</given-names></name>
          <name><surname>McCorquodale</surname><given-names>John</given-names></name>
          <name><surname>Parker</surname><given-names>Steven G</given-names></name>
          <name><surname>Johnson</surname><given-names>Christopher R</given-names></name>
        </person-group>
        <article-title>Uintah: A massively parallel problem solving environment</article-title>
        <source>Proceedings the Ninth International Symposium on High-Performance Distributed Computing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2000">2000</year>
        <pub-id pub-id-type="doi">10.1109/HPDC.2000.868632</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-chamberlain2007parallel">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Chamberlain</surname><given-names>Bradford L</given-names></name>
          <name><surname>Callahan</surname><given-names>David</given-names></name>
          <name><surname>Zima</surname><given-names>Hans P</given-names></name>
        </person-group>
        <article-title>Parallel programmability and the chapel language</article-title>
        <source>The International Journal of High Performance Computing Applications</source>
        <publisher-name>Sage Publications Sage UK: London, England</publisher-name>
        <year iso-8601-date="2007">2007</year>
        <volume>21</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1177/1094342007078442</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kale1993charm">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kale</surname><given-names>Laxmikant V</given-names></name>
          <name><surname>Krishnan</surname><given-names>Sanjeev</given-names></name>
        </person-group>
        <article-title>Charm++ A portable concurrent object oriented system based on C++</article-title>
        <source>Proceedings of the eighth annual conference on Object-oriented programming systems, languages, and applications</source>
        <year iso-8601-date="1993">1993</year>
      </element-citation>
    </ref>
    <ref id="ref-edwards2014kokkos">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Edwards</surname><given-names>H Carter</given-names></name>
          <name><surname>Trott</surname><given-names>Christian R</given-names></name>
          <name><surname>Sunderland</surname><given-names>Daniel</given-names></name>
        </person-group>
        <article-title>Kokkos: Enabling manycore performance portability through polymorphic memory access patterns</article-title>
        <source>Journal of Parallel and Distributed Computing</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <volume>74</volume>
        <issue>12</issue>
        <pub-id pub-id-type="doi">10.1016/j.jpdc.2014.07.003</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-bauer2012legion">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Bauer</surname><given-names>Michael</given-names></name>
          <name><surname>Treichler</surname><given-names>Sean</given-names></name>
          <name><surname>Slaughter</surname><given-names>Elliott</given-names></name>
          <name><surname>Aiken</surname><given-names>Alex</given-names></name>
        </person-group>
        <article-title>Legion: Expressing locality and independence with logical regions</article-title>
        <source>SC’12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <pub-id pub-id-type="doi">10.1109/SC.2012.71</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-bosilca2013parsec">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bosilca</surname><given-names>George</given-names></name>
          <name><surname>Bouteiller</surname><given-names>Aurelien</given-names></name>
          <name><surname>Danalis</surname><given-names>Anthony</given-names></name>
          <name><surname>Faverge</surname><given-names>Mathieu</given-names></name>
          <name><surname>Hérault</surname><given-names>Thomas</given-names></name>
          <name><surname>Dongarra</surname><given-names>Jack J</given-names></name>
        </person-group>
        <article-title>Parsec: Exploiting heterogeneity to enhance scalability</article-title>
        <source>Computing in Science &amp; Engineering</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2013">2013</year>
        <volume>15</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.1109/MCSE.2013.98</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-10.1016U002Fj.jpdc.2014.07.003">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Carter Edwards</surname><given-names>H.</given-names></name>
          <name><surname>Trott</surname><given-names>Christian R.</given-names></name>
          <name><surname>Sunderland</surname><given-names>Daniel</given-names></name>
        </person-group>
        <article-title>Kokkos</article-title>
        <source>J. Parallel Distrib. Comput.</source>
        <publisher-name>Academic Press, Inc.</publisher-name>
        <publisher-loc>USA</publisher-loc>
        <year iso-8601-date="2014-12">2014</year><month>12</month>
        <volume>74</volume>
        <issue>12</issue>
        <issn>0743-7315</issn>
        <uri>https://doi.org/10.1016/j.jpdc.2014.07.003</uri>
        <pub-id pub-id-type="doi">10.1016/j.jpdc.2014.07.003</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hahner2020dca">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hähner</surname><given-names>Urs R</given-names></name>
          <name><surname>Alvarez</surname><given-names>Gonzalo</given-names></name>
          <name><surname>Maier</surname><given-names>Thomas A</given-names></name>
          <name><surname>Solcà</surname><given-names>Raffaele</given-names></name>
          <name><surname>Staar</surname><given-names>Peter</given-names></name>
          <name><surname>Summers</surname><given-names>Michael S</given-names></name>
          <name><surname>Schulthess</surname><given-names>Thomas C</given-names></name>
        </person-group>
        <article-title>DCA++: A software framework to solve correlated electron problems with modern quantum cluster methods</article-title>
        <source>Computer Physics Communications</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>246</volume>
        <pub-id pub-id-type="doi">10.1016/j.cpc.2019.01.006</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
