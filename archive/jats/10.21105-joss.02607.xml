<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2607</article-id>
<article-id pub-id-type="doi">10.21105/joss.02607</article-id>
<title-group>
<article-title>Foolbox Native: Fast adversarial attacks to benchmark the
robustness of machine learning models in PyTorch, TensorFlow, and
JAX</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6795-9441</contrib-id>
<string-name>Jonas Rauber</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Roland Zimmermann</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Matthias Bethge</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<string-name>Wieland Brendel</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Tübingen AI Center, University of Tübingen,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>International Max Planck Research School for Intelligent
Systems, Tübingen, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Bernstein Center for Computational Neuroscience Tübingen,
Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-08-10">
<day>10</day>
<month>8</month>
<year>2020</year>
</pub-date>
<volume>5</volume>
<issue>53</issue>
<fpage>2607</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>machine learning</kwd>
<kwd>adversarial attacks</kwd>
<kwd>neural networks</kwd>
<kwd>pytorch</kwd>
<kwd>tensorflow</kwd>
<kwd>jax</kwd>
<kwd>keras</kwd>
<kwd>eagerpy</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Machine learning has made enormous progress in recent years and is
  now being used in many real-world applications. Nevertheless, even
  state-of-the-art machine learning models can be fooled by small,
  maliciously crafted perturbations of their input data. Foolbox is a
  popular Python library to benchmark the robustness of machine learning
  models against these adversarial perturbations. It comes with a huge
  collection of state-of-the-art adversarial attacks to find adversarial
  perturbations and thanks to its framework-agnostic design it is
  ideally suited for comparing the robustness of many different models
  implemented in different frameworks. Foolbox 3 aka Foolbox Native has
  been rewritten from scratch to achieve native performance on models
  developed in PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>), TensorFlow
  (<xref alt="Abadi et al., 2016" rid="ref-tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>), and JAX
  (<xref alt="Bradbury et al., 2018" rid="ref-jax" ref-type="bibr">Bradbury
  et al., 2018</xref>), all with one codebase without code
  duplication.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Evaluating the adversarial robustness of machine learning models is
  crucial to understanding their shortcomings and quantifying the
  implications on safety, security, and interpretability. Foolbox Native
  is the first adversarial robustness toolbox that is both fast and
  framework-agnostic. This is important because modern machine learning
  models such as deep neural networks are often computationally
  expensive and are implemented in different frameworks such as PyTorch
  and TensorFlow. Foolbox Native combines the framework-agnostic design
  of the original Foolbox
  (<xref alt="Rauber et al., 2017" rid="ref-rauber2017foolbox" ref-type="bibr">Rauber
  et al., 2017</xref>) with real batch support and native performance in
  PyTorch, TensorFlow, and JAX, all using a single codebase without code
  duplication. To achieve this, all adversarial attacks have been
  rewritten from scratch and now use EagerPy
  (<xref alt="Rauber et al., 2020" rid="ref-rauber2020eagerpy" ref-type="bibr">Rauber
  et al., 2020</xref>) instead of NumPy
  (<xref alt="Oliphant, 2006" rid="ref-numpy" ref-type="bibr">Oliphant,
  2006</xref>) to interface <italic>natively</italic> with the different
  frameworks.</p>
  <p>This is great for both users and developers of adversarial attacks.
  Users can efficiently evaluate the robustness of different models in
  different frameworks using the same set of state-of-the-art
  adversarial attacks, thus obtaining comparable results. Attack
  developers do not need to choose between supporting just one framework
  or reimplementing their new adversarial attack multiple times and
  dealing with code duplication. In addition, they both benefit from the
  comprehensive type annotations
  (<xref alt="Rossum et al., 2015" rid="ref-pep484" ref-type="bibr">Rossum
  et al., 2015</xref>) in Foolbox Native to catch bugs even before
  running their code.</p>
  <p>The combination of being framework-agnostic and simultaneously
  achieving native performance sets Foolbox Native apart from other
  adversarial attack libraries. The most popular alternative to Foolbox
  is CleverHans<xref ref-type="fn" rid="fn1">1</xref>. It was the first
  adversarial attack library and has traditionally focused solely on
  TensorFlow (plans to make it framework-agnostic <italic>in the
  future</italic> have been announced). The original Foolbox was the
  second adversarial attack library and the first one to be
  framework-agnostic. Back then, this was achieved at the expense of
  performance. The adversarial robustness toolbox
  ART<xref ref-type="fn" rid="fn2">2</xref> is another
  framework-agnostic adversarial attack library, but it is conceptually
  inspired by the original Foolbox and thus comes with the same
  performance trade-off.
  AdverTorch<xref ref-type="fn" rid="fn3">3</xref> is a popular
  adversarial attack library that was inspired by the original Foolbox
  but improved its performance by focusing soley on PyTorch. Foolbox
  Native is our attempt to improve the performance of Foolbox without
  sacrificing the framework-agnostic design that is crucial to
  consistently evaluate the robustness of different machine learning
  models that use different frameworks.</p>
</sec>
<sec id="use-cases">
  <title>Use Cases</title>
  <p>Foolbox was designed to make adversarial attacks easy to apply even
  without expert knowledge. It has been used in numerous scientific
  publications and has already been cited more than 220 times. On GitHub
  it has received contributions from several developers and has gathered
  more than 1.500 stars. It provides the reference implementations of
  various adversarial attacks, including the Boundary Attack
  (<xref alt="Brendel et al., 2018" rid="ref-brendel2018decisionbased" ref-type="bibr">Brendel
  et al., 2018</xref>), the Pointwise Attack
  (<xref alt="Schott et al., 2019" rid="ref-schott2018towards" ref-type="bibr">Schott
  et al., 2019</xref>), clipping-aware noise attacks
  (<xref alt="Rauber &amp; Bethge, 2020" rid="ref-rauber2020fast" ref-type="bibr">Rauber
  &amp; Bethge, 2020</xref>), the Brendel Bethge Attack
  (<xref alt="Brendel et al., 2019" rid="ref-brendel2019accurate" ref-type="bibr">Brendel
  et al., 2019</xref>), and the HopSkipJump Attack
  (<xref alt="Chen et al., 2020" rid="ref-chen2020hopskipjumpattack" ref-type="bibr">Chen
  et al., 2020</xref>), and is under active development since 2017.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>J.R. acknowledges support from the Bosch Research Foundation
  (Stifterverband, T113/30057/17) and the International Max Planck
  Research School for Intelligent Systems (IMPRS-IS). This work was
  supported by the German Federal Ministry of Education and Research
  (BMBF): Tübingen AI Center, FKZ: 01IS18039A, and by the Intelligence
  Advanced Research Projects Activity (IARPA) via Department of
  Interior/Interior Business Center (DoI/IBC) contract number
  D16PC00003. The U.S. Government is authorized to reproduce and
  distribute reprints for Governmental purposes notwithstanding any
  copyright annotation thereon. Disclaimer: The views and conclusions
  contained herein are those of the authors and should not be
  interpreted as necessarily representing the official policies or
  endorsements, either expressed or implied, of IARPA, DoI/IBC, or the
  U.S. Government.</p>
  <p>We thank all contributors to Foolbox, in particular Behar Veliqi,
  Evgenia Rusak, Jianbo Chen, Rene Bidart, Jerome Rony, Ben Feinstein,
  Eric R Meissner, Lars Holdijk, Lukas Schott, Carl-Johann
  Simon-Gabriel, Apostolos Modas, William Fleshman, Xuefei Ning,
  <ext-link ext-link-type="uri" xlink:href="https://github.com/bethgelab/foolbox/graphs/contributors">and
  many others</ext-link>.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-pytorch">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Paszke</surname><given-names>Adam</given-names></name>
          <name><surname>Gross</surname><given-names>Sam</given-names></name>
          <name><surname>Massa</surname><given-names>Francisco</given-names></name>
          <name><surname>Lerer</surname><given-names>Adam</given-names></name>
          <name><surname>Bradbury</surname><given-names>James</given-names></name>
          <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
          <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
          <name><surname>Lin</surname><given-names>Zeming</given-names></name>
          <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
          <name><surname>Antiga</surname><given-names>Luca</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
        <source>Advances in neural information processing systems</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-tensorflow">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
          <name><surname>Barham</surname><given-names>Paul</given-names></name>
          <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
          <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
          <name><surname>Davis</surname><given-names>Andy</given-names></name>
          <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
          <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
          <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
          <name><surname>Isard</surname><given-names>Michael</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>TensorFlow: A system for large-scale machine learning</article-title>
        <source>12th USENIX symposium on operating systems design and implementation (OSDI 16)</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-jax">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Bradbury</surname><given-names>James</given-names></name>
          <name><surname>Frostig</surname><given-names>Roy</given-names></name>
          <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
          <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
          <name><surname>Leary</surname><given-names>Chris</given-names></name>
          <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
          <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        </person-group>
        <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
        <year iso-8601-date="2018">2018</year>
        <uri>http://github.com/google/jax</uri>
      </element-citation>
    </ref>
    <ref id="ref-rauber2017foolbox">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Rauber</surname><given-names>Jonas</given-names></name>
          <name><surname>Brendel</surname><given-names>Wieland</given-names></name>
          <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
        </person-group>
        <article-title>Foolbox: A Python toolbox to benchmark the robustness of machine learning models</article-title>
        <source>Reliable machine learning in the wild workshop, 34th international conference on machine learning</source>
        <year iso-8601-date="2017">2017</year>
        <uri>https://arxiv.org/abs/1707.04131</uri>
      </element-citation>
    </ref>
    <ref id="ref-rauber2020eagerpy">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Rauber</surname><given-names>Jonas</given-names></name>
          <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
          <name><surname>Brendel</surname><given-names>Wieland</given-names></name>
        </person-group>
        <article-title>EagerPy: Writing code that works natively with PyTorch, TensorFlow, JAX, and NumPy</article-title>
        <source>arXiv preprint arXiv:2008.04175</source>
        <year iso-8601-date="2020">2020</year>
        <uri>https://eagerpy.jonasrauber.de</uri>
      </element-citation>
    </ref>
    <ref id="ref-numpy">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Oliphant</surname><given-names>Travis</given-names></name>
        </person-group>
        <article-title>NumPy: A guide to NumPy</article-title>
        <publisher-name>USA: Trelgol Publishing</publisher-name>
        <year iso-8601-date="2006">2006</year>
        <uri>http://www.numpy.org/</uri>
      </element-citation>
    </ref>
    <ref id="ref-pep484">
      <element-citation publication-type="report">
        <person-group person-group-type="author">
          <name><surname>Rossum</surname><given-names>Guido van</given-names></name>
          <name><surname>Lehtosalo</surname><given-names>Jukka</given-names></name>
          <name><surname>Langa</surname><given-names>Łukasz</given-names></name>
        </person-group>
        <article-title>Type hints</article-title>
        <publisher-name>Python Software Foundation</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <uri>https://www.python.org/dev/peps/pep-0484/</uri>
      </element-citation>
    </ref>
    <ref id="ref-brendel2018decisionbased">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Brendel</surname><given-names>Wieland</given-names></name>
          <name><surname>Rauber</surname><given-names>Jonas</given-names></name>
          <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
        </person-group>
        <article-title>Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</article-title>
        <source>International conference on learning representations</source>
        <year iso-8601-date="2018">2018</year>
        <uri>https://openreview.net/forum?id=SyZI0GWCZ</uri>
      </element-citation>
    </ref>
    <ref id="ref-schott2018towards">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Schott</surname><given-names>Lukas</given-names></name>
          <name><surname>Rauber</surname><given-names>Jonas</given-names></name>
          <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
          <name><surname>Brendel</surname><given-names>Wieland</given-names></name>
        </person-group>
        <article-title>Towards the first adversarially robust neural network model on MNIST</article-title>
        <source>International conference on learning representations</source>
        <year iso-8601-date="2019">2019</year>
        <uri>https://openreview.net/forum?id=S1EHOsC9tX</uri>
      </element-citation>
    </ref>
    <ref id="ref-rauber2020fast">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Rauber</surname><given-names>Jonas</given-names></name>
          <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
        </person-group>
        <article-title>Fast differentiable clipping-aware normalization and rescaling</article-title>
        <source>arXiv preprint arXiv:2007.07677</source>
        <year iso-8601-date="2020">2020</year>
        <uri>https://github.com/jonasrauber/clipping-aware-rescaling</uri>
      </element-citation>
    </ref>
    <ref id="ref-brendel2019accurate">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Brendel</surname><given-names>Wieland</given-names></name>
          <name><surname>Rauber</surname><given-names>Jonas</given-names></name>
          <name><surname>Kümmerer</surname><given-names>Matthias</given-names></name>
          <name><surname>Ustyuzhaninov</surname><given-names>Ivan</given-names></name>
          <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
        </person-group>
        <article-title>Accurate, reliable and fast robustness evaluation</article-title>
        <source>Advances in neural information processing systems 32</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-chen2020hopskipjumpattack">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Chen</surname><given-names>Jianbo</given-names></name>
          <name><surname>Jordan</surname><given-names>Michael I</given-names></name>
          <name><surname>Wainwright</surname><given-names>Martin J</given-names></name>
        </person-group>
        <article-title>HopSkipJumpAttack: A query-efficient decision-based attack</article-title>
        <source>2020 IEEE symposium on security and privacy (SP)</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <uri>http://dx.doi.org/10.1109/SP40000.2020.00045</uri>
        <pub-id pub-id-type="doi">10.1109/SP40000.2020.00045</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>https://github.com/tensorflow/cleverhans</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>https://github.com/Trusted-AI/adversarial-robustness-toolbox</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>https://github.com/BorealisAI/advertorch</p>
  </fn>
</fn-group>
</back>
</article>
