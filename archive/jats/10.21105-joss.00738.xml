<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">738</article-id>
<article-id pub-id-type="doi">10.21105/joss.00738</article-id>
<title-group>
<article-title>dit: a Python package for discrete information
theory</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-2149-9085</contrib-id>
<string-name>Ryan G. James</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Christopher J. Ellison</string-name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>James P. Crutchfield</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Complexity Sciences Center, Department of Physics,
University of California at Davis</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>None</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2018-04-27">
<day>27</day>
<month>4</month>
<year>2018</year>
</pub-date>
<volume>3</volume>
<issue>25</issue>
<fpage>738</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>information theory</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>dit</monospace>(<xref alt="Dit, n.d." rid="ref-dit" ref-type="bibr"><italic>Dit</italic>,
  n.d.</xref>) is a Python package for the study of
  <bold>d</bold>iscrete <bold>i</bold>nformation <bold>t</bold>heory.
  Information theory is a mathematical framework for the study of
  quantifying, compressing, and communicating random variables [Cover
  &amp; Thomas
  (<xref alt="2006" rid="ref-Cover2006" ref-type="bibr">2006</xref>)](<xref alt="MacKay, 2003" rid="ref-MacKay2003" ref-type="bibr">MacKay,
  2003</xref>)(<xref alt="Yeung, 2008" rid="ref-Yeung2008" ref-type="bibr">Yeung,
  2008</xref>). More recently, information theory has been utilized
  within the physical and social sciences to quantify how different
  components of a system interact. <monospace>dit</monospace> is
  primarily concerned with this aspect of the theory.</p>
  <p>Information theory is a powerful extension to probability and
  statistics, quantifying dependencies among arbitrary random variables
  in a way tha tis consistent and comparable across systems and scales.
  Information theory was originally developed to quantify how quickly
  and reliably information could be transmitted across an arbitrary
  channel. The demands of modern, data-driven science have been coopting
  and extending these quantities and methods into unknown, multivariate
  settings where the interpretation and best practices are not known.
  For example, there are at least four reasonable multivariate
  generalizations of the mutual information, none of which inherit all
  the interpretations of the standard bivariate case. Which is best to
  use is context-dependent. <monospace>dit</monospace> implements a vast
  range of multivariate information measures in an effort to allow
  information practitioners to study how these various measures behave
  and interact in a variety of contexts. We hope that having all these
  measures and techniques implemented in one place will allow the
  development of robust techniques for the automated quantification of
  dependencies within a system and concrete interpretation of what those
  dependencies mean.</p>
  <p><monospace>dit</monospace> implements the vast majority of
  information measure defined in the literature, including entropies
  (Shannon(<xref alt="Cover &amp; Thomas, 2006" rid="ref-Cover2006" ref-type="bibr">Cover
  &amp; Thomas, 2006</xref>), Renyi, Tsallis), multivariate mutual
  informations (co-information[Bell
  (<xref alt="2003" rid="ref-Bell2003" ref-type="bibr">2003</xref>)](<xref alt="McGill, 1954" rid="ref-mcgill1954multivariate" ref-type="bibr">McGill,
  1954</xref>), total
  correlation(<xref alt="Watanabe, 1960" rid="ref-watanabe1960information" ref-type="bibr">Watanabe,
  1960</xref>), dual total correlation[Te Sun
  (<xref alt="1980" rid="ref-te1980multiple" ref-type="bibr">1980</xref>)](<xref alt="Han, 1975" rid="ref-Han1975linear" ref-type="bibr">Han,
  1975</xref>)(<xref alt="Abdallah &amp; Plumbley, 2012" rid="ref-Abdallah2012" ref-type="bibr">Abdallah
  &amp; Plumbley, 2012</xref>), CAEKL mutual
  information(<xref alt="Chan et al., 2015" rid="ref-chan2015multivariate" ref-type="bibr">Chan
  et al., 2015</xref>)), common informations (Gács-Körner[Gács &amp;
  Körner
  (<xref alt="1973" rid="ref-Gacs1973" ref-type="bibr">1973</xref>)](<xref alt="Tyagi et al., 2011" rid="ref-tyagi2011function" ref-type="bibr">Tyagi
  et al., 2011</xref>), Wyner[Wyner
  (<xref alt="1975" rid="ref-wyner1975common" ref-type="bibr">1975</xref>)](<xref alt="Liu et al., 2010" rid="ref-liu2010common" ref-type="bibr">Liu
  et al., 2010</xref>),
  exact(<xref alt="Kumar et al., 2014" rid="ref-kumar2014exact" ref-type="bibr">Kumar
  et al., 2014</xref>), functional, minimal sufficient statistic), and
  channel
  capacity(<xref alt="Cover &amp; Thomas, 2006" rid="ref-Cover2006" ref-type="bibr">Cover
  &amp; Thomas, 2006</xref>). It includes methods of studying joint
  distributions including information diagrams, connected
  informations[Schneidman et al.
  (<xref alt="2003" rid="ref-Schneidman2003" ref-type="bibr">2003</xref>)](<xref alt="Amari, 2001" rid="ref-Amari2001" ref-type="bibr">Amari,
  2001</xref>), marginal utility of
  information(<xref alt="Allen et al., 2017" rid="ref-allen2017multiscale" ref-type="bibr">Allen
  et al., 2017</xref>), and the complexity
  profile(<xref alt="Bar-Yam, 2004" rid="ref-Baryam2004" ref-type="bibr">Bar-Yam,
  2004</xref>). It also includes several more specialized modules
  including bounds on the secret key agreement
  rate(<xref alt="Maurer &amp; Wolf, 1997" rid="ref-maurer1997intrinsic" ref-type="bibr">Maurer
  &amp; Wolf, 1997</xref>), partial information
  decomposition(<xref alt="Williams &amp; Beer, 2010" rid="ref-williams2010nonnegative" ref-type="bibr">Williams
  &amp; Beer, 2010</xref>), rate-distortion
  theory(<xref alt="Cover &amp; Thomas, 2006" rid="ref-Cover2006" ref-type="bibr">Cover
  &amp; Thomas, 2006</xref>) &amp; information
  bottleneck(<xref alt="Tishby et al., 2000" rid="ref-tishby2000information" ref-type="bibr">Tishby
  et al., 2000</xref>), and others. Please see the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/dit/dit"><monospace>dit</monospace>
  homepage</ext-link> for a complete and up-to-date list.</p>
  <p>Where possible, the implementations in <monospace>dit</monospace>
  support multivariate, conditional forms even if not defined that way
  in the literature. For example, <monospace>dit</monospace> implements
  the multivariate, conditional exact common information even though it
  was only defined for two variables.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Cover2006">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Cover</surname><given-names>T. M.</given-names></name>
          <name><surname>Thomas</surname><given-names>J. A.</given-names></name>
        </person-group>
        <source>Elements of Information Theory</source>
        <publisher-name>Wiley-Interscience</publisher-name>
        <publisher-loc>New York</publisher-loc>
        <year iso-8601-date="2006">2006</year>
        <isbn>0471241954</isbn>
        <pub-id pub-id-type="doi">10.1002/047174882X</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-MacKay2003">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>MacKay</surname><given-names>D. J. C.</given-names></name>
        </person-group>
        <source>Information theory, inference and learning algorithms</source>
        <publisher-name>Cambridge university press</publisher-name>
        <year iso-8601-date="2003">2003</year>
      </element-citation>
    </ref>
    <ref id="ref-Yeung2008">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Yeung</surname><given-names>R. W.</given-names></name>
        </person-group>
        <source>Information theory and network coding</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2008">2008</year>
        <pub-id pub-id-type="doi">10.1007/978-0-387-79234-7</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-dit">
      <element-citation>
        <article-title>Dit: A python package for discrete information theory. Available at: Https://github.com/dit/dit</article-title>
        <date-in-citation content-type="access-date"><year iso-8601-date="2018-04-27">2018</year><month>04</month><day>27</day></date-in-citation>
      </element-citation>
    </ref>
    <ref id="ref-Bell2003">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Bell</surname><given-names>A. J.</given-names></name>
        </person-group>
        <article-title>The co-information lattice</article-title>
        <source>Proc. Fifth intl. Workshop on independent component analysis and blind signal separation</source>
        <person-group person-group-type="editor">
          <name><surname>S. Amari</surname><given-names>S. Makino</given-names><suffix>A. Cichocki</suffix></name>
          <name><surname>Murata</surname><given-names>N.</given-names></name>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>New York</publisher-loc>
        <year iso-8601-date="2003">2003</year>
        <volume>ICA 2003</volume>
      </element-citation>
    </ref>
    <ref id="ref-mcgill1954multivariate">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>McGill</surname><given-names>W. J.</given-names></name>
        </person-group>
        <article-title>Multivariate information transmission</article-title>
        <source>Psychometrika</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="1954">1954</year>
        <volume>19</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1109/TIT.1954.1057469</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-watanabe1960information">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Watanabe</surname><given-names>S.</given-names></name>
        </person-group>
        <article-title>Information theoretical analysis of multivariate correlation</article-title>
        <source>IBM Journal of research and development</source>
        <publisher-name>IBM</publisher-name>
        <year iso-8601-date="1960">1960</year>
        <volume>4</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1147/rd.41.0066</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-te1980multiple">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Te Sun</surname><given-names>H.</given-names></name>
        </person-group>
        <article-title>Multiple mutual informations and multiple interactions in frequency data</article-title>
        <source>Information and Control</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="1980">1980</year>
        <volume>46</volume>
        <pub-id pub-id-type="doi">10.1016/S0019-9958(80)90478-7</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Han1975linear">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Han</surname><given-names>T. S.</given-names></name>
        </person-group>
        <article-title>Linear dependence structure of the entropy space</article-title>
        <source>Information and Control</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="1975">1975</year>
        <volume>29</volume>
        <pub-id pub-id-type="doi">10.1016/S0019-9958(75)80004-0</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Abdallah2012">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Abdallah</surname><given-names>S. A.</given-names></name>
          <name><surname>Plumbley</surname><given-names>M. D.</given-names></name>
        </person-group>
        <article-title>A measure of statistical complexity based on predictive information with application to finite spin systems</article-title>
        <source>Physics Letters A</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2012">2012</year>
        <volume>376</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1016/j.physleta.2011.10.066</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-chan2015multivariate">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Chan</surname><given-names>Chung</given-names></name>
          <name><surname>Al-Bashabsheh</surname><given-names>Ali</given-names></name>
          <name><surname>Ebrahimi</surname><given-names>Javad B</given-names></name>
          <name><surname>Kaced</surname><given-names>Tarik</given-names></name>
          <name><surname>Liu</surname><given-names>Tie</given-names></name>
        </person-group>
        <article-title>Multivariate mutual information inspired by secret-key agreement</article-title>
        <source>Proceedings of the IEEE</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <volume>103</volume>
        <issue>10</issue>
        <pub-id pub-id-type="doi">10.1109/JPROC.2015.2458316</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Gacs1973">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gács</surname><given-names>Peter</given-names></name>
          <name><surname>Körner</surname><given-names>János</given-names></name>
        </person-group>
        <article-title>Common information is far less than mutual information</article-title>
        <source>Problems of Control and Information Theory</source>
        <year iso-8601-date="1973">1973</year>
        <volume>2</volume>
        <issue>2</issue>
      </element-citation>
    </ref>
    <ref id="ref-tyagi2011function">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tyagi</surname><given-names>H.</given-names></name>
          <name><surname>Narayan</surname><given-names>P.</given-names></name>
          <name><surname>Gupta</surname><given-names>P.</given-names></name>
        </person-group>
        <article-title>When is a function securely computable?</article-title>
        <source>Information Theory, IEEE Transactions on</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <volume>57</volume>
        <issue>10</issue>
        <pub-id pub-id-type="doi">10.1109/TIT.2011.2165807</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wyner1975common">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wyner</surname><given-names>A. D.</given-names></name>
        </person-group>
        <article-title>The common information of two dependent random variables</article-title>
        <source>Information Theory, IEEE Transactions on</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="1975">1975</year>
        <volume>21</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1109/TIT.1975.1055346</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-liu2010common">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Liu</surname><given-names>Wei</given-names></name>
          <name><surname>Xu</surname><given-names>Ge</given-names></name>
          <name><surname>Chen</surname><given-names>Biao</given-names></name>
        </person-group>
        <article-title>The common information of n dependent random variables</article-title>
        <source>Communication, control, and computing (allerton), 2010 48th annual allerton conference on</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2010">2010</year>
        <pub-id pub-id-type="doi">10.1109/ALLERTON.2010.5706995</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kumar2014exact">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kumar</surname><given-names>G. R.</given-names></name>
          <name><surname>Li</surname><given-names>C. T.</given-names></name>
          <name><surname>El Gamal</surname><given-names>A.</given-names></name>
        </person-group>
        <article-title>Exact common information</article-title>
        <source>Information theory (ISIT), 2014 IEEE international symposium on</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2014">2014</year>
        <pub-id pub-id-type="doi">10.1109/ISIT.2014.6874815</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-allen2017multiscale">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Allen</surname><given-names>Benjamin</given-names></name>
          <name><surname>Stacey</surname><given-names>Blake C</given-names></name>
          <name><surname>Bar-Yam</surname><given-names>Yaneer</given-names></name>
        </person-group>
        <article-title>Multiscale information theory and the marginal utility of information</article-title>
        <source>Entropy</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>19</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.3390/e19060273</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Baryam2004">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bar-Yam</surname><given-names>Y.</given-names></name>
        </person-group>
        <article-title>Multiscale complexity/entropy</article-title>
        <source>Advances in Complex Systems</source>
        <publisher-name>World Scientific</publisher-name>
        <year iso-8601-date="2004">2004</year>
        <volume>7</volume>
        <issue>01</issue>
        <pub-id pub-id-type="doi">10.1142/S0219525904000068</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Amari2001">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Amari</surname><given-names>Shun-ichi</given-names></name>
        </person-group>
        <article-title>Information geometry on hierarchy of probability distributions</article-title>
        <source>Information Theory, IEEE Transactions on</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2001">2001</year>
        <volume>47</volume>
        <issue>5</issue>
        <pub-id pub-id-type="doi">10.1109/18.930911</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Schneidman2003">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Schneidman</surname><given-names>E.</given-names></name>
          <name><surname>Still</surname><given-names>S.</given-names></name>
          <name><surname>Berry</surname><given-names>M. J.</given-names></name>
          <name><surname>Bialek</surname><given-names>W.</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Network information and connected correlations</article-title>
        <source>Phys. Rev. Lett.</source>
        <publisher-name>APS</publisher-name>
        <year iso-8601-date="2003">2003</year>
        <volume>91</volume>
        <issue>23</issue>
        <pub-id pub-id-type="doi">10.1103/PhysRevLett.91.238701</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-williams2010nonnegative">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Williams</surname><given-names>Paul L</given-names></name>
          <name><surname>Beer</surname><given-names>Randall D</given-names></name>
        </person-group>
        <article-title>Nonnegative decomposition of multivariate information</article-title>
        <source>arXiv preprint arXiv:1004.2515</source>
        <year iso-8601-date="2010">2010</year>
      </element-citation>
    </ref>
    <ref id="ref-tishby2000information">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Tishby</surname><given-names>Naftali</given-names></name>
          <name><surname>Pereira</surname><given-names>Fernando C</given-names></name>
          <name><surname>Bialek</surname><given-names>William</given-names></name>
        </person-group>
        <article-title>The information bottleneck method</article-title>
        <source>arXiv preprint physics/0004057</source>
        <year iso-8601-date="2000">2000</year>
      </element-citation>
    </ref>
    <ref id="ref-maurer1997intrinsic">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Maurer</surname><given-names>Ueli</given-names></name>
          <name><surname>Wolf</surname><given-names>Stefan</given-names></name>
        </person-group>
        <article-title>The intrinsic conditional mutual information and perfect secrecy</article-title>
        <source>IEEE international symposium on information theory</source>
        <publisher-name>Citeseer</publisher-name>
        <year iso-8601-date="1997">1997</year>
        <pub-id pub-id-type="doi">10.1109/ISIT.1997.613003</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
