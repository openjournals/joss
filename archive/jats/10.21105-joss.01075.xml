<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1075</article-id>
<article-id pub-id-type="doi">10.21105/joss.01075</article-id>
<title-group>
<article-title>Yellowbrick: Visualizing the Scikit-Learn Model Selection
Process</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0660-7682</contrib-id>
<string-name>Benjamin Bengfort</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-1143-044X</contrib-id>
<string-name>Rebecca Bilbro</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Georgetown University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2018-07-30">
<day>30</day>
<month>7</month>
<year>2018</year>
</pub-date>
<volume>4</volume>
<issue>35</issue>
<fpage>1075</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>machine learning</kwd>
<kwd>visual analysis</kwd>
<kwd>model selection</kwd>
<kwd>python</kwd>
<kwd>scikit-learn</kwd>
<kwd>matplotlib</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Discussions of machine learning are frequently characterized by a
  singular focus on algorithmic behavior. Be it logistic regression,
  random forests, Bayesian methods, or artificial neural networks,
  practitioners are often quick to express their preference. However,
  model selection is more nuanced than simply picking the “right” or
  “wrong” algorithm. In practice, the workflow includes multiple
  iterations through feature engineering, algorithm selection, and
  hyperparameter tuning — summarized by Kumar et al. as a search for the
  maximally performing model selection triple
  (<xref alt="Kumar et al., 2016" rid="ref-kumar2016model" ref-type="bibr">Kumar
  et al., 2016</xref>). “Model selection,” they explain, “is iterative
  and exploratory because the space of [model selection triples] is
  usually infinite, and it is generally impossible for analysts to know
  a priori which [combination] will yield satisfactory accuracy and/or
  insights.”</p>
  <p>Treating model selection as search has led to automation through
  grid search methods, standardized APIs, drag and drop GUIs, and
  specialized database systems. However, the search problem is
  computationally intractable and research in both machine learning
  (<xref alt="Wickham et al., 2015" rid="ref-wickham_visualizing_2015" ref-type="bibr">Wickham
  et al., 2015</xref>) and visual analytics
  (<xref alt="Liu et al., 2017" rid="ref-liu_wang_liu_zhu_2017" ref-type="bibr">Liu
  et al., 2017</xref>) suggests human intuition and guidance can more
  effectively hone in on quality models than exhaustive optimization
  methods. By visualizing the model selection process, data scientists
  can interactively steer towards final, interpretable models and avoid
  pitfalls and traps
  (<xref alt="Kapoor et al., 2010" rid="ref-kapoor2010interactive" ref-type="bibr">Kapoor
  et al., 2010</xref>).</p>
  <p>Yellowbrick is a response to the call for open source visual
  steering tools. For data scientists, Yellowbrick helps evaluate the
  stability and predictive value of machine learning models and improves
  the speed of the experimental workflow. For data engineers,
  Yellowbrick provides visual tools for monitoring model performance in
  real world applications. For users of models, Yellowbrick provides
  visual interpretation of the behavior of the model in high dimensional
  feature space. Finally, for students, Yellowbrick is a framework for
  understanding a large variety of algorithms and methods.</p>
  <p>Implemented in Python, the Yellowbrick visualization package
  achieves steering by extending both scikit-learn
  (<xref alt="Pedregosa et al., 2011" rid="ref-sklearn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) and Matplotlib
  (<xref alt="Hunter, 2007" rid="ref-matplotlib" ref-type="bibr">Hunter,
  2007</xref>). Like Yellowbrick, both scikit-learn and Matplotlib are
  extensions of SciPy
  (<xref alt="Jones et al., 2001--" rid="ref-scipy" ref-type="bibr">Jones
  et al., 2001--</xref>), libraries intended to facilitate scientific
  computing. Scikit-learn provides a generalized API for machine
  learning by exposing the concept of an
  <monospace>Estimator</monospace>, an object that learns from data.
  Yellowbrick in turn extends this concept with the idea of a
  <monospace>Visualizer</monospace>, an object that both learns from
  data and visualizes the result. Visualizers wrap Matplotlib procedures
  to produce publication-ready figures and rich visual analytics.</p>
  <p>Because Yellowbrick is part of a rich visual and machine learning
  ecosystem, it provides visualizations for feature and target analysis,
  classification, regression, and clustering model visualization,
  hyperparameter tuning, and text analysis. A few selected examples of
  visual diagnostics for model selection and their interpretations
  follow.</p>
  <fig>
    <caption><p>Feature Analysis</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="figures/feature_analysis.png" xlink:title="" />
  </fig>
  <p>Because “more data beats better algorithms”
  (<xref alt="Rajaraman, 2008" rid="ref-rajaraman2008more" ref-type="bibr">Rajaraman,
  2008</xref>), the first step to creating valid, predictive models is
  to find the minimum set of features that predicts the dependent
  variable. Generally, this means finding features that describe data in
  high dimensional space that are <italic>separable</italic> (i.e., by a
  hyperplane). Tools like <monospace>RadViz</monospace>,
  <monospace>ParallelCoordinates</monospace>, and
  <monospace>Manifold</monospace> help visualize high dimensional data
  for quick diagnostics. Bayesian models and regressions suffer when
  independent variables are collinear (i.e., exhibit pairwise
  correlation). <monospace>Rank2D</monospace> visualizations show
  pairwise correlations among features and can facilitate feature
  elimination.</p>
  <fig>
    <caption><p>Regression Model Tuning</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="figures/regression.png" xlink:title="" />
  </fig>
  <p>Regression models hypothesize some underlying function influenced
  by noise whose central tendency can be inferred. The
  <monospace>PredictionError</monospace> visualizer shows the
  relationship of actual to predicted values, giving a sense of
  heteroskedasticity in the target, or regions of more or less error as
  predictions deviate from the 45 degree line. The
  <monospace>ResidualsPlot</monospace> shows the relationship of error
  in the training and test data and can also show regions of increased
  variability in the predictive model.</p>
  <fig>
    <caption><p>Classification Model Tuning</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="figures/classification.png" xlink:title="" />
  </fig>
  <p>Classification analysis focuses on the precision and recall of the
  model’s prediction of individual classes. The
  <monospace>ClassificationReport</monospace> visualizer allows for
  rapid comparison between models as a visual heatmap of these metrics.
  The <monospace>DiscriminationThreshold</monospace> visualizer for
  binary classifiers shows how adjusting the threshold for positive
  classification may influence precision and recall globally, as well as
  the number of points that may require manual checking for stricter
  determination.</p>
  <fig>
    <caption><p>Clustering Model Tuning</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="figures/clustering.png" xlink:title="" />
  </fig>
  <p>Searching for structure in unlabelled data can be challenging
  because evaluation is largely qualitative. When using K-Means models,
  choosing K has a large impact on the quality of the analysis; the
  <monospace>KElbowVisualizer</monospace> can help select the best K
  given computational constraints. The
  <monospace>SilhouetteVisualizer</monospace> shows the relationship of
  points in each cluster relative to other clusters and gives an
  overview of the composition and size of each cluster which may hint at
  how models group similar data points.</p>
  <fig>
    <caption><p>Hyperparameter Tuning</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="figures/hyperparameter_tuning.png" xlink:title="" />
  </fig>
  <p>Yellowbrick also offers several other techniques for hyperparameter
  tuning. Model and regression-specific
  <monospace>AlphaSelection</monospace> visualizers help identify the
  impact of regularization on linear models and the influence of
  complexity on the trade-off between error due to bias or variance.
  More generally, the <monospace>LearningCurve</monospace> visualizer
  shows how sensitive models are to the amount of data the model is
  trained on.</p>
  <p>Yellowbrick includes many more visualizations, intended to fit
  directly into the machine learning workflow, and many more are being
  added in each new release. From text analysis-specific visualizations
  to missing data analysis, to a <monospace>contrib</monospace> module
  that focuses on other machine learning libraries, Yellowbrick has
  tools to facilitate all parts of hypothesis driven development. The
  source code for Yellowbrick has been archived to Zenodo and the most
  recent version can be obtained with the linked DOI:
  (<xref alt="Bengfort et al., 2018" rid="ref-zenodo" ref-type="bibr">Bengfort
  et al., 2018</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Since we first introduced the idea of Yellowbrick at PyCon 2016,
  many people have joined us and stuck with us through 12 releases,
  ensuring the success of the project. Nathan Danielsen joined very
  early on and was one of our first maintainers, bringing an engineering
  perspective to our work and giving us much needed stability in
  testing. Larry Gray, Neal Humphrey, Jason Keung, Prema Roman, Kristen
  McIntyre, Jessica D’Amico and Adam Morris have also all joined our
  project as maintainers and core contributors, and we can’t thank them
  enough.</p>
  <p>Yellowbrick would not be possible without the invaluable
  contributions of those in the Python and Data Science communities. At
  the time of this writing, GitHub reports that 46 contributors have
  submitted pull requests that have been merged and released, and we
  expect this number to continue to grow. Every week, users submit
  feature requests, bug reports, suggestions and questions that allow us
  to make the software better and more robust. Others write blog posts
  about using Yellowbrick, encouraging both newcomers and seasoned
  practitioners to more fully understand the models they are fitting.
  Our sincere thanks to the community for their ongoing support and
  participation.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-zenodo">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bengfort</surname><given-names>Benjamin</given-names></name>
          <name><surname>Bilbro</surname><given-names>Rebecca</given-names></name>
          <name><surname>Danielsen</surname><given-names>Nathan</given-names></name>
          <name><surname>Gray</surname><given-names>Larry</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Yellowbrick</article-title>
        <year iso-8601-date="2018-11">2018</year><month>11</month>
        <uri>https://doi.org/10.5281/zenodo.1206239</uri>
        <pub-id pub-id-type="doi">10.5281/zenodo.1206239</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-sklearn">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
          <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
          <name><surname>Gramfort</surname><given-names>A.</given-names></name>
          <name><surname>Michel</surname><given-names>V.</given-names></name>
          <name><surname>Thirion</surname><given-names>B.</given-names></name>
          <name><surname>Grisel</surname><given-names>O.</given-names></name>
          <name><surname>Blondel</surname><given-names>M.</given-names></name>
          <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
          <name><surname>Weiss</surname><given-names>R.</given-names></name>
          <name><surname>Dubourg</surname><given-names>V.</given-names></name>
          <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
          <name><surname>Passos</surname><given-names>A.</given-names></name>
          <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
          <name><surname>Brucher</surname><given-names>M.</given-names></name>
          <name><surname>Perrot</surname><given-names>M.</given-names></name>
          <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>Journal of Machine Learning Research</source>
        <year iso-8601-date="2011">2011</year>
        <volume>12</volume>
      </element-citation>
    </ref>
    <ref id="ref-matplotlib">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hunter</surname><given-names>J. D.</given-names></name>
        </person-group>
        <article-title>Matplotlib: A 2D graphics environment</article-title>
        <source>Computing In Science &amp; Engineering</source>
        <publisher-name>IEEE COMPUTER SOC</publisher-name>
        <year iso-8601-date="2007">2007</year>
        <volume>9</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-scipy">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Jones</surname><given-names>Eric</given-names></name>
          <name><surname>Oliphant</surname><given-names>Travis</given-names></name>
          <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>SciPy: Open source scientific tools for Python</article-title>
        <uri>http://www.scipy.org/</uri>
      </element-citation>
    </ref>
    <ref id="ref-kumar2016model">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kumar</surname><given-names>Arun</given-names></name>
          <name><surname>McCann</surname><given-names>Robert</given-names></name>
          <name><surname>Naughton</surname><given-names>Jeffrey</given-names></name>
          <name><surname>Patel</surname><given-names>Jignesh M</given-names></name>
        </person-group>
        <article-title>Model selection management systems: The next frontier of advanced analytics</article-title>
        <source>ACM SIGMOD Record</source>
        <publisher-name>ACM</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <volume>44</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1145/2935694.2935698</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-liu_wang_liu_zhu_2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Liu</surname><given-names>Shixia</given-names></name>
          <name><surname>Wang</surname><given-names>Xiting</given-names></name>
          <name><surname>Liu</surname><given-names>Mengchen</given-names></name>
          <name><surname>Zhu</surname><given-names>Jun</given-names></name>
        </person-group>
        <article-title>Towards better analysis of machine learning models: A visual analytics perspective</article-title>
        <source>Visual Informatics</source>
        <year iso-8601-date="2017-03">2017</year><month>03</month>
        <volume>1</volume>
        <issue>1</issue>
        <uri>https://www.sciencedirect.com/science/article/pii/S2468502X17300086</uri>
        <pub-id pub-id-type="doi">10.1016/j.visinf.2017.01.006</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wickham_visualizing_2015">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wickham</surname><given-names>Hadley</given-names></name>
          <name><surname>Cook</surname><given-names>Dianne</given-names></name>
          <name><surname>Hofmann</surname><given-names>Heike</given-names></name>
        </person-group>
        <article-title>Visualizing statistical models: Removing the blindfold</article-title>
        <source>Statistical Analysis and Data Mining: The ASA Data Science Journal</source>
        <year iso-8601-date="2015">2015</year>
        <date-in-citation content-type="access-date"><year iso-8601-date="2015-10-26">2015</year><month>10</month><day>26</day></date-in-citation>
        <volume>8</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1002/sam.11271</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kapoor2010interactive">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kapoor</surname><given-names>Ashish</given-names></name>
          <name><surname>Lee</surname><given-names>Bongshin</given-names></name>
          <name><surname>Tan</surname><given-names>Desney</given-names></name>
          <name><surname>Horvitz</surname><given-names>Eric</given-names></name>
        </person-group>
        <article-title>Interactive optimization for steering machine classification</article-title>
        <source>Proceedings of the SIGCHI conference on human factors in computing systems</source>
        <publisher-name>ACM</publisher-name>
        <year iso-8601-date="2010">2010</year>
        <pub-id pub-id-type="doi">10.1145/1753326.1753529</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-rajaraman2008more">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Rajaraman</surname><given-names>Anand</given-names></name>
        </person-group>
        <article-title>More data usually beats better algorithms</article-title>
        <source>Datawocky Blog</source>
        <year iso-8601-date="2008">2008</year>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
