<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">1420</article-id>
<article-id pub-id-type="doi">10.21105/joss.01420</article-id>
<title-group>
<article-title>ECabc: A feature tuning program focused on Artificial
Neural Network hyperparameters</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9884-7351</contrib-id>
<string-name>Sanskriti Sharma</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0690-576X</contrib-id>
<string-name>Hernan Gelaf-Romer</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7363-4050</contrib-id>
<string-name>Travis Kessler</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5455-8611</contrib-id>
<string-name>John Hunter Mack</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Energy and Combustion Research Laboratory, University of
Massachusetts Lowell, Lowell, MA 01854, U.S.A.</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2019-04-12">
<day>12</day>
<month>4</month>
<year>2019</year>
</pub-date>
<volume>4</volume>
<issue>39</issue>
<fpage>1420</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>artificial bee colony</kwd>
<kwd>hyperparameter optimization</kwd>
<kwd>machine learning</kwd>
<kwd>artificial neural networks</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>A rich body of literature exists regarding the optimization of
  artificial neural networks (ANN), a method comprised of densely
  interconnected adaptive processing units
  (<xref alt="Hassoun, 2009" rid="ref-Hassoun_2009" ref-type="bibr">Hassoun,
  2009</xref>). This need for optimization arises from a large number of
  user-set parameters that greatly affect the quality of the ANN’s
  training. Traditionally, this has been done manually. However,
  presently with higher computing capacity, it is possible to utilize
  algorithmic approaches which greatly increase the speed of
  optimization while enhancing model performance. Commonly used
  optimization routines include genetic algorithms
  (<xref alt="Castillo et al., 2000" rid="ref-Castillo_2000" ref-type="bibr">Castillo
  et al., 2000</xref>) and particle swarm optimizations
  (<xref alt="Cam et al., 2015" rid="ref-Cam_2015" ref-type="bibr">Cam
  et al., 2015</xref>). However, Cam et al.
  (<xref alt="2015" rid="ref-Cam_2015" ref-type="bibr">2015</xref>)
  showed that an Artificial Bee Colony (ABC) performs best by providing
  higher accuracy and a shorter run time.</p>
  <p>ECabc is an open source Python package based on an Artificial Bee
  Colony, used for tuning functions such as hyperparameters of
  artificial neural networks. The method is modelled after the honey
  foraging techniques of bees, where the search space for solutions is
  in as many dimensions as the number of parameters being tuned. The
  algorithm is outlined in Figure 1.</p>
  <fig>
    <caption><p>Stages of the algorithm</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="images/state.png" xlink:title="" />
  </fig>
  <p>The algorithm begins with initializing a pre-decided number of bees
  and running the fitness function for each bee, thus evaluating them.
  The fitness function is a user defined function that is used to
  generate the solutions and the error associated with each employer
  bee’s values at a given position. After evaluation, the bees enter the
  “Employer Bee Phase” during which each bee is moved in one random
  “dimension” and then reevaluated. If the new position provides a
  better fitness score, the old position is abandoned and the bee
  memorizes the new one. This is repeated for every employer bee. Next,
  ECabc enters the “Calculate Probabilities” state where the fitness
  scores of all the employer bees are totaled and the best position is
  memorized. Then a probability is calculated for each bee, which is the
  bee’s fitness score divided by the total score. The next phase of the
  algorithm is the “Onlooker Bee Phase”, in which employer bees are
  chosen based on their probabilities, i.e., those with a higher
  probability will have a higher likelihood of being chosen. The chosen
  bee is then moved in one dimension and then reevaluated. Again, if the
  new score is better, the old score is forgotten in favor of it. Every
  time a bee is moved in one dimension but doesn’t choose the new
  position, the bee’s counter goes up. Then in the “Scout Bee Phase”, if
  the counter has hit a certain pre-decided number, the bee is given a
  new, randomly assigned position. Ultimately, the best fitness value of
  the entire population is compared to a pre-decided value. If it
  doesn’t meet that standard, the ECabc returns to the Employer Bee
  Phase. Alternatively, a specified number of iterations can be used as
  a stopping criteria and for each iteration, the algorithm will return
  to the Employer Bee Phase.</p>
  <p>While it has several applications, ECabc has been successfully used
  by the Energy and Combustion Research Laboratory (ECRL) at the
  University of Massachusetts Lowell to tune the hyperparameters of
  ECNet, a large-scale machine learning project for predicting fuel
  properties
  (<xref alt="Kessler &amp; Mack, 2017" rid="ref-Kessler_2017_1" ref-type="bibr">Kessler
  &amp; Mack, 2017</xref>). ECNet provides scientists an open source
  tool for predicting key fuel properties of potential next-generation
  biofuels, reducing the need for costly fuel synthesis and
  experimentation
  (<xref alt="Kessler et al., 2017" rid="ref-Kessler_2017_2" ref-type="bibr">Kessler
  et al., 2017</xref>). By increasing the accuracy of ECNet and similar
  models efficiently, ECabc helps to provide a higher degree of
  confidence in discovering new, optimal fuels. A single run of ECabc on
  ECNet yielded a lower average root mean square error (RMSE) for cetane
  number (CN) and yield sooting index (YSI) when compared to the RMSE
  generated by a year of manual tuning. While the manual tuning
  generated an RMSE of 10.13, the ECabc was able to yield an RMSE of
  8.06 in one run of 500 iterations.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Hassoun_2009">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Hassoun</surname><given-names>M. H.</given-names></name>
        </person-group>
        <source>Fundamentals of Artificial Neural Networks</source>
        <publisher-name>PHI Learning</publisher-name>
        <publisher-loc>Delhi, India</publisher-loc>
        <year iso-8601-date="2009">2009</year>
        <isbn>9788120313569</isbn>
      </element-citation>
    </ref>
    <ref id="ref-Castillo_2000">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Castillo</surname><given-names>P.</given-names></name>
          <name><surname>Merelo</surname><given-names>J.</given-names></name>
          <name><surname>Prieto</surname><given-names>A.</given-names></name>
          <name><surname>Rivas</surname><given-names>V.</given-names></name>
          <name><surname>Romero</surname><given-names>G.</given-names></name>
        </person-group>
        <article-title>G-Prop: Global optimization of multilayer perceptrons using GAs</article-title>
        <source>Neurocomputing</source>
        <year iso-8601-date="2000-11">2000</year><month>11</month>
        <volume>35</volume>
        <issue>1–4</issue>
        <pub-id pub-id-type="doi">10.1016/s0925-2312(00)00302-7</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Cam_2015">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Cam</surname><given-names>Z.</given-names></name>
          <name><surname>Yetis</surname><given-names>S.</given-names></name>
          <name><surname>Yildirim</surname><given-names>T.</given-names></name>
        </person-group>
        <source>Learning parameter optimization of multi-layer perceptron using artificial bee colony, genetic algorithm and particle swarm optimization</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2015-03">2015</year><month>03</month>
        <isbn>978-1-4799-8221-9</isbn>
        <pub-id pub-id-type="doi">10.1109/sami.2015.7061899</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Kessler_2017_1">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kessler</surname><given-names>T.</given-names></name>
          <name><surname>Mack</surname><given-names>J.</given-names></name>
        </person-group>
        <article-title>ECNet: Large scale machine learning projects for fuel property prediction</article-title>
        <source>Journal of Open Source Software</source>
        <year iso-8601-date="2017">2017</year>
        <volume>2</volume>
        <issue>17</issue>
        <pub-id pub-id-type="doi">10.21105/joss.00401</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Kessler_2017_2">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kessler</surname><given-names>T.</given-names></name>
          <name><surname>Sacia</surname><given-names>E.</given-names></name>
          <name><surname>Bell</surname><given-names>A.</given-names></name>
          <name><surname>Mack</surname><given-names>J.</given-names></name>
        </person-group>
        <article-title>Artificial neural network based predictions of cetane number for furanic biofuel additives</article-title>
        <source>Fuel</source>
        <year iso-8601-date="2017">2017</year>
        <volume>206</volume>
        <pub-id pub-id-type="doi">10.1016/j.fuel.2017.06.015</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
