<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2868</article-id>
<article-id pub-id-type="doi">10.21105/joss.02868</article-id>
<title-group>
<article-title>ivadomed: A Medical Imaging Deep Learning
Toolbox</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4318-0024</contrib-id>
<string-name>Charley Gros</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-8581-2929</contrib-id>
<string-name>Andreanne Lemay</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5554-8108</contrib-id>
<string-name>Olivier Vincent</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<string-name>Lucas Rouhier</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-9786-9332</contrib-id>
<string-name>Marie-Helene Bourget</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Anthime Bucquet</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Joseph Paul Cohen</string-name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-3662-9532</contrib-id>
<string-name>Julien Cohen-Adad</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>NeuroPoly Lab, Institute of Biomedical Engineering,
Polytechnique Montreal, Montreal, Canada</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Mila, Quebec AI Institute, Montreal, QC,
Canada</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>AIMI, Stanford University, Stanford, CA, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Functional Neuroimaging Unit, CRIUGM, Université de
Montréal, Montreal, QC, Canada</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2020-11-03">
<day>3</day>
<month>11</month>
<year>2020</year>
</pub-date>
<volume>6</volume>
<issue>58</issue>
<fpage>2868</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Deep Learning</kwd>
<kwd>Medical Imaging</kwd>
<kwd>Segmentation</kwd>
<kwd>Open-source</kwd>
<kwd>Pipeline</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>ivadomed</monospace> is an open-source Python package
  for designing, end-to-end training, and evaluating deep learning
  models applied to medical imaging data. The package includes APIs,
  command-line tools, documentation, and tutorials.
  <monospace>ivadomed</monospace> also includes pre-trained models such
  as spinal tumor segmentation and vertebral labeling. Original features
  of <monospace>ivadomed</monospace> include a data loader that can
  parse image and subject metadata for custom data splitting or extra
  information during training and evaluation. Any dataset following the
  <ext-link ext-link-type="uri" xlink:href="https://bids.neuroimaging.io/">Brain
  Imaging Data Structure (BIDS)</ext-link> convention will be compatible
  with <monospace>ivadomed</monospace>. Beyond the traditional deep
  learning methods, <monospace>ivadomed</monospace> features
  cutting-edge architectures, such as FiLM
  (<xref alt="Perez et al., 2017" rid="ref-perez2017film" ref-type="bibr">Perez
  et al., 2017</xref>) and HeMis
  (<xref alt="Havaei et al., 2016" rid="ref-havaei2016hemis" ref-type="bibr">Havaei
  et al., 2016</xref>), as well as various uncertainty estimation
  methods (aleatoric and epistemic), and losses adapted to imbalanced
  classes and non-binary predictions. Example applications of
  <monospace>ivadomed</monospace> include MRI object detection,
  segmentation, and labeling of anatomical and pathological structures.
  <monospace>ivadomed</monospace>’s main project page is available at
  https://ivadomed.org.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Deep learning applied to medical imaging presents many challenges:
  datasets are often not publicly-available, ground truth labels are
  difficult to produce, and needs are usually tailored to particular
  datasets and tasks
  (<xref alt="Kim et al., 2019" rid="ref-kim_deep_2019" ref-type="bibr">Kim
  et al., 2019</xref>). There already exists a few deep learning
  frameworks for medical imaging, but
  <ext-link ext-link-type="uri" xlink:href="https://ivadomed.org/en/latest/purpose.html#comparison-with-other-projects">each
  has their pros &amp; cons</ext-link>. <monospace>ivadomed</monospace>
  notably addresses unmet needs in terms of data management,
  readily-available uncertainty outputs, missing modalities (in case of
  multi-channel training) and model comparison, to only name few of the
  original features.</p>
  <p>Another challenge of medical imaging is the heterogeneity of the
  data across hospitals (e.g., contrast, resolution), making it
  difficult to create models that can generalize well. Recent
  cutting-edge methods address this problem, such as FiLM
  (<xref alt="Perez et al., 2017" rid="ref-perez2017film" ref-type="bibr">Perez
  et al., 2017</xref>) and HeMis
  (<xref alt="Havaei et al., 2016" rid="ref-havaei2016hemis" ref-type="bibr">Havaei
  et al., 2016</xref>), however they are usually not implemented within
  a comprehensive framework. In addition to providing these
  architectures, <monospace>ivadomed</monospace> features losses adapted
  to imbalanced classes and non-binary predictions.</p>
  <fig>
    <caption><p>Overview of <monospace>ivadomed</monospace> main
    features.<styled-content id="figU003Aoverview"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="https://raw.githubusercontent.com/ivadomed/doc-figures/main/index/overview_title.png" xlink:title="" />
  </fig>
  <sec id="loader">
    <title>Loader</title>
    <p><bold>Standard data structure:</bold> In machine learning, lots
    of time is spent curating the data (renaming, filtering per feature)
    before entering the training pipeline
    (<xref alt="Willemink et al., 2020" rid="ref-Willemink2020-au" ref-type="bibr">Willemink
    et al., 2020</xref>). <monospace>ivadomed</monospace> features an
    advanced data loader compatible with a standardized data structure
    in neuroimaging: the Brain Imaging Data Structure (BIDS)
    (<xref alt="Gorgolewski et al., 2016" rid="ref-bids_2016" ref-type="bibr">Gorgolewski
    et al., 2016</xref>). Thus, any dataset following the BIDS
    convention can readily be used by <monospace>ivadomed</monospace>.
    BIDS convention is originally designed around MRI data and accepts
    NIfTI file formats, but the BIDS community is actively expanding its
    specifications to other modalities (CT, MEG/EEG, microscopy), which
    <monospace>ivadomed</monospace> will then be able to
    accommodate.</p>
    <p><bold>Access to metadata:</bold> One benefit of the BIDS
    convention is that each image file is associated with a JSON file
    containing metadata. <monospace>ivadomed</monospace>’s loader can
    parse image metadata (e.g., acquisition parameters, image contrast,
    resolution) and subject metadata (e.g., pathology, age, sex) for
    custom data splitting or extra information during training and
    evaluation. It is possible to modulate specific layers of a
    convolutional neural network using metadata information to tailor it
    towards a particular data domain or to enable experiments with
    architectures such as FiLM
    (<xref alt="Perez et al., 2017" rid="ref-perez2017film" ref-type="bibr">Perez
    et al., 2017</xref>). Metadata could also be useful to mitigate
    class imbalance via data balancing techniques.</p>
  </sec>
  <sec id="training">
    <title>Training</title>
    <p><bold>Architectures:</bold> <monospace>ivadomed</monospace>
    includes all the necessary components for training segmentation
    models from start to finish, including data augmentation
    transformations and transfer learning. Available architectures
    include: 2D U-Net
    (<xref alt="Ronneberger et al., 2015" rid="ref-Ronneberger2015unet" ref-type="bibr">Ronneberger
    et al., 2015</xref>), 3D U-Net
    (<xref alt="Isensee et al., 2017" rid="ref-isensee2017brain" ref-type="bibr">Isensee
    et al., 2017</xref>), ResNet
    (<xref alt="He et al., 2016" rid="ref-he2016resnet" ref-type="bibr">He
    et al., 2016</xref>), DenseNet
    (<xref alt="Huang et al., 2017" rid="ref-Huang2017densenet" ref-type="bibr">Huang
    et al., 2017</xref>), Count-ception
    (<xref alt="Cohen et al., 2017" rid="ref-Cohen2017countception" ref-type="bibr">Cohen
    et al., 2017</xref>), and HeMIS U-Net. These models can easily be
    enriched via attention blocks
    (<xref alt="Oktay et al., 2018" rid="ref-oktay2018attention" ref-type="bibr">Oktay
    et al., 2018</xref>) or FiLM layers (which modulate U-Net features
    using metadata).</p>
    <p><bold>Losses and class imbalance:</bold> Popular losses are
    available: Dice coefficient
    (<xref alt="Milletari et al., 2016" rid="ref-milletari2016v" ref-type="bibr">Milletari
    et al., 2016</xref>), cross-entropy, and L2 norm, including some
    adapted to medical imaging challenges, such as the adaptive wing
    loss
    (<xref alt="X. Wang et al., 2019" rid="ref-wang_adaptive_2019" ref-type="bibr">X.
    Wang et al., 2019</xref>) for soft labels and the focal Dice loss
    (<xref alt="Wong et al., 2018" rid="ref-wong20183d" ref-type="bibr">Wong
    et al., 2018</xref>) for class imbalance. Useful to alleviate
    overfitting, mixup
    (<xref alt="Zhang et al., 2017" rid="ref-zhang2017mixup" ref-type="bibr">Zhang
    et al., 2017</xref>) was modified to handle segmentation tasks. To
    mitigate class imbalance, <monospace>ivadomed</monospace> supports
    cascaded architectures. With a single inference, it is possible to
    narrow down the region of interest via object detection and then
    segment a specific structure, as illustrated in
    <xref alt="Figure 2" rid="figU003Alemay2020">Figure 2</xref>.</p>
    <p><bold>Getting started:</bold> It can be overwhelming to get
    started and choose across all the available models, losses, and
    parameters. <monospace>ivadomed</monospace>’s repository includes
    the script <monospace>ivadomed_automate_training</monospace> to
    configure and launch multiple trainings across GPUs. In case of
    interruption during training, all parameters are saved after each
    epoch so that training can be resumed at any time.</p>
  </sec>
  <sec id="evaluation">
    <title>Evaluation</title>
    <p><bold>Uncertainty:</bold> Aleatoric
    (<xref alt="G. Wang et al., 2019" rid="ref-wang_aleatoric_2019" ref-type="bibr">G.
    Wang et al., 2019</xref>) and/or epistemic
    (<xref alt="Nair et al., 2020" rid="ref-nair_exploring_2018" ref-type="bibr">Nair
    et al., 2020</xref>) uncertainties can be computed voxel-wise and/or
    object-wise
    (<xref alt="Roy et al., 2018" rid="ref-roy_quicknat_2018" ref-type="bibr">Roy
    et al., 2018</xref>). Multiple metrics are available, including
    entropy and coefficient of variation.</p>
    <p><bold>Post-processing:</bold> Predictions can be conveniently
    filtered using popular methods, e.g., fill holes, remove small
    objects, threshold using uncertainty. It is also possible to compute
    metrics for specific object sizes (e.g., small vs. large lesions).
    <monospace>ivadomed</monospace> has a module to find the optimal
    threshold value on the soft output prediction, via a grid-search
    finding applied to evaluation metrics or ROC curve.</p>
    <p><bold>Visualize performance:</bold> Convenient visualization
    tools are available for model design and optimization: GIF
    animations across training epochs, visual quality control of data
    augmentation, training curve plots, integration of the TensorBoard
    module, and output images with true/false positive labels. See
    <ext-link ext-link-type="uri" xlink:href="https://ivadomed.org/en/latest/tutorials/cascaded_architecture.html#visualize-training-data">this
    example tutorial</ext-link>.</p>
  </sec>
</sec>
<sec id="usage">
  <title>Usage</title>
  <p>Past and ongoing research projects using
  <monospace>ivadomed</monospace> are listed
  <ext-link ext-link-type="uri" xlink:href="https://ivadomed.org/en/latest/use_cases.html">here</ext-link>.
  The figure below illustrates a cascaded architecture for segmenting
  spinal tumors on MRI data
  (<xref alt="Lemay et al., 2020" rid="ref-lemay_fully_2020" ref-type="bibr">Lemay
  et al., 2020</xref>).</p>
  <fig>
    <caption><p>Fully automatic spinal cord tumor segmentation
    framework..<styled-content id="figU003Alemay2020"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="https://raw.githubusercontent.com/ivadomed/doc-figures/main/use_cases/lemay_2020.png" xlink:title="" />
  </fig>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The authors thank Ainsleigh Hill, Giselle Martel, Alexandru Jora,
  Nick Guenther, Joshua Newton, Christian Perone, Konstantinos Nasiotis,
  Valentine Louis-Lucas, Benoît Sauty-De-Chalon, Alexandru Foias and
  Leander Van Eekelen for their useful contributions, and Guillaume
  Dumas for proof-reading the manuscript. Funded by IVADO, the Canada
  Research Chair in Quantitative Magnetic Resonance Imaging [950-230815,
  950-233166], CIHR [FDN-143263], CFI [32454, 34824], FRQS [28826],
  NSERC [RGPIN-2019-07244], FRQNT [2020‐RS4‐265502 UNIQUE] and
  TransMedTech. C.G. has a fellowship from IVADO [EX-2018-4], A.L. has a
  fellowship from NSERC, FRQNT and UNIQUE, O.V. has a fellowship from
  NSERC, FRQNT and UNIQUE, M.H.B. has a fellowship from IVADO and
  FRQNT.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-wang_adaptive_2019">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Wang</surname><given-names>Xinyao</given-names></name>
          <name><surname>Bo</surname><given-names>Liefeng</given-names></name>
          <name><surname>Fuxin</surname><given-names>Li</given-names></name>
        </person-group>
        <article-title>Adaptive wing loss for robust face alignment via heatmap regression</article-title>
        <source>Proceedings of the IEEE international conference on computer vision</source>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.1109/iccv.2019.00707</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-kim_deep_2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kim</surname><given-names>Mingyu</given-names></name>
          <name><surname>Yun</surname><given-names>Jihye</given-names></name>
          <name><surname>Cho</surname><given-names>Yongwon</given-names></name>
          <name><surname>Shin</surname><given-names>Keewon</given-names></name>
          <name><surname>Jang</surname><given-names>Ryoungwoo</given-names></name>
          <name><surname>Bae</surname><given-names>Hyun-jin</given-names></name>
          <name><surname>Kim</surname><given-names>Namkug</given-names></name>
        </person-group>
        <article-title>Deep Learning in Medical Imaging</article-title>
        <source>Neurospine</source>
        <year iso-8601-date="2019-12">2019</year><month>12</month>
        <date-in-citation content-type="access-date"><year iso-8601-date="2020-09-17">2020</year><month>09</month><day>17</day></date-in-citation>
        <volume>16</volume>
        <issue>4</issue>
        <issn>2586-6583</issn>
        <uri>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6945006/</uri>
        <pub-id pub-id-type="doi">10.14245/ns.1938396.198</pub-id>
        <pub-id pub-id-type="pmid">31905454</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-bids_2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gorgolewski</surname><given-names>Krzysztof J.</given-names></name>
          <name><surname>Auer</surname><given-names>Tibor</given-names></name>
          <name><surname>Calhoun</surname><given-names>Vince D.</given-names></name>
          <name><surname>Craddock</surname><given-names>R. Cameron</given-names></name>
          <name><surname>Das</surname><given-names>Samir</given-names></name>
          <name><surname>Duff</surname><given-names>Eugene P.</given-names></name>
          <name><surname>Flandin</surname><given-names>Guillaume</given-names></name>
          <name><surname>Ghosh</surname><given-names>Satrajit S.</given-names></name>
          <name><surname>Glatard</surname><given-names>Tristan</given-names></name>
          <name><surname>Halchenko</surname><given-names>Yaroslav O.</given-names></name>
          <name><surname>Handwerker</surname><given-names>Daniel A.</given-names></name>
          <name><surname>Hanke</surname><given-names>Michael</given-names></name>
          <name><surname>Keator</surname><given-names>David</given-names></name>
          <name><surname>Li</surname><given-names>Xiangrui</given-names></name>
          <name><surname>Michael</surname><given-names>Zachary</given-names></name>
          <name><surname>Maumet</surname><given-names>Camille</given-names></name>
          <name><surname>Nichols</surname><given-names>B. Nolan</given-names></name>
          <name><surname>Nichols</surname><given-names>Thomas E.</given-names></name>
          <name><surname>Pellman</surname><given-names>John</given-names></name>
          <name><surname>Poline</surname><given-names>Jean-Baptiste</given-names></name>
          <name><surname>Rokem</surname><given-names>Ariel</given-names></name>
          <name><surname>Schaefer</surname><given-names>Gunnar</given-names></name>
          <name><surname>Sochat</surname><given-names>Vanessa</given-names></name>
          <name><surname>Triplett</surname><given-names>William</given-names></name>
          <name><surname>Turner</surname><given-names>Jessica A.</given-names></name>
          <name><surname>Varoquaux</surname><given-names>Gaël</given-names></name>
          <name><surname>Poldrack</surname><given-names>Russell A.</given-names></name>
        </person-group>
        <article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title>
        <source>Scientific Data</source>
        <year iso-8601-date="2016-06">2016</year><month>06</month>
        <date-in-citation content-type="access-date"><year iso-8601-date="2020-09-21">2020</year><month>09</month><day>21</day></date-in-citation>
        <volume>3</volume>
        <issue>1</issue>
        <issn>2052-4463</issn>
        <uri>https://www.nature.com/articles/sdata201644</uri>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-lemay_fully_2020">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Lemay</surname><given-names>Andreanne</given-names></name>
          <name><surname>Gros</surname><given-names>Charley</given-names></name>
          <name><surname>Zhuo</surname><given-names>Zhizheng</given-names></name>
          <name><surname>Zhang</surname><given-names>Jie</given-names></name>
          <name><surname>Duan</surname><given-names>Yunyun</given-names></name>
          <name><surname>Paperi</surname><given-names>Sara</given-names></name>
          <name><surname>Cohen-Adad</surname><given-names>Julien</given-names></name>
          <name><surname>Liu</surname><given-names>Yaou</given-names></name>
        </person-group>
        <article-title>Fully Automatic Spinal Cord Tumor Segmentation on MRI with Deep Learning</article-title>
        <source>Organization for human brain mapping</source>
        <publisher-loc>OHBM Annual meeting</publisher-loc>
        <year iso-8601-date="2020-07">2020</year><month>07</month>
      </element-citation>
    </ref>
    <ref id="ref-nair_exploring_2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Nair</surname><given-names>Tanya</given-names></name>
          <name><surname>Precup</surname><given-names>Doina</given-names></name>
          <name><surname>Arnold</surname><given-names>Douglas L</given-names></name>
          <name><surname>Arbel</surname><given-names>Tal</given-names></name>
        </person-group>
        <article-title>Exploring uncertainty measures in deep networks for multiple sclerosis lesion detection and segmentation</article-title>
        <source>Medical image analysis</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>59</volume>
        <pub-id pub-id-type="doi">10.1016/j.media.2019.101557</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wang_aleatoric_2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wang</surname><given-names>Guotai</given-names></name>
          <name><surname>Li</surname><given-names>Wenqi</given-names></name>
          <name><surname>Aertsen</surname><given-names>Michael</given-names></name>
          <name><surname>Deprest</surname><given-names>Jan</given-names></name>
          <name><surname>Ourselin</surname><given-names>Sébastien</given-names></name>
          <name><surname>Vercauteren</surname><given-names>Tom</given-names></name>
        </person-group>
        <article-title>Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks</article-title>
        <source>Neurocomputing</source>
        <year iso-8601-date="2019-04">2019</year><month>04</month>
        <date-in-citation content-type="access-date"><year iso-8601-date="2020-09-24">2020</year><month>09</month><day>24</day></date-in-citation>
        <volume>338</volume>
        <issn>0925-2312</issn>
        <uri>http://www.sciencedirect.com/science/article/pii/S0925231219301961</uri>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2019.01.103</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-oktay2018attention">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Oktay</surname><given-names>Ozan</given-names></name>
          <name><surname>Schlemper</surname><given-names>Jo</given-names></name>
          <name><surname>Folgoc</surname><given-names>Loic Le</given-names></name>
          <name><surname>Lee</surname><given-names>Matthew</given-names></name>
          <name><surname>Heinrich</surname><given-names>Mattias</given-names></name>
          <name><surname>Misawa</surname><given-names>Kazunari</given-names></name>
          <name><surname>Mori</surname><given-names>Kensaku</given-names></name>
          <name><surname>McDonagh</surname><given-names>Steven</given-names></name>
          <name><surname>Hammerla</surname><given-names>Nils Y</given-names></name>
          <name><surname>Kainz</surname><given-names>Bernhard</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Attention u-net: Learning where to look for the pancreas</article-title>
        <source>arXiv preprint arXiv:1804.03999</source>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-zhang2017mixup">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Zhang</surname><given-names>Hongyi</given-names></name>
          <name><surname>Cisse</surname><given-names>Moustapha</given-names></name>
          <name><surname>Dauphin</surname><given-names>Yann N</given-names></name>
          <name><surname>Lopez-Paz</surname><given-names>David</given-names></name>
        </person-group>
        <article-title>Mixup: Beyond empirical risk minimization</article-title>
        <source>arXiv preprint arXiv:1710.09412</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-perez2017film">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Perez</surname><given-names>Ethan</given-names></name>
          <name><surname>Strub</surname><given-names>Florian</given-names></name>
          <name><surname>De Vries</surname><given-names>Harm</given-names></name>
          <name><surname>Dumoulin</surname><given-names>Vincent</given-names></name>
          <name><surname>Courville</surname><given-names>Aaron</given-names></name>
        </person-group>
        <article-title>Film: Visual reasoning with a general conditioning layer</article-title>
        <source>arXiv preprint arXiv:1709.07871</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-havaei2016hemis">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Havaei</surname><given-names>Mohammad</given-names></name>
          <name><surname>Guizard</surname><given-names>Nicolas</given-names></name>
          <name><surname>Chapados</surname><given-names>Nicolas</given-names></name>
          <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        </person-group>
        <article-title>HeMIS: Hetero-modal image segmentation</article-title>
        <source>arXiv preprint arXiv:1607.05194</source>
        <year iso-8601-date="2016">2016</year>
        <pub-id pub-id-type="doi">10.1007/978-3-319-46723-8_54</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Cohen2017countception">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Cohen</surname><given-names>Joseph Paul</given-names></name>
          <name><surname>Boucher</surname><given-names>Genevieve</given-names></name>
          <name><surname>Glastonbury</surname><given-names>Craig A.</given-names></name>
          <name><surname>Lo</surname><given-names>Henry Z.</given-names></name>
          <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        </person-group>
        <article-title>Count-ception: Counting by Fully Convolutional Redundant Counting</article-title>
        <source>International conference on computer vision workshop on BioImage computing</source>
        <year iso-8601-date="2017">2017</year>
        <uri>http://arxiv.org/abs/1703.08710</uri>
        <pub-id pub-id-type="doi">10.1109/iccvw.2017.9</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Huang2017densenet">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Huang</surname><given-names>Gao</given-names></name>
          <name><surname>Liu</surname><given-names>Zhuang</given-names></name>
          <name><surname>Maaten</surname><given-names>Laurens van der</given-names></name>
          <name><surname>Weinberger</surname><given-names>Kilian Q.</given-names></name>
        </person-group>
        <article-title>Densely Connected Convolutional Networks</article-title>
        <source>Computer vision and pattern recognition</source>
        <year iso-8601-date="2017">2017</year>
        <uri>https://arxiv.org/abs/1608.06993</uri>
      </element-citation>
    </ref>
    <ref id="ref-Ronneberger2015unet">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Ronneberger</surname><given-names>Olaf</given-names></name>
          <name><surname>Fischer</surname><given-names>Philipp</given-names></name>
          <name><surname>Brox</surname><given-names>Thomas</given-names></name>
        </person-group>
        <article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title>
        <source>Medical image computing and computer assisted intervention</source>
        <year iso-8601-date="2015-05">2015</year><month>05</month>
        <uri>https://arxiv.org/abs/1505.04597</uri>
      </element-citation>
    </ref>
    <ref id="ref-he2016resnet">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>He</surname><given-names>Kaiming</given-names></name>
          <name><surname>Zhang</surname><given-names>Xiangyu</given-names></name>
          <name><surname>Ren</surname><given-names>Shaoqing</given-names></name>
          <name><surname>Sun</surname><given-names>Jian</given-names></name>
        </person-group>
        <article-title>Deep Residual Learning for Image Recognition</article-title>
        <source>Conference on computer vision and pattern recognition</source>
        <year iso-8601-date="2016">2016</year>
        <isbn>978-1-4673-8851-1</isbn>
        <issn>1664-1078</issn>
        <uri>http://ieeexplore.ieee.org/document/7780459/</uri>
        <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>
        <pub-id pub-id-type="pmid">23554596</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-roy_quicknat_2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Roy</surname><given-names>Abhijit Guha</given-names></name>
          <name><surname>Conjeti</surname><given-names>Sailesh</given-names></name>
          <name><surname>Navab</surname><given-names>Nassir</given-names></name>
          <name><surname>Wachinger</surname><given-names>Christian</given-names></name>
        </person-group>
        <article-title>QuickNAT: A Fully Convolutional Network for Quick and Accurate Segmentation of Neuroanatomy</article-title>
        <source>arXiv:1801.04161 [cs]</source>
        <year iso-8601-date="2018-11">2018</year><month>11</month>
        <date-in-citation content-type="access-date"><year iso-8601-date="2020-09-28">2020</year><month>09</month><day>28</day></date-in-citation>
        <uri>http://arxiv.org/abs/1801.04161</uri>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.11.042</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-isensee2017brain">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Isensee</surname><given-names>Fabian</given-names></name>
          <name><surname>Kickingereder</surname><given-names>Philipp</given-names></name>
          <name><surname>Wick</surname><given-names>Wolfgang</given-names></name>
          <name><surname>Bendszus</surname><given-names>Martin</given-names></name>
          <name><surname>Maier-Hein</surname><given-names>Klaus H</given-names></name>
        </person-group>
        <article-title>Brain tumor segmentation and radiomics survival prediction: Contribution to the BRATS 2017 challenge</article-title>
        <source>International MICCAI brainlesion workshop</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-milletari2016v">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Milletari</surname><given-names>Fausto</given-names></name>
          <name><surname>Navab</surname><given-names>Nassir</given-names></name>
          <name><surname>Ahmadi</surname><given-names>Seyed-Ahmad</given-names></name>
        </person-group>
        <article-title>V-net: Fully convolutional neural networks for volumetric medical image segmentation</article-title>
        <source>2016 fourth international conference on 3D vision (3DV)</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <pub-id pub-id-type="doi">10.1109/3dv.2016.79</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-wong20183d">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Wong</surname><given-names>Ken CL</given-names></name>
          <name><surname>Moradi</surname><given-names>Mehdi</given-names></name>
          <name><surname>Tang</surname><given-names>Hui</given-names></name>
          <name><surname>Syeda-Mahmood</surname><given-names>Tanveer</given-names></name>
        </person-group>
        <article-title>3D segmentation with exponential logarithmic loss for highly unbalanced object sizes</article-title>
        <source>International conference on medical image computing and computer-assisted intervention</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <pub-id pub-id-type="doi">10.1007/978-3-030-00931-1_70</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Willemink2020-au">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Willemink</surname><given-names>Martin J</given-names></name>
          <name><surname>Koszek</surname><given-names>Wojciech A</given-names></name>
          <name><surname>Hardell</surname><given-names>Cailin</given-names></name>
          <name><surname>Wu</surname><given-names>Jie</given-names></name>
          <name><surname>Fleischmann</surname><given-names>Dominik</given-names></name>
          <name><surname>Harvey</surname><given-names>Hugh</given-names></name>
          <name><surname>Folio</surname><given-names>Les R</given-names></name>
          <name><surname>Summers</surname><given-names>Ronald M</given-names></name>
          <name><surname>Rubin</surname><given-names>Daniel L</given-names></name>
          <name><surname>Lungren</surname><given-names>Matthew P</given-names></name>
        </person-group>
        <article-title>Preparing medical imaging data for machine learning</article-title>
        <source>Radiology</source>
        <year iso-8601-date="2020-04">2020</year><month>04</month>
        <volume>295</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
