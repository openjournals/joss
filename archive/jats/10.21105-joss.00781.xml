<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">781</article-id>
<article-id pub-id-type="doi">10.21105/joss.00781</article-id>
<title-group>
<article-title>approxposterior: Approximate Posterior Distributions in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-9293-4043</contrib-id>
<string-name>David P. Fleming</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9623-3401</contrib-id>
<string-name>Jake VanderPlas</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Washington</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2018-08-01">
<day>1</day>
<month>8</month>
<year>2018</year>
</pub-date>
<volume>3</volume>
<issue>29</issue>
<fpage>781</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>This package is a Python implementation of “Bayesian Active
  Learning for Posterior Estimation” by
  (<xref alt="Kandasamy et al., 2015" rid="ref-Kandasamy2015" ref-type="bibr">Kandasamy
  et al., 2015</xref>) and “Adaptive Gaussian process approximation for
  Bayesian inference with expensive likelihood functions”
  (<xref alt="Wang &amp; Li, 2017" rid="ref-Wang2017" ref-type="bibr">Wang
  &amp; Li, 2017</xref>). These algorithms allows the user to compute
  approximate posterior probability distributions using computationally
  expensive forward models by training a Gaussian Process (GP) surrogate
  for the likelihood evaluation. The algorithms leverage the inherent
  uncertainty in the GP’s predictions to identify high-likelihood
  regions in parameter space where the GP is uncertain. The algorithms
  then run the forward model at these points to compute their likelihood
  and re-trains the GP to maximize the GP’s predictive ability while
  minimizing the number of forward model evaluations. Please read
  (<xref alt="Kandasamy et al., 2015" rid="ref-Kandasamy2015" ref-type="bibr">Kandasamy
  et al., 2015</xref>) and
  (<xref alt="Wang &amp; Li, 2017" rid="ref-Wang2017" ref-type="bibr">Wang
  &amp; Li, 2017</xref>) for in-depth descriptions of the respective
  algorithms. <italic>approxposterior</italic> is under active
  development on GitHub and community participation is encouraged. The
  code is available on GitHub
  (<xref alt="Fleming, 2018" rid="ref-approxposterior_github" ref-type="bibr">Fleming,
  2018</xref>).</p>
  <p>The following figure is a simple demonstration of
  <italic>approxposterior</italic> produced using a Jupyter Notebook
  provided with the code on GitHub. In the left panel, we show the true
  posterior probability distribution computed by Markov Chain Monte
  Carlo (MCMC) compared against the result of
  <italic>approxposterior</italic>. The two distributions are in
  excellent agreement. In the right panel, we estimate how the
  performance of <italic>approxposterior</italic> compares against MCMC
  by tracking the number of forward model evaluations required for both
  methods to converge using the fiducial parameters given in
  (<xref alt="Wang &amp; Li, 2017" rid="ref-Wang2017" ref-type="bibr">Wang
  &amp; Li, 2017</xref>) and by tracking the computational time required
  for all parts of the <italic>approxposterior</italic> algorithm, such
  as training the GP. Since the
  (<xref alt="Wang &amp; Li, 2017" rid="ref-Wang2017" ref-type="bibr">Wang
  &amp; Li, 2017</xref>) forward model is analytic and hence requires
  little computational effort, we estimate computational times by
  adopting a range of forward model runtimes, from 10 microseconds to
  10,000 seconds. For MCMC, the
  (<xref alt="Wang &amp; Li, 2017" rid="ref-Wang2017" ref-type="bibr">Wang
  &amp; Li, 2017</xref>) example requires 400,000 iterations to
  converge, so we can estimate how long MCMC would take to finish for a
  given forward model runtime by multiplying the number of iterations by
  the runtime since the forward model is evaluated each iteration. For
  <italic>approxposterior</italic>, we adopt a similar procedure, but
  also add the forward model runtimes for building the GP training set,
  the time required to train the GP, and the time required to derive the
  approximate posterior distribution.</p>
  <p>In terms of <italic>approxposterior</italic> performance, we see
  two regimes: for very quick forward models, the GP training time
  dominates performance as the GP predictions take little time, about 30
  microseconds. For slow forward models, the runtime is dominated by
  evaluating the forward model to generate the training set for the GP.
  In this regime, <italic>approxposterior</italic> is several orders of
  magnitude faster than MCMC.</p>
  <fig>
    <caption><p>Left: Joint posterior probability distribution of the
    two model parameters from the Wang and Li (2017) example. The black
    density map denotes the true distribution while the red contours
    denote the approximate distribution derived using
    <italic>approxposterior</italic>. The two distributions are in
    excellent agreement. Right: Total computational time required to
    compute the posterior probability distribution of the model
    parameters from the Wang and Li (2017) example as a function of
    forward model evaluation time. The MCMC method (blue) runs the
    forward model for each MCMC iteration, while the orange curve was
    derived using the <italic>approxposterior</italic> BAPE
    implementation.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="acc_scal.png" xlink:title="" />
  </fig>
  <p>Left: Joint posterior probability distribution of the two model
  parameters from the
  (<xref alt="Wang &amp; Li, 2017" rid="ref-Wang2017" ref-type="bibr">Wang
  &amp; Li, 2017</xref>) example. The black density map denotes the true
  distribution while the red contours denote the approximate
  distribution derived using <italic>approxposterior</italic>. The two
  distributions are in excellent agreement. Right: Total computational
  time required to compute the posterior probability distribution of the
  model parameters from the
  (<xref alt="Wang &amp; Li, 2017" rid="ref-Wang2017" ref-type="bibr">Wang
  &amp; Li, 2017</xref>) example as a function of forward model
  evaluation time. The MCMC method (blue) runs the forward model for
  each MCMC iteration, while the orange curve was derived using the
  <italic>approxposterior</italic> BAPE implementation.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>DPF was supported by NASA Headquarters under the NASA Earth and
  Space Science Fellowship Program - Grant 80NSSC17K0482. This work is
  supported in part by an NSF IGERT grant DGE-1258485.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-Kandasamy2015">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kandasamy</surname><given-names>Kirthevasan</given-names></name>
          <name><surname>Schneider</surname><given-names>Jeff</given-names></name>
          <name><surname>Poczos</surname><given-names>Barnabas</given-names></name>
        </person-group>
        <article-title>Bayesian active learning for posterior estimation</article-title>
        <source>International Joint Conference on Artificial Intelligence</source>
        <year iso-8601-date="2015">2015</year>
      </element-citation>
    </ref>
    <ref id="ref-Wang2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Wang</surname><given-names>H.</given-names></name>
          <name><surname>Li</surname><given-names>J.</given-names></name>
        </person-group>
        <article-title>Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions</article-title>
        <source>ArXiv e-prints</source>
        <year iso-8601-date="2017-03">2017</year><month>03</month>
        <uri>https://arxiv.org/abs/1703.09930</uri>
      </element-citation>
    </ref>
    <ref id="ref-approxposterior_github">
      <element-citation publication-type="webpage">
        <person-group person-group-type="author">
          <name><surname>Fleming</surname><given-names>David P.</given-names></name>
        </person-group>
        <article-title>Approxposterior on GitHub</article-title>
        <year iso-8601-date="2018">2018</year>
        <date-in-citation content-type="access-date"><year iso-8601-date="2018-04-24">2018</year><month>04</month><day>24</day></date-in-citation>
        <uri>https://github.com/dflemin3/approxposterior</uri>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
