<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">2505</article-id>
<article-id pub-id-type="doi">10.21105/joss.02505</article-id>
<title-group>
<article-title>sbi: A toolkit for simulation-based
inference</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-8768-4227</contrib-id>
<string-name>Alvaro Tejero-Cantero</string-name>
<xref ref-type="aff" rid="aff-e"/>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4979-7092</contrib-id>
<string-name>Jan Boelts</string-name>
<xref ref-type="aff" rid="aff-e"/>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-3573-0404</contrib-id>
<string-name>Michael Deistler</string-name>
<xref ref-type="aff" rid="aff-e"/>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4320-4663</contrib-id>
<string-name>Jan-Matthis Lueckmann</string-name>
<xref ref-type="aff" rid="aff-e"/>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-9333-7777</contrib-id>
<string-name>Conor Durkan</string-name>
<xref ref-type="aff" rid="aff-e"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6987-4836</contrib-id>
<string-name>Pedro J. Gonçalves</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-8515-0459</contrib-id>
<string-name>David S. Greenberg</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-5154-8912</contrib-id>
<string-name>Jakob H. Macke</string-name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-5"/>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<aff id="aff-e">
<institution-wrap>
<institution>Equally contributing authors</institution>
</institution-wrap>
</aff>
<aff id="aff-1">
<institution-wrap>
<institution>Computational Neuroengineering, Department of Electrical
and Computer Engineering, Technical University of Munich</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>School of Informatics, University of
Edinburgh</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Neural Systems Analysis, Center of Advanced European
Studies and Research (caesar), Bonn</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Model-Driven Machine Learning, Centre for Materials and
Coastal Research, Helmholtz-Zentrum Geesthacht</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Machine Learning in Science, University of
Tübingen</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Empirical Inference, Max Planck Institute for Intelligent
Systems, Tübingen</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>5</volume>
<issue>52</issue>
<fpage>2505</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>simulation science</kwd>
<kwd>likelihood-free inference</kwd>
<kwd>bayesian inference</kwd>
<kwd>system identification</kwd>
<kwd>parameter identification</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Scientists and engineers employ stochastic numerical simulators to
  model empirically observed phenomena. In contrast to purely
  statistical models, simulators express scientific principles that
  provide powerful inductive biases, improve generalization to new data
  or scenarios and allow for fewer, more interpretable and
  domain-relevant parameters. Despite these advantages, tuning a
  simulator’s parameters so that its outputs match data is challenging.
  Simulation-based inference (SBI) seeks to identify parameter sets that
  a) are compatible with prior knowledge and b) match empirical
  observations. Importantly, SBI does not seek to recover a single
  ‘best’ data-compatible parameter set, but rather to identify all high
  probability regions of parameter space that explain observed data, and
  thereby to quantify parameter uncertainty. In Bayesian terminology,
  SBI aims to retrieve the posterior distribution over the parameters of
  interest. In contrast to conventional Bayesian inference, SBI is also
  applicable when one can run model simulations, but no formula or
  algorithm exists for evaluating the probability of data given
  parameters, i.e. the likelihood.</p>
  <p>We present <monospace>sbi</monospace>, a PyTorch-based package that
  implements SBI algorithms based on neural networks.
  <monospace>sbi</monospace> facilitates inference on black-box
  simulators for practising scientists and engineers by providing a
  unified interface to state-of-the-art algorithms together with
  documentation and tutorials.</p>
</sec>
<sec id="motivation">
  <title>Motivation</title>
  <p>Bayesian inference is a principled approach for determining
  parameters consistent with empirical observations: Given a prior over
  parameters, a stochastic simulator, and observations, it returns a
  posterior distribution. In cases where the simulator likelihood
  <italic>can</italic> be evaluated, many methods for approximate
  Bayesian inference exist
  (<xref alt="Baydin et al., 2019" rid="ref-baydin2020" ref-type="bibr">Baydin
  et al., 2019</xref>;
  <xref alt="Graham &amp; Storkey, 2017" rid="ref-graham2017" ref-type="bibr">Graham
  &amp; Storkey, 2017</xref>;
  <xref alt="Le et al., 2017" rid="ref-le2016" ref-type="bibr">Le et
  al., 2017</xref>; e.g.,
  <xref alt="Metropolis et al., 1953" rid="ref-metropolis1953" ref-type="bibr">Metropolis
  et al., 1953</xref>;
  <xref alt="Neal, 2003" rid="ref-neal2003" ref-type="bibr">Neal,
  2003</xref>). For more general simulators, however, evaluating the
  likelihood of data given parameters might be computationally
  intractable. Traditional algorithms for this ‘likelihood-free’ setting
  (<xref alt="Cranmer et al., 2020" rid="ref-cranmer2019" ref-type="bibr">Cranmer
  et al., 2020</xref>) are based on Monte-Carlo rejection
  (<xref alt="Pritchard et al., 1999" rid="ref-pritchard1999" ref-type="bibr">Pritchard
  et al., 1999</xref>;
  <xref alt="Sisson et al., 2007" rid="ref-sisson2007" ref-type="bibr">Sisson
  et al., 2007</xref>), an approach known as <italic>Approximate
  Bayesian Computation</italic> (ABC). More recently, algorithms based
  on neural networks have been developed
  (<xref alt="Greenberg et al., 2019" rid="ref-greenberg2019" ref-type="bibr">Greenberg
  et al., 2019</xref>;
  <xref alt="Hermans et al., 2020" rid="ref-hermans2019" ref-type="bibr">Hermans
  et al., 2020</xref>;
  <xref alt="Lueckmann et al., 2017" rid="ref-lueckmann2017" ref-type="bibr">Lueckmann
  et al., 2017</xref>;
  <xref alt="Papamakarios et al., 2019" rid="ref-papamakarios2019a" ref-type="bibr">Papamakarios
  et al., 2019</xref>;
  <xref alt="Papamakarios &amp; Murray, 2016" rid="ref-papamakarios2016" ref-type="bibr">Papamakarios
  &amp; Murray, 2016</xref>). These algorithms are not based on
  rejecting simulations, but rather train deep neural conditional
  density estimators or classifiers on simulated data. To aid in
  effective application of these algorithms to a wide range of problems,
  <monospace>sbi</monospace> closely integrates with PyTorch and offers
  state-of-the-art neural network-based SBI algorithms
  (<xref alt="Greenberg et al., 2019" rid="ref-greenberg2019" ref-type="bibr">Greenberg
  et al., 2019</xref>;
  <xref alt="Hermans et al., 2020" rid="ref-hermans2019" ref-type="bibr">Hermans
  et al., 2020</xref>;
  <xref alt="Papamakarios et al., 2019" rid="ref-papamakarios2019a" ref-type="bibr">Papamakarios
  et al., 2019</xref>) with flexible choice of network architectures and
  flow-based density estimators. With <monospace>sbi</monospace>,
  researchers can easily implement new neural inference algorithms,
  benefiting from the infrastructure to manage simulators and a unified
  posterior representation. Users, in turn, can profit from a single
  inference interface that allows them to either use their own custom
  neural network, or choose from a growing library of preconfigured
  options provided with the package.</p>
  <sec id="related-software-and-use-in-research">
    <title>Related software and use in research</title>
    <p>We are aware of several mature packages that implement SBI
    algorithms. <monospace>elfi</monospace>
    (<xref alt="Lintusaari et al., 2018" rid="ref-elfi2018" ref-type="bibr">Lintusaari
    et al., 2018</xref>) is a package offering BOLFI, a Gaussian
    process-based algorithm
    (<xref alt="Gutmann &amp; Corander, 2016" rid="ref-gutmann2015" ref-type="bibr">Gutmann
    &amp; Corander, 2016</xref>), and some classical ABC algorithms. The
    package <monospace>carl</monospace>
    (<xref alt="Louppe et al., 2016" rid="ref-louppe2016" ref-type="bibr">Louppe
    et al., 2016</xref>) implements the algorithm described in Cranmer
    et al.
    (<xref alt="2015" rid="ref-cranmer2015carl" ref-type="bibr">2015</xref>).
    Two other SBI packages, currently under development, are
    <monospace>hypothesis</monospace>
    (<xref alt="Hermans, 2019" rid="ref-hypothesis-repo" ref-type="bibr">Hermans,
    2019</xref>) and <monospace>pydelfi</monospace>
    (<xref alt="Alsing, 2019" rid="ref-pydelfi-repo" ref-type="bibr">Alsing,
    2019</xref>). <monospace>pyabc</monospace>
    (<xref alt="Klinger et al., 2018" rid="ref-Klinger2018" ref-type="bibr">Klinger
    et al., 2018</xref>) and <monospace>ABCpy</monospace>
    (<xref alt="Dutta et al., 2017" rid="ref-dutta2017" ref-type="bibr">Dutta
    et al., 2017</xref>) are two packages offering a diversity of ABC
    algorithms.</p>
    <p><monospace>sbi</monospace> is closely integrated with PyTorch
    (<xref alt="Paszke et al., 2019" rid="ref-paszke2019" ref-type="bibr">Paszke
    et al., 2019</xref>) and uses <monospace>nflows</monospace>
    (<xref alt="Durkan et al., 2019" rid="ref-nflows-repo" ref-type="bibr">Durkan
    et al., 2019</xref>) for flow-based density estimators.
    <monospace>sbi</monospace> builds on experience accumulated
    developing <monospace>delfi</monospace>
    (<xref alt="mackelab.org, 2017" rid="ref-delfi-repo" ref-type="bibr">mackelab.org,
    2017</xref>), which it succeeds. <monospace>delfi</monospace> was
    based on <monospace>theano</monospace>
    (<xref alt="Al-Rfou et al., 2016" rid="ref-theano" ref-type="bibr">Al-Rfou
    et al., 2016</xref>) (development discontinued) and developed both
    for SBI research
    (<xref alt="Greenberg et al., 2019" rid="ref-greenberg2019" ref-type="bibr">Greenberg
    et al., 2019</xref>;
    <xref alt="Lueckmann et al., 2017" rid="ref-lueckmann2017" ref-type="bibr">Lueckmann
    et al., 2017</xref>) and for scientific applications
    (<xref alt="Gonçalves et al., 2019" rid="ref-goncalves2019" ref-type="bibr">Gonçalves
    et al., 2019</xref>). The <monospace>sbi</monospace> codebase
    started as a fork of <monospace>lfi</monospace>
    (<xref alt="Durkan, 2020" rid="ref-lfi-repo" ref-type="bibr">Durkan,
    2020</xref>), developed for Durkan et al.
    (<xref alt="2020" rid="ref-durkan2020" ref-type="bibr">2020</xref>).</p>
  </sec>
</sec>
<sec id="description">
  <title>Description</title>
  <p><monospace>sbi</monospace> currently implements three families of
  neural inference algorithms:</p>
  <list list-type="bullet">
    <list-item>
      <p>Sequential Neural <italic>Posterior</italic> Estimation (SNPE)
      trains a deep neural density estimator that directly estimates the
      posterior distribution of parameters given data. Afterwards, it
      can sample parameter sets from the posterior, or evaluate the
      posterior density on any parameter set. Currently, SNPE-C
      (<xref alt="Greenberg et al., 2019" rid="ref-greenberg2019" ref-type="bibr">Greenberg
      et al., 2019</xref>) is implemented in
      <monospace>sbi</monospace>.</p>
    </list-item>
    <list-item>
      <p>Sequential Neural <italic>Likelihood</italic> Estimation (SNLE)
      (<xref alt="Papamakarios et al., 2019" rid="ref-papamakarios2019a" ref-type="bibr">Papamakarios
      et al., 2019</xref>) trains a deep neural density estimator of the
      likelihood, which then allows to sample from the posterior using
      e.g. MCMC.</p>
    </list-item>
    <list-item>
      <p>Sequential Neural <italic>Ratio</italic> Estimation (SNRE)
      (<xref alt="Durkan et al., 2020" rid="ref-durkan2020" ref-type="bibr">Durkan
      et al., 2020</xref>;
      <xref alt="Hermans et al., 2020" rid="ref-hermans2019" ref-type="bibr">Hermans
      et al., 2020</xref>) trains a classifier to estimate density
      ratios, which in turn can be used to sample from the posterior
      e.g. with MCMC.</p>
    </list-item>
  </list>
  <p>The inference step returns a <monospace>NeuralPosterior</monospace>
  object that represents the uncertainty about the parameters
  conditional on an observation, i.e. the posterior distribution. This
  object can be sampled from —and if the chosen algorithm allows,
  evaluated— with the same API as a standard PyTorch probability
  distribution.</p>
  <p>An important challenge in making SBI algorithms usable by a broader
  community is to deal with diverse, often pre-existing, complex
  simulators. <monospace>sbi</monospace> works with any simulator as
  long as it can be wrapped in a Python callable. Furthermore,
  <monospace>sbi</monospace> ensures that custom simulators work well
  with neural networks, e.g. by performing automatic shape inference,
  standardizing inputs or handling failed simulations. To maximize
  simulator performance, <monospace>sbi</monospace> leverages
  vectorization where available and optionally parallelizes simulations
  using <monospace>joblib</monospace>
  (<xref alt="Varoquaux, 2008" rid="ref-joblib" ref-type="bibr">Varoquaux,
  2008</xref>). Moreover, if dimensionality reduction of the simulator
  output is desired, <monospace>sbi</monospace> can use a trainable
  summarizing network to extract relevant features from raw simulator
  output and spare the user manual feature engineering.</p>
  <p>In addition to the full-featured interface,
  <monospace>sbi</monospace> provides also a <italic>simple</italic>
  interface which consists of a single function call with reasonable
  defaults. This allows new users to get familiarized with
  simulation-based inference and quickly obtain results without having
  to define custom networks or tune hyperparameters.</p>
  <p>With <monospace>sbi</monospace>, we aim to support scientific
  discovery and computational engineering by making Bayesian inference
  applicable to the widest class of models (simulators with no
  likelihood available), and practical for complex problems. We have
  designed an open architecture and adopted community-oriented
  development practices in order to invite other machine-learning
  researchers to join us in this long-term vision.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work has been supported by the German Federal Ministry of
  Education and Research (BMBF, project `ADIMEM’, FKZ 01IS18052 A-D),
  the German Research Foundation (DFG) through SFB 1089 `Synaptic
  Microcircuits’, SPP 2041 `Computational Connectomics’ and Germany’s
  Excellence Strategy – EXC-Number 2064/1 – Project number
  390727645.</p>
  <p>Conor Durkan was supported by the EPSRC Centre for Doctoral
  Training in Data Science, funded by the UK Engineering and Physical
  Sciences Research Council (grant EP/L016427/1) and the University of
  Edinburgh.</p>
  <p>We are grateful to Artur Bekasov, George Papamakarios and Iain
  Murray for making <monospace>nflows</monospace>
  (<xref alt="Durkan et al., 2019" rid="ref-nflows-repo" ref-type="bibr">Durkan
  et al., 2019</xref>) available, a package for normalizing flow-based
  density estimation which <monospace>sbi</monospace> leverages
  extensively.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-dutta2017">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Dutta</surname><given-names>Ritabrata</given-names></name>
          <name><surname>Schoengens</surname><given-names>Marcel</given-names></name>
          <name><surname>Onnela</surname><given-names>Jukka-Pekka</given-names></name>
          <name><surname>Mira</surname><given-names>Antonietta</given-names></name>
        </person-group>
        <article-title>ABCpy: A user-friendly, extensible, and parallel library for approximate bayesian computation</article-title>
        <source>Proceedings of the platform for advanced scientific computing conference</source>
        <year iso-8601-date="2017">2017</year>
        <uri>http://doi.acm.org/10.1145/3093172.3093233</uri>
        <pub-id pub-id-type="doi">10.1145/3093172.3093233</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-baydin2020">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Baydin</surname><given-names>Atilim Güneş</given-names></name>
          <name><surname>Shao</surname><given-names>Lei</given-names></name>
          <name><surname>Bhimji</surname><given-names>Wahid</given-names></name>
          <name><surname>Heinrich</surname><given-names>Lukas</given-names></name>
          <name><surname>Meadows</surname><given-names>Lawrence</given-names></name>
          <name><surname>Liu</surname><given-names>Jialin</given-names></name>
          <name><surname>Munk</surname><given-names>Andreas</given-names></name>
          <name><surname>Naderiparizi</surname><given-names>Saeid</given-names></name>
          <name><surname>Gram-Hansen</surname><given-names>Bradley</given-names></name>
          <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Etalumis: Bringing probabilistic programming to scientific simulators at scale</article-title>
        <source>Proceedings of the international conference for high performance computing, networking, storage and analysis</source>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.1145/3295500.3356180</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-cranmer2015carl">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Cranmer</surname><given-names>Kyle</given-names></name>
          <name><surname>Pavez</surname><given-names>Juan</given-names></name>
          <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
        </person-group>
        <article-title>Approximating likelihood ratios with calibrated discriminative classifiers</article-title>
        <source>arXiv preprint arXiv:1506.02169</source>
        <year iso-8601-date="2015">2015</year>
      </element-citation>
    </ref>
    <ref id="ref-cranmer2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Cranmer</surname><given-names>Kyle</given-names></name>
          <name><surname>Brehmer</surname><given-names>Johann</given-names></name>
          <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
        </person-group>
        <article-title>The frontier of simulation-based inference</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year iso-8601-date="2020">2020</year>
        <pub-id pub-id-type="doi">10.1073/pnas.1912789117</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-delfi-repo">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>mackelab.org</surname></name>
        </person-group>
        <article-title>DELFI: Density estimation likelihood-free inference</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/mackelab/delfi; GitHub</publisher-name>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-durkan2020">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Durkan</surname><given-names>Conor</given-names></name>
          <name><surname>Murray</surname><given-names>Iain</given-names></name>
          <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        </person-group>
        <article-title>On contrastive learning for likelihood-free inference</article-title>
        <source>Proceedings of the 36th International Conference on Machine Learning</source>
        <publisher-name>PMLR</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>98</volume>
      </element-citation>
    </ref>
    <ref id="ref-elfi2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Lintusaari</surname><given-names>Jarno</given-names></name>
          <name><surname>Vuollekoski</surname><given-names>Henri</given-names></name>
          <name><surname>Kangasrääsiö</surname><given-names>Antti</given-names></name>
          <name><surname>Skytén</surname><given-names>Kusti</given-names></name>
          <name><surname>Järvenpää</surname><given-names>Marko</given-names></name>
          <name><surname>Marttinen</surname><given-names>Pekka</given-names></name>
          <name><surname>Gutmann</surname><given-names>Michael U</given-names></name>
          <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
          <name><surname>Corander</surname><given-names>Jukka</given-names></name>
          <name><surname>Kaski</surname><given-names>Samuel</given-names></name>
        </person-group>
        <article-title>ELFI: Engine for likelihood-free inference</article-title>
        <source>The Journal of Machine Learning Research</source>
        <publisher-name>JMLR</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>19</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
    <ref id="ref-goncalves2019">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gonçalves</surname><given-names>Pedro J</given-names></name>
          <name><surname>Lueckmann</surname><given-names>Jan-Matthis</given-names></name>
          <name><surname>Deistler</surname><given-names>Michael</given-names></name>
          <name><surname>Nonnenmacher</surname><given-names>Marcel</given-names></name>
          <name><surname>Öcal</surname><given-names>Kaan</given-names></name>
          <name><surname>Bassetto</surname><given-names>Giacomo</given-names></name>
          <name><surname>Chintaluri</surname><given-names>Chaitanya</given-names></name>
          <name><surname>Podlaski</surname><given-names>William F</given-names></name>
          <name><surname>Haddad</surname><given-names>Sara A</given-names></name>
          <name><surname>Vogels</surname><given-names>Tim P</given-names></name>
          <name><surname>Greenberg</surname><given-names>David S.</given-names></name>
          <name><surname>Macke</surname><given-names>Jakob H.</given-names></name>
        </person-group>
        <article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title>
        <source>bioRxiv</source>
        <publisher-name>Cold Spring Harbor Laboratory</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <pub-id pub-id-type="doi">10.1101/838383 </pub-id>
      </element-citation>
    </ref>
    <ref id="ref-graham2017">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Graham</surname><given-names>Matthew M.</given-names></name>
          <name><surname>Storkey</surname><given-names>Amos J.</given-names></name>
        </person-group>
        <article-title>Asymptotically exact inference in differentiable generative models</article-title>
        <source>Electronic Journal of Statistics</source>
        <publisher-name>The Institute of Mathematical Statistics; the Bernoulli Society</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>11</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.1214/17-EJS1340SI</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-greenberg2019">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Greenberg</surname><given-names>David</given-names></name>
          <name><surname>Nonnenmacher</surname><given-names>Marcel</given-names></name>
          <name><surname>Macke</surname><given-names>Jakob</given-names></name>
        </person-group>
        <article-title>Automatic posterior transformation for likelihood-free inference</article-title>
        <source>Proceedings of the 36th international conference on machine learning</source>
        <publisher-name>PMLR</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>97</volume>
      </element-citation>
    </ref>
    <ref id="ref-gutmann2015">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Gutmann</surname><given-names>Michael U</given-names></name>
          <name><surname>Corander</surname><given-names>Jukka</given-names></name>
        </person-group>
        <article-title>Bayesian optimization for likelihood-free inference of simulator-based statistical models</article-title>
        <source>The Journal of Machine Learning Research</source>
        <year iso-8601-date="2016">2016</year>
        <volume>17</volume>
        <issue>1</issue>
      </element-citation>
    </ref>
    <ref id="ref-hermans2019">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Hermans</surname><given-names>Joeri</given-names></name>
          <name><surname>Begy</surname><given-names>Volodimir</given-names></name>
          <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
        </person-group>
        <article-title>Likelihood-free MCMC with approximate likelihood ratios</article-title>
        <source>Proceedings of the 37th international conference on machine learning</source>
        <publisher-name>PMLR</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>98</volume>
      </element-citation>
    </ref>
    <ref id="ref-hypothesis-repo">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Hermans</surname><given-names>Joeri</given-names></name>
        </person-group>
        <article-title>Hypothesis</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/montefiore-ai/hypothesis; GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-joblib">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Varoquaux</surname><given-names>Gael</given-names></name>
        </person-group>
        <article-title>Joblib</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/joblib/joblib; GitHub</publisher-name>
        <year iso-8601-date="2008">2008</year>
      </element-citation>
    </ref>
    <ref id="ref-Klinger2018">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Klinger</surname><given-names>Emmanuel</given-names></name>
          <name><surname>Rickert</surname><given-names>Dennis</given-names></name>
          <name><surname>Hasenauer</surname><given-names>Jan</given-names></name>
        </person-group>
        <article-title>pyABC: Distributed, likelihood-free inference</article-title>
        <source>Bioinformatics</source>
        <publisher-name>Oxford University Press</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>34</volume>
        <issue>20</issue>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty361</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-le2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Le</surname><given-names>Tuan Anh</given-names></name>
          <name><surname>Baydin</surname><given-names>Atilim Gunes</given-names></name>
          <name><surname>Wood</surname><given-names>Frank</given-names></name>
        </person-group>
        <article-title>Inference compilation and universal probabilistic programming</article-title>
        <source>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017</source>
        <publisher-name>JMLR</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>54</volume>
      </element-citation>
    </ref>
    <ref id="ref-lfi-repo">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Durkan</surname><given-names>Conor</given-names></name>
        </person-group>
        <article-title>Lfi</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/conormdurkan/lfi; GitHub</publisher-name>
        <year iso-8601-date="2020">2020</year>
      </element-citation>
    </ref>
    <ref id="ref-louppe2016">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
          <name><surname>Cranmer</surname><given-names>Kyle</given-names></name>
          <name><surname>Pavez</surname><given-names>Juan</given-names></name>
        </person-group>
        <article-title>carl: A likelihood-free inference toolbox</article-title>
        <source>Journal of Open Source Software</source>
        <publisher-name>The Open Journal</publisher-name>
        <year iso-8601-date="2016">2016</year>
        <volume>1</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.21105/joss.00011</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-lueckmann2017">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Lueckmann</surname><given-names>Jan-Matthis</given-names></name>
          <name><surname>Goncalves</surname><given-names>Pedro J</given-names></name>
          <name><surname>Bassetto</surname><given-names>Giacomo</given-names></name>
          <name><surname>Öcal</surname><given-names>Kaan</given-names></name>
          <name><surname>Nonnenmacher</surname><given-names>Marcel</given-names></name>
          <name><surname>Macke</surname><given-names>Jakob H</given-names></name>
        </person-group>
        <article-title>Flexible statistical inference for mechanistic models of neural dynamics</article-title>
        <source>Advances in neural information processing systems 30</source>
        <year iso-8601-date="2017">2017</year>
      </element-citation>
    </ref>
    <ref id="ref-metropolis1953">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Metropolis</surname><given-names>Nicholas</given-names></name>
          <name><surname>Rosenbluth</surname><given-names>Arianna W</given-names></name>
          <name><surname>Rosenbluth</surname><given-names>Marshall N</given-names></name>
          <name><surname>Teller</surname><given-names>Augusta H</given-names></name>
          <name><surname>Teller</surname><given-names>Edward</given-names></name>
        </person-group>
        <article-title>Equation of state calculations by fast computing machines</article-title>
        <source>The Journal of Chemical Physics</source>
        <publisher-name>American Institute of Physics</publisher-name>
        <year iso-8601-date="1953">1953</year>
        <volume>21</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.1063/1.1699114</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-neal2003">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Neal</surname><given-names>Radford M</given-names></name>
        </person-group>
        <article-title>Slice sampling</article-title>
        <source>The Annals of Statistics</source>
        <publisher-name>Institute of Mathematical Statistics</publisher-name>
        <year iso-8601-date="2003">2003</year>
        <volume>31</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1214/aos/1056562461 </pub-id>
      </element-citation>
    </ref>
    <ref id="ref-nflows-repo">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Durkan</surname><given-names>Conor</given-names></name>
          <name><surname>Bekasov</surname><given-names>Artur</given-names></name>
          <name><surname>Papamakarios</surname><given-names>George</given-names></name>
          <name><surname>Murray</surname><given-names>Iain</given-names></name>
        </person-group>
        <article-title>nflows: Normalizing flows in PyTorch</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/bayesiains/nflows; GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-papamakarios2016">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Papamakarios</surname><given-names>George</given-names></name>
          <name><surname>Murray</surname><given-names>Iain</given-names></name>
        </person-group>
        <article-title>Fast \epsilon-free inference of simulation models with bayesian conditional density estimation</article-title>
        <source>Advances in neural information processing systems 29</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
    <ref id="ref-papamakarios2019a">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Papamakarios</surname><given-names>George</given-names></name>
          <name><surname>Sterratt</surname><given-names>David</given-names></name>
          <name><surname>Murray</surname><given-names>Iain</given-names></name>
        </person-group>
        <article-title>Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows</article-title>
        <source>Proceedings of Machine Learning Research</source>
        <publisher-name>PMLR</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>89</volume>
      </element-citation>
    </ref>
    <ref id="ref-paszke2019">
      <element-citation publication-type="chapter">
        <person-group person-group-type="author">
          <name><surname>Paszke</surname><given-names>Adam</given-names></name>
          <name><surname>Gross</surname><given-names>Sam</given-names></name>
          <name><surname>Massa</surname><given-names>Francisco</given-names></name>
          <name><surname>Lerer</surname><given-names>Adam</given-names></name>
          <name><surname>Bradbury</surname><given-names>James</given-names></name>
          <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
          <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
          <name><surname>Lin</surname><given-names>Zeming</given-names></name>
          <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
          <name><surname>Antiga</surname><given-names>Luca</given-names></name>
          <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
          <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
          <name><surname>Yang</surname><given-names>Edward</given-names></name>
          <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
          <name><surname>Raison</surname><given-names>Martin</given-names></name>
          <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
          <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
          <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
          <name><surname>Fang</surname><given-names>Lu</given-names></name>
          <name><surname>Bai</surname><given-names>Junjie</given-names></name>
          <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
        </person-group>
        <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
        <source>Advances in Neural Information Processing Systems 32</source>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-pritchard1999">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Pritchard</surname><given-names>Jonathan K</given-names></name>
          <name><surname>Seielstad</surname><given-names>Mark T</given-names></name>
          <name><surname>Perez-Lezaun</surname><given-names>Anna</given-names></name>
          <name><surname>Feldman</surname><given-names>Marcus W</given-names></name>
        </person-group>
        <article-title>Population growth of human Y chromosomes: A study of Y chromosome microsatellites.</article-title>
        <source>Molecular biology and evolution</source>
        <publisher-name>Oxford University Press</publisher-name>
        <year iso-8601-date="1999">1999</year>
        <volume>16</volume>
        <issue>12</issue>
        <pub-id pub-id-type="doi">10.1093/oxfordjournals.molbev.a026091</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-pydelfi-repo">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Alsing</surname><given-names>Justin</given-names></name>
        </person-group>
        <article-title>pydelfi: Density estimation likelihood-free inference</article-title>
        <source>GitHub repository</source>
        <publisher-name>https://github.com/justinalsing/pydelfi; GitHub</publisher-name>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-sisson2007">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Sisson</surname><given-names>Scott A</given-names></name>
          <name><surname>Fan</surname><given-names>Yanan</given-names></name>
          <name><surname>Tanaka</surname><given-names>Mark M</given-names></name>
        </person-group>
        <article-title>Sequential Monte Carlo without likelihoods</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <publisher-name>National Academy of Sciences</publisher-name>
        <year iso-8601-date="2007">2007</year>
        <volume>104</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.1073/pnas.0607208104</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-theano">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Al-Rfou</surname><given-names>Rami</given-names></name>
          <name><surname>Alain</surname><given-names>Guillaume</given-names></name>
          <name><surname>Almahairi</surname><given-names>Amjad</given-names></name>
          <name><surname>Angermueller</surname><given-names>Christof</given-names></name>
          <name><surname>Bahdanau</surname><given-names>Dzmitry</given-names></name>
          <name><surname>Ballas</surname><given-names>Nicolas</given-names></name>
          <name><surname>Bastien</surname><given-names>Frédéric</given-names></name>
          <name><surname>Bayer</surname><given-names>Justin</given-names></name>
          <name><surname>Belikov</surname><given-names>Anatoly</given-names></name>
          <name><surname>Belopolsky</surname><given-names>Alexander</given-names></name>
          <string-name>others</string-name>
        </person-group>
        <article-title>Theano: A python framework for fast computation of mathematical expressions</article-title>
        <source>arXiv</source>
        <year iso-8601-date="2016">2016</year>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
