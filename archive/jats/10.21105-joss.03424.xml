<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3424</article-id>
<article-id pub-id-type="doi">10.21105/joss.03424</article-id>
<title-group>
<article-title>Abmarl: Connecting Agent-Based Simulations with
Multi-Agent Reinforcement Learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-1033-439X</contrib-id>
<string-name>Edward Rusu</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<string-name>Ruben Glatt</string-name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Lawrence Livermore National Laboratory</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-06-10">
<day>10</day>
<month>6</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>64</issue>
<fpage>3424</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2021</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>agent-based simulation</kwd>
<kwd>multi-agent reinforcement learning</kwd>
<kwd>machine learning</kwd>
<kwd>agent-based modeling</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Abmarl is a package for developing Agent-Based Simulations and
  training them with Multi-Agent Reinforcement Learning (MARL). We
  provide an intuitive command line interface for engaging with the full
  workflow of MARL experimentation: training, visualizing, and analyzing
  agent behavior. We define an Agent-Based Simulation Interface and
  Simulation Manager, which control which agents interact with the
  simulation at each step. We support integration with popular
  reinforcement learning simulation interfaces, including gym.Env
  (<xref alt="Brockman et al., 2016" rid="ref-gym" ref-type="bibr">Brockman
  et al., 2016</xref>) and MultiAgentEnv
  (<xref alt="Liang et al., 2018" rid="ref-rllib" ref-type="bibr">Liang
  et al., 2018</xref>). We leverage RLlib’s framework for reinforcement
  learning and extend it to more easily support custom simulations,
  algorithms, and policies. We enable researchers to rapidly prototype
  MARL experiments and simulation design and lower the barrier for
  pre-existing projects to prototype Reinforcement Learning (RL) as a
  potential solution.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In 2016, Brockman et al.
  (<xref alt="2016" rid="ref-gym" ref-type="bibr">2016</xref>) published
  OpenAI Gym, an interface for single-agent simulations. This interface
  quickly became one of the most popular connections between simulation
  and training in RL experimentation. It has been used by many
  simulation benchmarks for single-agent reinforcement learning,
  including the Arcade Learning Environment
  (<xref alt="Bellemare et al., 2013" rid="ref-arcade" ref-type="bibr">Bellemare
  et al., 2013</xref>). Since then the field of Deep Reinforcement
  Learning (DRL) has exploded in both algorithm development and
  simulation design, and over the past few years researchers have been
  extending their interest to Multi-Agent Reinforcement Learning (MARL).
  Surprisingly complex and hierarchical behavior emerges in the
  interaction among multiple agents, especially when those agents differ
  in their objectives
  (<xref alt="Bowen Baker, 2020" rid="ref-hide-n-seek" ref-type="bibr">Bowen
  Baker, 2020</xref>).</p>
  <p>To train agents with Multi-Agent Reinforcement Learning, one needs
  two components: simulation and algorithm (also commonly referred to as
  environment and agent). Much effort has been given to the development
  of MARL algorithms, which has brought us exciting breakthroughs and
  enhancements in the field of artificial intelligence. Our aim,
  however, focuses on the simulation component of MARL.</p>
  <p>Several projects have attempted to define a standard set of
  benchmark scenarios for Multi-Agent problems. In their groundbreaking
  work, Lowe et al.
  (<xref alt="2020" rid="ref-maddpg" ref-type="bibr">2020</xref>)
  introduced MADDPG, a “centralized training, decentralized execution”
  multi-agent algorithm. Along with their algorithmic development, they
  created Multi-Particle Environment (MPE) (now managed as a part of
  PettingZoo) as a benchmark suite that includes continuous movement and
  communication features. Zheng et al.
  (<xref alt="2018" rid="ref-magent" ref-type="bibr">2018</xref>)
  produced scalable grid-based simulations and demonstrated emergent
  behavior in multi-team games on the order of millions of agents.
  Samvelyan et al.
  (<xref alt="2019" rid="ref-smac" ref-type="bibr">2019</xref>) and
  Perez-Liebana et al.
  (<xref alt="2019" rid="ref-marlo" ref-type="bibr">2019</xref>) brought
  RL research closer to home, giving researchers access to dozens of
  multi-agent scenarios in the popular games StarCraft and Minecraft,
  respectively. Suarez et al.
  (<xref alt="2019" rid="ref-neuralmmo" ref-type="bibr">2019</xref>)
  also targeted video games by supporting MARL in MMORPG-styled
  simulations with persistent, open-ended tasks among multiple agents.
  Zhou et al.
  (<xref alt="2020" rid="ref-smarts" ref-type="bibr">2020</xref>)
  brought us realistic traffic simulation scenarios to lead society
  towards autonomous driving.</p>
  <p>Each of these efforts and more are great milestones in RL
  simulation. Naturally, each of them couples the simulation interface
  with the underlying simulation. Terry et al.
  (<xref alt="2020" rid="ref-pettingzoo" ref-type="bibr">2020</xref>)
  have attempted to unify some of the more popular simulations under a
  single interface, giving researchers easier access to these
  simulations. While this is a step towards a standard multi-agent
  interface, most simulation efforts are tied to a specific set of
  already-built simulations with limited flexibility.</p>
  <p>Abmarl defines an interface for multi-agent simulations that is
  versatile, extendible, and intuitive. Rather than adapting gym’s
  interface for a targeted multi-agent simulation, we have built an
  interface from scratch that allows for the greatest flexibility while
  connecting to one of the most advanced, general-purpose, and
  open-source libraries: RLlib
  (<xref alt="Liang et al., 2018" rid="ref-rllib" ref-type="bibr">Liang
  et al., 2018</xref>). Our interface manages the loop between agents
  and the trainer, enabling the researcher to focus on simulation design
  and algorithmic development without worrying about the data
  exchange.</p>
  <p>We developed and tuned Abmarl’s intuitive command-line interface
  through practical experience while working on
  (<xref alt="Dawson et al., 2021" rid="ref-hybrid" ref-type="bibr">Dawson
  et al., 2021</xref>). Our interface gives researchers a running-start
  in MARL experimentation. We handle all the workflow elements needed to
  setup, run, and reproduce MARL experiments, providing direct access to
  train, visualize, and analyze experiments. We streamline the
  savvy-practitioner’s experience and lower the barrier for new
  researchers to join the field. The analysis module sets Abmarl apart
  from others as it provides a simple command line interface to add
  analytics to trained policies, allowing researchers to generate
  additional statistics and visualizations of agent and simulation
  metrics after the policy has been trained.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work was performed under the auspices of the U.S. Department
  of Energy by Lawrence Livermore National Laboratory under contract
  DE-AC52-07NA27344. Lawrence Livermore National Security, LLC through
  the support of LDRD 20-SI-005, 20-COMP-015, and 21-COMP-017.
  LLNL-CODE-815883.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-gym">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Brockman</surname><given-names>Greg</given-names></name>
          <name><surname>Cheung</surname><given-names>Vicki</given-names></name>
          <name><surname>Pettersson</surname><given-names>Ludwig</given-names></name>
          <name><surname>Schneider</surname><given-names>Jonas</given-names></name>
          <name><surname>Schulman</surname><given-names>John</given-names></name>
          <name><surname>Tang</surname><given-names>Jie</given-names></name>
          <name><surname>Zaremba</surname><given-names>Wojciech</given-names></name>
        </person-group>
        <article-title>OpenAI Gym</article-title>
        <year iso-8601-date="2016">2016</year>
        <uri>https://arxiv.org/abs/1606.01540</uri>
      </element-citation>
    </ref>
    <ref id="ref-rllib">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Liang</surname><given-names>Eric</given-names></name>
          <name><surname>Liaw</surname><given-names>Richard</given-names></name>
          <name><surname>Nishihara</surname><given-names>Robert</given-names></name>
          <name><surname>Moritz</surname><given-names>Philipp</given-names></name>
          <name><surname>Fox</surname><given-names>Roy</given-names></name>
          <name><surname>Goldberg</surname><given-names>Ken</given-names></name>
          <name><surname>Gonzalez</surname><given-names>Joseph</given-names></name>
          <name><surname>Jordan</surname><given-names>Michael</given-names></name>
          <name><surname>Stoica</surname><given-names>Ion</given-names></name>
        </person-group>
        <article-title>RLlib: Abstractions for distributed reinforcement learning</article-title>
        <source>Proceedings of the 35th international conference on machine learning</source>
        <person-group person-group-type="editor">
          <name><surname>Dy</surname><given-names>Jennifer</given-names></name>
          <name><surname>Krause</surname><given-names>Andreas</given-names></name>
        </person-group>
        <publisher-name>PMLR</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>80</volume>
        <uri>http://proceedings.mlr.press/v80/liang18b.html</uri>
      </element-citation>
    </ref>
    <ref id="ref-arcade">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Bellemare</surname><given-names>M. G.</given-names></name>
          <name><surname>Naddaf</surname><given-names>Y.</given-names></name>
          <name><surname>Veness</surname><given-names>J.</given-names></name>
          <name><surname>Bowling</surname><given-names>M.</given-names></name>
        </person-group>
        <article-title>The Arcade Learning Environment: An evaluation platform for general agents</article-title>
        <source>Journal of Artificial Intelligence Research</source>
        <year iso-8601-date="2013-06">2013</year><month>06</month>
        <volume>47</volume>
        <pub-id pub-id-type="doi">10.1613/jair.3912</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hide-n-seek">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Bowen Baker</surname><given-names>Todor Markov</given-names><suffix>Ingmar Kanitscheider</suffix></name>
        </person-group>
        <article-title>Emergent tool use from multi-agent autocurricula</article-title>
        <year iso-8601-date="2020">2020</year>
        <uri>https://openreview.net/pdf?id=SkxpxJBKwS</uri>
      </element-citation>
    </ref>
    <ref id="ref-magent">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Zheng</surname><given-names>Lianmin</given-names></name>
          <name><surname>Yang</surname><given-names>Jiacheng</given-names></name>
          <name><surname>Cai</surname><given-names>Han</given-names></name>
          <name><surname>Zhou</surname><given-names>Ming</given-names></name>
          <name><surname>Zhang</surname><given-names>Weinan</given-names></name>
          <name><surname>Wang</surname><given-names>Jun</given-names></name>
          <name><surname>Yu</surname><given-names>Yong</given-names></name>
        </person-group>
        <article-title>MAgent: A many-agent reinforcement learning platform for artificial collective intelligence</article-title>
        <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
        <year iso-8601-date="2018">2018</year>
        <volume>32</volume>
        <issue>1</issue>
        <uri>https://ojs.aaai.org/index.php/AAAI/article/view/11371</uri>
      </element-citation>
    </ref>
    <ref id="ref-smac">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Samvelyan</surname><given-names>Mikayel</given-names></name>
          <name><surname>Rashid</surname><given-names>Tabish</given-names></name>
          <name><surname>Witt</surname><given-names>Christian Schroeder de</given-names></name>
          <name><surname>Farquhar</surname><given-names>Gregory</given-names></name>
          <name><surname>Nardelli</surname><given-names>Nantas</given-names></name>
          <name><surname>Rudner</surname><given-names>Tim G. J.</given-names></name>
          <name><surname>Hung</surname><given-names>Chia-Man</given-names></name>
          <name><surname>Torr</surname><given-names>Philiph H. S.</given-names></name>
          <name><surname>Foerster</surname><given-names>Jakob</given-names></name>
          <name><surname>Whiteson</surname><given-names>Shimon</given-names></name>
        </person-group>
        <article-title>The StarCraft Multi-Agent Challenge</article-title>
        <source>Computing Research Repository</source>
        <year iso-8601-date="2019">2019</year>
        <volume>abs/1902.04043</volume>
        <uri>https://arxiv.org/abs/1902.04043</uri>
      </element-citation>
    </ref>
    <ref id="ref-neuralmmo">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Suarez</surname><given-names>Joseph</given-names></name>
          <name><surname>Du</surname><given-names>Yilun</given-names></name>
          <name><surname>Isola</surname><given-names>Phillip</given-names></name>
          <name><surname>Mordatch</surname><given-names>Igor</given-names></name>
        </person-group>
        <article-title>Neural MMO: A massively multiplayer game environment for intelligent agents</article-title>
        <year iso-8601-date="2019">2019</year>
        <uri>https://openreview.net/forum?id=S1gWz2CcKX</uri>
      </element-citation>
    </ref>
    <ref id="ref-pettingzoo">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Terry</surname><given-names>Justin K</given-names></name>
          <name><surname>Black</surname><given-names>Benjamin</given-names></name>
          <name><surname>Jayakumar</surname><given-names>Mario</given-names></name>
          <name><surname>Hari</surname><given-names>Ananth</given-names></name>
          <name><surname>Santos</surname><given-names>Luis</given-names></name>
          <name><surname>Dieffendahl</surname><given-names>Clemens</given-names></name>
          <name><surname>Williams</surname><given-names>Niall L</given-names></name>
          <name><surname>Lokesh</surname><given-names>Yashas</given-names></name>
          <name><surname>Sullivan</surname><given-names>Ryan</given-names></name>
          <name><surname>Horsch</surname><given-names>Caroline</given-names></name>
          <name><surname>Ravi</surname><given-names>Praveen</given-names></name>
        </person-group>
        <article-title>PettingZoo: Gym for multi-agent reinforcement learning</article-title>
        <source>arXiv preprint arXiv:2009.14471</source>
        <year iso-8601-date="2020">2020</year>
        <uri>https://arxiv.org/pdf/2009.14471.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-hybrid">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Dawson</surname><given-names>William</given-names></name>
          <name><surname>Glatt</surname><given-names>Ruben</given-names></name>
          <name><surname>Rusu</surname><given-names>Edward</given-names></name>
          <name><surname>Soper</surname><given-names>Braden</given-names></name>
          <name><surname>Goldhahn</surname><given-names>Ryan</given-names></name>
        </person-group>
        <article-title>Hybrid information-driven multi-agent reinforcement learning</article-title>
        <year iso-8601-date="2021-02">2021</year><month>02</month>
        <uri>https://arxiv.org/abs/2102.01004</uri>
      </element-citation>
    </ref>
    <ref id="ref-maddpg">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Lowe</surname><given-names>Ryan</given-names></name>
          <name><surname>Wu</surname><given-names>Yi</given-names></name>
          <name><surname>Tamar</surname><given-names>Aviv</given-names></name>
          <name><surname>Harb</surname><given-names>Jean</given-names></name>
          <name><surname>Abbeel</surname><given-names>Pieter</given-names></name>
          <name><surname>Mordatch</surname><given-names>Igor</given-names></name>
        </person-group>
        <article-title>Multi-agent actor-critic for mixed cooperative-competitve environments</article-title>
        <year iso-8601-date="2020">2020</year>
        <uri>https://papers.nips.cc/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-smarts">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Zhou</surname><given-names>Ming</given-names></name>
          <name><surname>Luo</surname><given-names>Jun</given-names></name>
          <name><surname>Villella</surname><given-names>Julian</given-names></name>
          <name><surname>Yang</surname><given-names>Yaodong</given-names></name>
          <name><surname>Rusu</surname><given-names>David</given-names></name>
          <name><surname>Miao</surname><given-names>Jiayu</given-names></name>
          <name><surname>Zhang</surname><given-names>Weinan</given-names></name>
          <name><surname>Alban</surname><given-names>Montgomery</given-names></name>
          <name><surname>Fadakar</surname><given-names>Iman</given-names></name>
          <name><surname>Chen</surname><given-names>Zheng</given-names></name>
          <name><surname>Huang</surname><given-names>Aurora Chongxi</given-names></name>
          <name><surname>Wen</surname><given-names>Ying</given-names></name>
          <name><surname>Hassanzadeh</surname><given-names>Kimia</given-names></name>
          <name><surname>Graves</surname><given-names>Daniel</given-names></name>
          <name><surname>Chen</surname><given-names>Dong</given-names></name>
          <name><surname>Zhu</surname><given-names>Zhengbang</given-names></name>
          <name><surname>Nguyen</surname><given-names>Nhat</given-names></name>
          <name><surname>Elsayed</surname><given-names>Mohamed</given-names></name>
          <name><surname>Shao</surname><given-names>Kun</given-names></name>
          <name><surname>Ahilan</surname><given-names>Sanjeevan</given-names></name>
          <name><surname>Zhang</surname><given-names>Baokuan</given-names></name>
          <name><surname>Wu</surname><given-names>Jiannan</given-names></name>
          <name><surname>Fu</surname><given-names>Zhengang</given-names></name>
          <name><surname>Rezaee</surname><given-names>Kasra</given-names></name>
          <name><surname>Yadmellat</surname><given-names>Peyman</given-names></name>
          <name><surname>Rohani</surname><given-names>Mohsen</given-names></name>
          <name><surname>Nieves</surname><given-names>Nicolas Perez</given-names></name>
          <name><surname>Ni</surname><given-names>Yihan</given-names></name>
          <name><surname>Banijamali</surname><given-names>Seyedershad</given-names></name>
          <name><surname>Rivers</surname><given-names>Alexander Cowen</given-names></name>
          <name><surname>Tian</surname><given-names>Zheng</given-names></name>
          <name><surname>Palenicek</surname><given-names>Daniel</given-names></name>
          <name><surname>Ammar</surname><given-names>Haitham bou</given-names></name>
          <name><surname>Zhang</surname><given-names>Hongbo</given-names></name>
          <name><surname>Liu</surname><given-names>Wulong</given-names></name>
          <name><surname>Hao</surname><given-names>Jianye</given-names></name>
          <name><surname>Wang</surname><given-names>Jun</given-names></name>
        </person-group>
        <article-title>SMARTS: Scalable multi-agent reinforcement learning training school for autonomous driving</article-title>
        <year iso-8601-date="2020-11">2020</year><month>11</month>
        <uri>https://arxiv.org/abs/2010.09776</uri>
      </element-citation>
    </ref>
    <ref id="ref-marlo">
      <element-citation>
        <person-group person-group-type="author">
          <name><surname>Perez-Liebana</surname><given-names>Diego</given-names></name>
          <name><surname>Hofmann</surname><given-names>Katja</given-names></name>
          <name><surname>Mohanty</surname><given-names>Sharada Prasanna</given-names></name>
          <name><surname>Kuno</surname><given-names>Noburu</given-names></name>
        </person-group>
        <article-title>The multi-agent reinforcement learning in MalmO competition</article-title>
        <year iso-8601-date="2019">2019</year>
        <uri>https://arxiv.org/pdf/1901.08129.pdf</uri>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
